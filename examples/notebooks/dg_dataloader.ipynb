{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\willi\\.conda\\envs\\octo\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "from robomimic.utils.rlds_utils import droid_dataset_transform, robomimic_transform, robomimic_dg_transform, TorchRLDSDataset\n",
    "\n",
    "from octo.data.dataset import make_dataset_from_rlds, make_interleaved_dataset, make_single_dataset\n",
    "from octo.data.utils.data_utils import combine_dataset_statistics\n",
    "from octo.utils.spec import ModuleSpec\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "from octo.utils.spec import ModuleSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'robomimic.utils.rlds_utils' from 'c:\\\\workspace\\\\deligrasp_policy_learning\\\\robomimic\\\\utils\\\\rlds_utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import robomimic\n",
    "import numpy as np\n",
    "importlib.reload(robomimic.utils.rlds_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "builder = tfds.builder_from_directory(builder_dir=f\"{DATA_PATH}/{DATASET_NAMES}/1.0.0\")\n",
    "ds = tfds.load(f\"{DATASET_NAMES}\", data_dir=f\"{DATA_PATH}\", split=\"train\")\n",
    "trajectory = next(iter(next(iter(ds))['steps']))\n",
    "gripper_force = trajectory[\"action\"][7]\n",
    "ad = trajectory[\"action_dict\"]\n",
    "# for i in ds:\n",
    "#     for j in i['steps']:\n",
    "#         print(j['observation']['state'][-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _gcd_import at 0x0000024C42433400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x0000024C42433400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _gcd_import at 0x0000024C42433400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x0000024C42433400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _gcd_import at 0x0000024C42433400> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function _gcd_import at 0x0000024C42433400>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(robomimic.utils.rlds_utils)\n",
    "DATA_PATH = \"C:/Users/willi/tensorflow_datasets\"    # UPDATE WITH PATH TO RLDS DATASETS\n",
    "DATASET_NAMES = \"deligrasp_dataset\"\n",
    "EXP_LOG_PATH = \"C:/workspace/deligrasp_policy_learning/logs\" # UPDATE WITH PATH TO DESIRED LOGGING DIRECTORY\n",
    "sample_weights = [1]\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "# builder = tfds.builder_from_directory(f\"{DATA_PATH}/1.0.0\")\n",
    "# builder.info.features\n",
    "\n",
    "BASE_DATASET_KWARGS = {\n",
    "    \"name\": \"deligrasp_dataset\",\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"image_obs_keys\": {\"primary\": \"image\", \"secondary\": \"wrist_image\"},\n",
    "    \"proprio_obs_key\": \"state\",\n",
    "    \"language_key\": \"language_instruction\",\n",
    "    \"action_proprio_normalization_type\": \"bounds\",\n",
    "    \"action_normalization_mask\": [True] * 9 + [False] * 2,      # don't normalize final (gripper) dimension\n",
    "    \"standardize_fn\": ModuleSpec.create(\"robomimic.utils.rlds_utils:deligrasp_dataset_transform\"),\n",
    "    # \"standardize_fn\": droid_dataset_transform,\n",
    "    # \"train\": True,\n",
    "}\n",
    "\n",
    "stats = make_dataset_from_rlds(**BASE_DATASET_KWARGS, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m action_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrotation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgripper_position\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgripper_force\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m action_shapes \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m), (), ()]\n\u001b[1;32m---> 24\u001b[0m action_stats \u001b[38;5;241m=\u001b[39m ActionUtils\u001b[38;5;241m.\u001b[39mget_action_stats_dict(\u001b[43mstats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, action_keys, action_shapes)\n\u001b[0;32m     26\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(robomimic\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrlds_utils\u001b[38;5;241m.\u001b[39mrobomimic_dg_transform, num_parallel_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "dataset = make_single_dataset(\n",
    "    BASE_DATASET_KWARGS,\n",
    "    train=True,\n",
    "    traj_transform_kwargs=dict(\n",
    "        window_size=2,\n",
    "        action_horizon=15,\n",
    "        subsample_length=50,\n",
    "        skip_unlabeled=True,\n",
    "    ),\n",
    "    frame_transform_kwargs=dict(\n",
    "        image_augment_kwargs=dict(\n",
    "        ),\n",
    "        resize_size=dict(\n",
    "            primary=[128, 128],\n",
    "            secondary=[128, 128],\n",
    "        ),\n",
    "        num_parallel_calls=200,\n",
    "    )\n",
    ")\n",
    "\n",
    "import robomimic.utils.action_utils as ActionUtils\n",
    "rds = dataset.dataset_statistics[0]\n",
    "action_keys = ['translation', 'rotation', 'gripper_position', 'gripper_force']\n",
    "action_shapes = [(3,1), (3,1), (), ()]\n",
    "action_stats = ActionUtils.get_action_stats_dict(stats[\"action\"], action_keys, action_shapes)\n",
    "\n",
    "dataset = dataset.map(robomimic.utils.rlds_utils.robomimic_dg_transform, num_parallel_calls=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dataset = robomimic.utils.rlds_utils.TorchRLDSDataset(dataset)\n",
    "train_loader = DataLoader(\n",
    "    pytorch_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=0,  # important to keep this to 0 so PyTorch does not mess with the parallelism\n",
    ")\n",
    "\n",
    "dli = iter(train_loader)\n",
    "batch = next(dli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:12,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, sample in tqdm.tqdm(enumerate(train_loader)):\n",
    "    if i == 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations\n",
      "type of camera/image/varied_camera_1_left_image: <class 'numpy.ndarray'>\n",
      "type of camera/image/varied_camera_1_left_image: float32\n",
      "type of camera/image/varied_camera_2_left_image: <class 'numpy.ndarray'>\n",
      "type of camera/image/varied_camera_2_left_image: float32\n",
      "type of raw_language: <class 'numpy.ndarray'>\n",
      "type of raw_language: object\n",
      "type of robot_state/cartesian_position: <class 'numpy.ndarray'>\n",
      "type of robot_state/cartesian_position: float32\n",
      "type of robot_state/gripper_position: <class 'numpy.ndarray'>\n",
      "type of robot_state/gripper_position: float32\n",
      "type of robot_state/applied_force: <class 'numpy.ndarray'>\n",
      "type of robot_state/applied_force: float32\n",
      "type of robot_state/contact_force: <class 'numpy.ndarray'>\n",
      "type of robot_state/contact_force: float32\n",
      "Actions\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n",
      "type: <class 'numpy.ndarray'>\n",
      "type: float32\n"
     ]
    }
   ],
   "source": [
    "# find trajectory length for each trajectory in dataset\n",
    "# traj_lengths = []\n",
    "# for traj in dataset:\n",
    "#     print(traj['obs']['camera/image/varied_camera_1_left_image'].shape)\n",
    "    \n",
    "i = 0\n",
    "s = None\n",
    "for sample in pytorch_dataset:\n",
    "    # print(sample)\n",
    "    s = sample\n",
    "    i += 1\n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "ok = s['obs'].keys()\n",
    "ak = s['actions']\n",
    "\n",
    "print(\"Observations\")\n",
    "for k in ok:\n",
    "    print(f\"type of {k}: {type(s['obs'][k])}\")\n",
    "    print(f\"dtype of {k}: {s['obs'][k].dtype}\")\n",
    "print(\"Actions\")\n",
    "for a in ak:\n",
    "    print(f\"type: {type(a)}\")\n",
    "    print(f\"dtype: {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(torch.utils.data.IterableDataset):\n",
    "    \"\"\"Thin wrapper around RLDS dataset for use with PyTorch dataloaders.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rlds_dataset,\n",
    "        train=True,\n",
    "    ):\n",
    "        self._rlds_dataset = rlds_dataset\n",
    "        self._is_train = train\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in self._rlds_dataset.as_numpy_iterator():\n",
    "            rl = sample['obs']['raw_language']\n",
    "            sample['obs']['raw_language'] = rl.tolist()\n",
    "            yield sample\n",
    "\n",
    "    def __len__(self):\n",
    "        lengths = np.array(\n",
    "            [\n",
    "                stats[\"num_transitions\"]\n",
    "                for stats in self._rlds_dataset.dataset_statistics\n",
    "            ]\n",
    "        )\n",
    "        if hasattr(self._rlds_dataset, \"sample_weights\"):\n",
    "            lengths *= np.array(self._rlds_dataset.sample_weights)\n",
    "        total_len = lengths.sum()\n",
    "        if self._is_train:\n",
    "            return int(0.95 * total_len)\n",
    "        else:\n",
    "            return int(0.05 * total_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dataset = test(dataset)\n",
    "train_loader = DataLoader(\n",
    "    pytorch_dataset,\n",
    "    batch_size=1,\n",
    "    num_workers=0,  # important to keep this to 0 so PyTorch does not mess with the parallelism\n",
    ")\n",
    "\n",
    "dli = iter(train_loader)\n",
    "batch = next(dli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'dict'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader)):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menumerating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sample)\n",
      "File \u001b[1;32mc:\\Users\\willi\\.conda\\envs\\octo\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\willi\\.conda\\envs\\octo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\willi\\.conda\\envs\\octo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\willi\\.conda\\envs\\octo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[98], line 20\u001b[0m, in \u001b[0;36mtest.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlds_dataset\u001b[38;5;241m.\u001b[39mas_numpy_iterator():\n\u001b[1;32m---> 20\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\willi\\.conda\\envs\\octo\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\willi\\.conda\\envs\\octo\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\willi\\.conda\\envs\\octo\\lib\\site-packages\\torchvision\\transforms\\functional.py:140\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    138\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (F_pil\u001b[38;5;241m.\u001b[39m_is_pil_image(pic) \u001b[38;5;129;01mor\u001b[39;00m _is_numpy(pic)):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(pic)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_numpy(pic) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpic should be 2/3 dimensional. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpic\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'dict'>"
     ]
    }
   ],
   "source": [
    "for i, sample in tqdm.tqdm(enumerate(train_loader)):\n",
    "    print(\"enumerating\")\n",
    "    print(sample)\n",
    "    print(i)\n",
    "    if i == 5000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robomimic.scripts.config_gen.helper import *\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "EXP_LOG_PATH = \"C:/workspace/deligrasp_policy_learning/logs\" # UPDATE WITH PATH TO DESIRED LOGGING DIRECTORY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from robomimic.scripts.config_gen.helper import *\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "#############################################################################\n",
    "# *************** Replace with your paths/config information ****************\n",
    "\n",
    "# Note: Assumes naming of dataset in \"datasets\" for the full DROID dataset is\n",
    "# droid\n",
    "\n",
    "DATA_PATH = \"C:/Users/willi/tensorflow_datasets/deligrasp_dataset_scaled\"    # UPDATE WITH PATH TO RLDS DATASETS\n",
    "EXP_LOG_PATH = \"C:/workspace/deligrasp_policy_learning/logs\" # UPDATE WITH PATH TO DESIRED LOGGING DIRECTORY\n",
    "EXP_NAMES = OrderedDict(\n",
    "    [\n",
    "        # Note: you can add co-training dataset here appending\n",
    "        # a new dataset to \"datasets\" and adjusting \"sample_weights\"\n",
    "        # accordingly\n",
    "        (\"droid\", {\"datasets\": [\"droid\"],\n",
    "                   \"sample_weights\": [1]})                                    \n",
    "    ])\n",
    "\n",
    "#############################################################################\n",
    "\n",
    "def make_generator_helper(args):\n",
    "    algo_name_short = \"diffusion_policy\"\n",
    "\n",
    "    generator = get_generator(\n",
    "        algo_name=\"diffusion_policy\",\n",
    "        config_file=os.path.join(base_path, 'robomimic/exps/templates/diffusion_policy_test.json'),\n",
    "        args=args,\n",
    "        exp_log_path=EXP_LOG_PATH,\n",
    "        algo_name_short=algo_name_short,\n",
    "        pt=True,\n",
    "    )\n",
    "    if args.ckpt_mode is None:\n",
    "        args.ckpt_mode = \"off\"\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.data_format\",\n",
    "        name=\"\",\n",
    "        group=-1,\n",
    "        values=[\n",
    "            \"droid_rlds\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.num_epochs\",\n",
    "        name=\"\",\n",
    "        group=-1,\n",
    "        values=[100000],\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.data_path\",\n",
    "        name=\"\",\n",
    "        group=-1,\n",
    "        values=[DATA_PATH],\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.shuffle_buffer_size\",\n",
    "        name=\"\",\n",
    "        group=-1,\n",
    "        values=[500000],\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.batch_size\",\n",
    "        name=\"bz\",\n",
    "        group=1212111,\n",
    "        values=[128],\n",
    "        hidename=False,\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.subsample_length\",\n",
    "        name=\"subsample_length\",\n",
    "        group=7070707,\n",
    "        values=[\n",
    "            100\n",
    "        ],\n",
    "        hidename=True,\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.num_parallel_calls\",\n",
    "        name=\"num_parallel_calls\",\n",
    "        group=404040404,\n",
    "        values=[\n",
    "            200\n",
    "        ],\n",
    "        hidename=True,\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.traj_transform_threads\",\n",
    "        name=\"traj_transform_threads\",\n",
    "        group=303030303,\n",
    "        values=[\n",
    "            48\n",
    "        ],\n",
    "        hidename=True,\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"train.traj_read_threads\",\n",
    "        name=\"traj_read_threads\",\n",
    "        group=908090809,\n",
    "        values=[\n",
    "            48\n",
    "        ],\n",
    "        hidename=True,\n",
    "    )\n",
    "\n",
    "    generator.add_param(\n",
    "        key=\"algo.noise_samples\",\n",
    "        name=\"noise_samples\",\n",
    "        group=1010101,\n",
    "        values=[8],\n",
    "        value_names=[\"8\"]\n",
    "    )\n",
    "\n",
    "    # use ddim by default\n",
    "    generator.add_param(\n",
    "        key=\"algo.ddim.enabled\",\n",
    "        name=\"ddim\",\n",
    "        group=1001,\n",
    "        values=[\n",
    "            True,\n",
    "            # False,\n",
    "        ],\n",
    "        hidename=True,\n",
    "    )\n",
    "    generator.add_param(\n",
    "        key=\"algo.ddpm.enabled\",\n",
    "        name=\"ddpm\",\n",
    "        group=1001,\n",
    "        values=[\n",
    "            False,\n",
    "            # True,\n",
    "        ],\n",
    "        hidename=True,\n",
    "    )\n",
    "\n",
    "    if args.env == \"deligrasp\":\n",
    "        generator.add_param(\n",
    "            key=\"train.data\",\n",
    "            name=\"ds\",\n",
    "            group=2,\n",
    "            values=[\n",
    "                [\n",
    "                    {\"path\": \"~/datasets/square/ph/square_ph_abs_tmp.hdf5\"}, # replace with your own path\n",
    "                ],\n",
    "            ],\n",
    "            value_names=[\n",
    "                \"square\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # update env config to use absolute action control\n",
    "        generator.add_param(\n",
    "            key=\"experiment.env_meta_update_dict\",\n",
    "            name=\"\",\n",
    "            group=-1,\n",
    "            values=[\n",
    "                {\"env_kwargs\": {\"controller_configs\": {\"control_delta\": False}}}\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        generator.add_param(\n",
    "            key=\"train.action_keys\",\n",
    "            name=\"ac_keys\",\n",
    "            group=-1,\n",
    "            values=[\n",
    "                [\n",
    "                    \"action_dict/abs_pos\",\n",
    "                    \"action_dict/abs_rot_6d\",\n",
    "                    \"action_dict/gripper\",\n",
    "                    # \"actions\",\n",
    "                ],\n",
    "            ],\n",
    "            value_names=[\n",
    "                \"abs\",\n",
    "            ],\n",
    "        )\n",
    "    \n",
    "    elif args.env == \"droid\":\n",
    "        generator.add_param(\n",
    "            key=\"train.sample_weights\",\n",
    "            name=\"sample_weights\",\n",
    "            group=24988,\n",
    "            values=[\n",
    "                EXP_NAMES[k][\"sample_weights\"] for k in EXP_NAMES.keys()\n",
    "            ],\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"train.dataset_names\",\n",
    "            name=\"dataset_names\",\n",
    "            group=24988,\n",
    "            values=[\n",
    "                EXP_NAMES[k][\"datasets\"] for k in EXP_NAMES.keys()\n",
    "            ],\n",
    "            value_names=list(EXP_NAMES.keys())\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"train.action_keys\",\n",
    "            name=\"ac_keys\",\n",
    "            group=-1,\n",
    "            values=[\n",
    "                [\n",
    "                    \"action/abs_pos\",\n",
    "                    \"action/abs_rot_6d\",\n",
    "                    \"action/gripper_position\",\n",
    "                ],\n",
    "            ],\n",
    "            value_names=[\n",
    "                \"abs\",\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"train.action_shapes\",\n",
    "            name=\"ac_shapes\",\n",
    "            group=-1,\n",
    "            values=[\n",
    "                [\n",
    "                    (1, 3),\n",
    "                    (1, 6),\n",
    "                    (1, 1),\n",
    "                ],\n",
    "            ],\n",
    "            value_names=[\n",
    "                \"ac_shapes\",\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.image_dim\",\n",
    "            name=\"\",\n",
    "            group=-1,\n",
    "            values=[\n",
    "                [128, 128],\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.modalities.obs.rgb\",\n",
    "            name=\"cams\",\n",
    "            group=130,\n",
    "            values=[\n",
    "                # [\"camera/image/hand_camera_left_image\"],\n",
    "                # [\"camera/image/hand_camera_left_image\", \"camera/image/hand_camera_right_image\"],\n",
    "                [\"camera/image/varied_camera_1_left_image\", \"camera/image/varied_camera_2_left_image\"],\n",
    "                # [\n",
    "                    # \"camera/image/hand_camera_left_image\", \"camera/image/hand_camera_right_image\",\n",
    "                #     \"camera/image/varied_camera_1_left_image\", \"camera/image/varied_camera_1_right_image\",\n",
    "                #     \"camera/image/varied_camera_2_left_image\", \"camera/image/varied_camera_2_right_image\",\n",
    "                # ],\n",
    "            ],\n",
    "            value_names=[\n",
    "                # \"wrist\",\n",
    "                # \"wrist-stereo\",\n",
    "                \"2cams\",\n",
    "                # \"3cams-stereo\",\n",
    "            ]\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.obs_randomizer_class\",\n",
    "            name=\"obsrand\",\n",
    "            group=130,\n",
    "            values=[\n",
    "                # \"ColorRandomizer\", # jitter only\n",
    "                [\"ColorRandomizer\", \"CropRandomizer\"], # jitter, followed by crop\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.obs_randomizer_kwargs\",\n",
    "            name=\"obsrandargs\",\n",
    "            group=130,\n",
    "            values=[\n",
    "                # {}, # jitter only\n",
    "                [{}, {\"crop_height\": 116, \"crop_width\": 116, \"num_crops\": 1, \"pos_enc\": False}], # jitter, followed by crop\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "\n",
    "        ### CONDITIONING\n",
    "        generator.add_param(\n",
    "            key=\"train.goal_mode\",\n",
    "            name=\"goal_mode\",\n",
    "            group=24986,\n",
    "            values = [\n",
    "                # \"geom\",\n",
    "                None, # Change this to \"geom\" to do goal conditioning\n",
    "\n",
    "            ]\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"train.truncated_geom_factor\",\n",
    "            name=\"truncated_geom_factor\",\n",
    "            group=5555,\n",
    "            values = [\n",
    "                0.3,\n",
    "                # 0.5\n",
    "            ]\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.modalities.obs.low_dim\",\n",
    "            name=\"ldkeys\",\n",
    "            group=24986,\n",
    "            values=[\n",
    "                [\"robot_state/cartesian_position\", \"robot_state/gripper_position\"],\n",
    "            ],\n",
    "            value_names=[\n",
    "                \"proprio-lang\",\n",
    "            ],\n",
    "            hidename=False,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.core_kwargs.backbone_kwargs.use_cam\",\n",
    "            name=\"\",\n",
    "            group=2498,\n",
    "            values=[\n",
    "                False,\n",
    "                # True,\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.core_kwargs.backbone_kwargs.pretrained\",\n",
    "            name=\"\",\n",
    "            group=2498,\n",
    "            values=[\n",
    "                # False,\n",
    "                True,\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.core_class\",\n",
    "            name=\"visenc\",\n",
    "            group=-1,\n",
    "            values=[\"VisualCore\"],\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.core_kwargs.backbone_class\",\n",
    "            name=\"\",\n",
    "            group=-1,\n",
    "            values=[\"ResNet50Conv\"],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.core_kwargs.feature_dimension\",\n",
    "            name=\"visdim\",\n",
    "            group=1234,\n",
    "            values=[\n",
    "                512,\n",
    "                # None,\n",
    "                # None\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.core_kwargs.flatten\",\n",
    "            name=\"flatten\",\n",
    "            group=1234,\n",
    "            values=[\n",
    "                True,\n",
    "                # False,\n",
    "                # False\n",
    "            ],\n",
    "            hidename=True,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.fuser\",\n",
    "            name=\"fuser\",\n",
    "            group=1234,\n",
    "            values=[\n",
    "                None,\n",
    "                # \"transformer\",\n",
    "                # \"perceiver\"\n",
    "            ],\n",
    "            hidename=False,\n",
    "        )\n",
    "        generator.add_param(\n",
    "            key=\"observation.encoder.rgb.core_kwargs.backbone_kwargs.downsample\",\n",
    "            name=\"\",\n",
    "            group=1234,\n",
    "            values=[\n",
    "                False,\n",
    "            ],\n",
    "            hidename=False,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    generator.add_param(\n",
    "        key=\"train.output_dir\",\n",
    "        name=\"\",\n",
    "        group=-1,\n",
    "        values=[\n",
    "            \"{exp_log_path}/{env}/{mod}/{algo_name_short}\".format(\n",
    "                exp_log_path=EXP_LOG_PATH,\n",
    "                env=args.env,\n",
    "                mod=args.mod, \n",
    "                algo_name_short=algo_name_short,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return generator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = get_argparser()\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    make_generator(args, make_generator_helper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
