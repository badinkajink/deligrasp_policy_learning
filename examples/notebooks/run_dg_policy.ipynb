{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b15f2e",
   "metadata": {},
   "source": [
    "# Run a trained policy\n",
    "\n",
    "This notebook will provide examples on how to run a trained policy and visualize the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000a4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\willi\\.conda\\envs\\octo_85b83fc\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import h5py\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "\n",
    "import robomimic\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.torch_utils as TorchUtils\n",
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "from robomimic.envs.env_base import EnvBase\n",
    "from robomimic.algo import RolloutPolicy\n",
    "import urllib.request\n",
    "# packages = [robomimic, FileUtils, TorchUtils, TensorUtils, ObsUtils, robomimic.envs.env_base, robomimic.algo]\n",
    "# import importlib\n",
    "# for i in packages:\n",
    "    # importlib.reload(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f4b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "from robomimic.utils.rlds_utils import droid_dataset_transform, robomimic_transform, TorchRLDSDataset, robomimic_dg_transform, dg_dataset_transform\n",
    "\n",
    "from octo.data.dataset import make_dataset_from_rlds, make_interleaved_dataset, make_single_dataset\n",
    "from octo.data.utils.data_utils import combine_dataset_statistics\n",
    "from octo.utils.spec import ModuleSpec\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "from octo.utils.spec import ModuleSpec\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47427159",
   "metadata": {},
   "source": [
    "### Loading trained policy\n",
    "\n",
    "We have a convenient function called `policy_from_checkpoint` that takes care of building the correct model from the checkpoint and load the trained weights. Of course you could also load the checkpoint manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a32935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/workspace/droid_policy_learning/logs/droid/im/diffusion_policy/10-07-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241007230909/models/deligrasp_model_epoch_30.pth\n",
      "============= Loaded Config =============\n",
      "{\n",
      "    \"algo_name\": \"diffusion_policy\",\n",
      "    \"experiment\": {\n",
      "        \"name\": \"bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None\",\n",
      "        \"validate\": false,\n",
      "        \"logging\": {\n",
      "            \"terminal_output_to_txt\": true,\n",
      "            \"log_tb\": true,\n",
      "            \"log_wandb\": true,\n",
      "            \"wandb_proj_name\": \"jaf\"\n",
      "        },\n",
      "        \"mse\": {\n",
      "            \"enabled\": true,\n",
      "            \"every_n_epochs\": 10,\n",
      "            \"on_save_ckpt\": true,\n",
      "            \"num_samples\": 6,\n",
      "            \"visualize\": true\n",
      "        },\n",
      "        \"save\": {\n",
      "            \"enabled\": true,\n",
      "            \"every_n_seconds\": null,\n",
      "            \"every_n_epochs\": 10,\n",
      "            \"epochs\": [],\n",
      "            \"on_best_validation\": false,\n",
      "            \"on_best_rollout_return\": false,\n",
      "            \"on_best_rollout_success_rate\": true\n",
      "        },\n",
      "        \"epoch_every_n_steps\": 100,\n",
      "        \"validation_epoch_every_n_steps\": 10,\n",
      "        \"env\": null,\n",
      "        \"additional_envs\": null,\n",
      "        \"render\": false,\n",
      "        \"render_video\": true,\n",
      "        \"keep_all_videos\": false,\n",
      "        \"video_skip\": 5,\n",
      "        \"rollout\": {\n",
      "            \"enabled\": false,\n",
      "            \"n\": 50,\n",
      "            \"horizon\": 400,\n",
      "            \"rate\": 40,\n",
      "            \"warmstart\": 0,\n",
      "            \"terminate_on_success\": true\n",
      "        },\n",
      "        \"env_meta_update_dict\": {},\n",
      "        \"ckpt_path\": null\n",
      "    },\n",
      "    \"train\": {\n",
      "        \"data\": null,\n",
      "        \"output_dir\": \"C:/workspace/droid_policy_learning/logs/droid/im/diffusion_policy\\\\10-07-None\",\n",
      "        \"num_data_workers\": 4,\n",
      "        \"hdf5_cache_mode\": null,\n",
      "        \"hdf5_use_swmr\": true,\n",
      "        \"hdf5_load_next_obs\": false,\n",
      "        \"hdf5_normalize_obs\": false,\n",
      "        \"hdf5_filter_key\": null,\n",
      "        \"hdf5_validation_filter_key\": null,\n",
      "        \"seq_length\": 15,\n",
      "        \"pad_seq_length\": true,\n",
      "        \"frame_stack\": 2,\n",
      "        \"pad_frame_stack\": true,\n",
      "        \"dataset_keys\": [],\n",
      "        \"action_keys\": [\n",
      "            \"action/rel_pos\",\n",
      "            \"action/rel_rot_6d\",\n",
      "            \"action/gripper_position\",\n",
      "            \"action/gripper_force\"\n",
      "        ],\n",
      "        \"action_shapes\": [\n",
      "            [\n",
      "                1,\n",
      "                3\n",
      "            ],\n",
      "            [\n",
      "                1,\n",
      "                6\n",
      "            ],\n",
      "            [\n",
      "                1,\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1,\n",
      "                1\n",
      "            ]\n",
      "        ],\n",
      "        \"action_config\": {\n",
      "            \"action/cartesian_position\": {\n",
      "                \"normalization\": \"min_max\"\n",
      "            },\n",
      "            \"action/abs_pos\": {\n",
      "                \"normalization\": \"min_max\"\n",
      "            },\n",
      "            \"action/abs_rot_6d\": {\n",
      "                \"normalization\": \"min_max\",\n",
      "                \"format\": \"rot_6d\",\n",
      "                \"convert_at_runtime\": \"rot_euler\"\n",
      "            },\n",
      "            \"action/abs_rot_euler\": {\n",
      "                \"normalization\": \"min_max\",\n",
      "                \"format\": \"rot_euler\"\n",
      "            },\n",
      "            \"action/gripper_position\": {\n",
      "                \"normalization\": null\n",
      "            },\n",
      "            \"action/gripper_force\": {\n",
      "                \"normalization\": null\n",
      "            },\n",
      "            \"action/cartesian_velocity\": {\n",
      "                \"normalization\": null\n",
      "            },\n",
      "            \"action/rel_pos\": {\n",
      "                \"normalization\": null\n",
      "            },\n",
      "            \"action/rel_rot_6d\": {\n",
      "                \"format\": \"rot_6d\",\n",
      "                \"normalization\": null,\n",
      "                \"convert_at_runtime\": \"rot_euler\"\n",
      "            },\n",
      "            \"action/rel_rot_euler\": {\n",
      "                \"format\": \"rot_euler\",\n",
      "                \"normalization\": null\n",
      "            },\n",
      "            \"action/gripper_velocity\": {\n",
      "                \"normalization\": null\n",
      "            }\n",
      "        },\n",
      "        \"goal_mode\": null,\n",
      "        \"truncated_geom_factor\": 0.3,\n",
      "        \"cuda\": true,\n",
      "        \"batch_size\": 16,\n",
      "        \"num_epochs\": 30,\n",
      "        \"seed\": 1,\n",
      "        \"max_grad_norm\": null,\n",
      "        \"data_format\": \"dg_rlds\",\n",
      "        \"shuffled_obs_key_groups\": [\n",
      "            [\n",
      "                [\n",
      "                    \"camera/image/varied_camera_1_left_image\",\n",
      "                    \"camera/image/varied_camera_1_right_image\",\n",
      "                    \"camera/extrinsics/varied_camera_1_left\",\n",
      "                    \"camera/extrinsics/varied_camera_1_right\"\n",
      "                ],\n",
      "                [\n",
      "                    \"camera/image/varied_camera_2_left_image\",\n",
      "                    \"camera/image/varied_camera_2_right_image\",\n",
      "                    \"camera/extrinsics/varied_camera_2_left\",\n",
      "                    \"camera/extrinsics/varied_camera_2_right\"\n",
      "                ]\n",
      "            ]\n",
      "        ],\n",
      "        \"data_path\": \"C:/Users/willi/tensorflow_datasets\",\n",
      "        \"shuffle_buffer_size\": 250,\n",
      "        \"sample_weights\": [\n",
      "            1\n",
      "        ],\n",
      "        \"dataset_names\": [\n",
      "            \"deligrasp_dataset\"\n",
      "        ],\n",
      "        \"subsample_length\": 50,\n",
      "        \"num_parallel_calls\": 200,\n",
      "        \"traj_transform_threads\": 48,\n",
      "        \"traj_read_threads\": 48\n",
      "    },\n",
      "    \"algo\": {\n",
      "        \"optim_params\": {\n",
      "            \"policy\": {\n",
      "                \"learning_rate\": {\n",
      "                    \"initial\": 0.0001,\n",
      "                    \"decay_factor\": 0.1,\n",
      "                    \"epoch_schedule\": []\n",
      "                },\n",
      "                \"regularization\": {\n",
      "                    \"L2\": 0.0\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"horizon\": {\n",
      "            \"observation_horizon\": 2,\n",
      "            \"action_horizon\": 8,\n",
      "            \"prediction_horizon\": 16\n",
      "        },\n",
      "        \"unet\": {\n",
      "            \"enabled\": true,\n",
      "            \"diffusion_step_embed_dim\": 256,\n",
      "            \"down_dims\": [\n",
      "                256,\n",
      "                512,\n",
      "                1024\n",
      "            ],\n",
      "            \"kernel_size\": 5,\n",
      "            \"n_groups\": 8\n",
      "        },\n",
      "        \"ema\": {\n",
      "            \"enabled\": true,\n",
      "            \"power\": 0.75\n",
      "        },\n",
      "        \"ddpm\": {\n",
      "            \"enabled\": false,\n",
      "            \"num_train_timesteps\": 100,\n",
      "            \"num_inference_timesteps\": 100,\n",
      "            \"beta_schedule\": \"squaredcos_cap_v2\",\n",
      "            \"clip_sample\": true,\n",
      "            \"prediction_type\": \"epsilon\"\n",
      "        },\n",
      "        \"noise_samples\": 8,\n",
      "        \"ddim\": {\n",
      "            \"enabled\": true,\n",
      "            \"num_train_timesteps\": 100,\n",
      "            \"num_inference_timesteps\": 10,\n",
      "            \"beta_schedule\": \"squaredcos_cap_v2\",\n",
      "            \"clip_sample\": true,\n",
      "            \"set_alpha_to_one\": true,\n",
      "            \"steps_offset\": 0,\n",
      "            \"prediction_type\": \"epsilon\"\n",
      "        }\n",
      "    },\n",
      "    \"observation\": {\n",
      "        \"image_dim\": [\n",
      "            128,\n",
      "            128\n",
      "        ],\n",
      "        \"modalities\": {\n",
      "            \"obs\": {\n",
      "                \"low_dim\": [\n",
      "                    \"robot_state/cartesian_position\",\n",
      "                    \"robot_state/gripper_position\",\n",
      "                    \"robot_state/applied_force\",\n",
      "                    \"robot_state/contact_force\"\n",
      "                ],\n",
      "                \"rgb\": [\n",
      "                    \"camera/image/varied_camera_1_left_image\",\n",
      "                    \"camera/image/varied_camera_2_left_image\"\n",
      "                ],\n",
      "                \"depth\": [],\n",
      "                \"scan\": []\n",
      "            },\n",
      "            \"goal\": {\n",
      "                \"low_dim\": [],\n",
      "                \"rgb\": [],\n",
      "                \"depth\": [],\n",
      "                \"scan\": []\n",
      "            }\n",
      "        },\n",
      "        \"encoder\": {\n",
      "            \"low_dim\": {\n",
      "                \"core_class\": null,\n",
      "                \"core_kwargs\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            },\n",
      "            \"rgb\": {\n",
      "                \"fuser\": null,\n",
      "                \"core_class\": \"VisualCore\",\n",
      "                \"core_kwargs\": {\n",
      "                    \"feature_dimension\": 512,\n",
      "                    \"backbone_class\": \"ResNet50Conv\",\n",
      "                    \"backbone_kwargs\": {\n",
      "                        \"pretrained\": true,\n",
      "                        \"input_coord_conv\": false,\n",
      "                        \"use_cam\": false,\n",
      "                        \"downsample\": false\n",
      "                    },\n",
      "                    \"pool_class\": null,\n",
      "                    \"pool_kwargs\": null,\n",
      "                    \"flatten\": true\n",
      "                },\n",
      "                \"input_maps\": {},\n",
      "                \"obs_randomizer_class\": [\n",
      "                    \"ColorRandomizer\",\n",
      "                    \"CropRandomizer\"\n",
      "                ],\n",
      "                \"obs_randomizer_kwargs\": [\n",
      "                    {},\n",
      "                    {\n",
      "                        \"crop_height\": 116,\n",
      "                        \"crop_width\": 116,\n",
      "                        \"num_crops\": 1,\n",
      "                        \"pos_enc\": false\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"depth\": {\n",
      "                \"fuser\": null,\n",
      "                \"core_class\": \"VisualCore\",\n",
      "                \"core_kwargs\": {},\n",
      "                \"input_maps\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            },\n",
      "            \"scan\": {\n",
      "                \"fuser\": null,\n",
      "                \"core_class\": \"ScanCore\",\n",
      "                \"core_kwargs\": {},\n",
      "                \"input_maps\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"meta\": {\n",
      "        \"hp_base_config_file\": \"c:\\\\workspace\\\\droid_policy_learning\\\\robomimic/exps/templates/diffusion_policy.json\",\n",
      "        \"hp_keys\": [\n",
      "            \"bz\",\n",
      "            \"subsample_length\",\n",
      "            \"num_parallel_calls\",\n",
      "            \"traj_transform_threads\",\n",
      "            \"traj_read_threads\",\n",
      "            \"noise_samples\",\n",
      "            \"ddim\",\n",
      "            \"ddpm\",\n",
      "            \"sample_weights\",\n",
      "            \"dataset_names\",\n",
      "            \"ac_keys\",\n",
      "            \"ac_shapes\",\n",
      "            \"cams\",\n",
      "            \"obsrand\",\n",
      "            \"obsrandargs\",\n",
      "            \"goal_mode\",\n",
      "            \"truncated_geom_factor\",\n",
      "            \"ldkeys\",\n",
      "            \"visenc\",\n",
      "            \"visdim\",\n",
      "            \"flatten\",\n",
      "            \"fuser\"\n",
      "        ],\n",
      "        \"hp_values\": [\n",
      "            16,\n",
      "            50,\n",
      "            200,\n",
      "            48,\n",
      "            48,\n",
      "            \"8\",\n",
      "            true,\n",
      "            false,\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            \"deligrasp\",\n",
      "            \"rel\",\n",
      "            \"ac_shapes\",\n",
      "            \"workspace_wrist\",\n",
      "            [\n",
      "                \"ColorRandomizer\",\n",
      "                \"CropRandomizer\"\n",
      "            ],\n",
      "            [\n",
      "                {},\n",
      "                {\n",
      "                    \"crop_height\": 116,\n",
      "                    \"crop_width\": 116,\n",
      "                    \"num_crops\": 1,\n",
      "                    \"pos_enc\": false\n",
      "                }\n",
      "            ],\n",
      "            null,\n",
      "            0.3,\n",
      "            \"proprio-lang\",\n",
      "            \"VisualCore\",\n",
      "            512,\n",
      "            true,\n",
      "            null\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot_state/applied_force', 'robot_state/contact_force', 'robot_state/gripper_position', 'robot_state/cartesian_position']\n",
      "using obs modality: rgb with keys: ['camera/image/varied_camera_1_left_image', 'camera/image/varied_camera_2_left_image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n",
      "Algo: _create_shapes\n",
      "Algo: Attempt 3\n",
      "Algo: obs_keys: {\n",
      "    \"obs\": {\n",
      "        \"low_dim\": [\n",
      "            \"robot_state/cartesian_position\",\n",
      "            \"robot_state/gripper_position\",\n",
      "            \"robot_state/applied_force\",\n",
      "            \"robot_state/contact_force\"\n",
      "        ],\n",
      "        \"rgb\": [\n",
      "            \"camera/image/varied_camera_1_left_image\",\n",
      "            \"camera/image/varied_camera_2_left_image\"\n",
      "        ],\n",
      "        \"depth\": [],\n",
      "        \"scan\": []\n",
      "    },\n",
      "    \"goal\": {\n",
      "        \"low_dim\": [],\n",
      "        \"rgb\": [],\n",
      "        \"depth\": [],\n",
      "        \"scan\": []\n",
      "    }\n",
      "}\n",
      "Algo: obs_key_shapes: OrderedDict([('robot_state/gripper_position', [1]), ('camera/image/varied_camera_1_left_image', [3, 128, 128]), ('camera/image/varied_camera_2_left_image', [3, 128, 128]), ('robot_state/applied_force', [1]), ('robot_state/contact_force', [1]), ('robot_state/cartesian_position', [6])])\n",
      "Algo: modalities: {\n",
      "    \"obs\": {\n",
      "        \"low_dim\": [\n",
      "            \"robot_state/cartesian_position\",\n",
      "            \"robot_state/gripper_position\",\n",
      "            \"robot_state/applied_force\",\n",
      "            \"robot_state/contact_force\"\n",
      "        ],\n",
      "        \"rgb\": [\n",
      "            \"camera/image/varied_camera_1_left_image\",\n",
      "            \"camera/image/varied_camera_2_left_image\"\n",
      "        ],\n",
      "        \"depth\": [],\n",
      "        \"scan\": []\n",
      "    },\n",
      "    \"goal\": {\n",
      "        \"low_dim\": [],\n",
      "        \"rgb\": [],\n",
      "        \"depth\": [],\n",
      "        \"scan\": []\n",
      "    }\n",
      "}\n",
      "Algo: modality values: dict_values([['robot_state/cartesian_position', 'robot_state/gripper_position', 'robot_state/applied_force', 'robot_state/contact_force'], ['camera/image/varied_camera_1_left_image', 'camera/image/varied_camera_2_left_image'], [], []])\n",
      "Algo: whatever this is: ['robot_state/cartesian_position', 'robot_state/gripper_position', 'robot_state/applied_force', 'robot_state/contact_force', 'camera/image/varied_camera_1_left_image', 'camera/image/varied_camera_2_left_image']\n",
      "Algo: now on key: robot_state/gripper_position\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[robot_state/gripper_position]: [1]\n",
      "Algo: now on key: camera/image/varied_camera_1_left_image\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[camera/image/varied_camera_1_left_image]: [3, 128, 128]\n",
      "Algo: now on key: camera/image/varied_camera_2_left_image\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[camera/image/varied_camera_2_left_image]: [3, 128, 128]\n",
      "Algo: now on key: robot_state/applied_force\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[robot_state/applied_force]: [1]\n",
      "Algo: now on key: robot_state/contact_force\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[robot_state/contact_force]: [1]\n",
      "Algo: now on key: robot_state/cartesian_position\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[robot_state/cartesian_position]: [6]\n",
      "obs_shapes OrderedDict([('robot_state/gripper_position', [1]), ('camera/image/varied_camera_1_left_image', [3, 128, 128]), ('camera/image/varied_camera_2_left_image', [3, 128, 128]), ('robot_state/applied_force', [1]), ('robot_state/contact_force', [1]), ('robot_state/cartesian_position', [6])])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\.conda\\envs\\octo_85b83fc\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\willi\\.conda\\envs\\octo_85b83fc\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP obs_dim: 512\n",
      "DP obs_dim shape: [512]\n",
      "DP global_cond_dim: 1024\n",
      "number of parameters: 7.990606e+07\n",
      "============= Loaded Policy =============\n",
      "DiffusionPolicyUNet (\n",
      "  ModuleDict(\n",
      "    (policy): ModuleDict(\n",
      "      (obs_encoder): DataParallel(\n",
      "        (module): ObservationGroupEncoder(\n",
      "            group=obs\n",
      "            ObservationEncoder(\n",
      "                Key(\n",
      "                    name=robot_state/gripper_position\n",
      "                    shape=[1]\n",
      "                    modality=low_dim\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): None\n",
      "                    )\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=camera/image/varied_camera_1_left_image\n",
      "                    shape=[3, 128, 128]\n",
      "                    modality=rgb\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): ColorRandomizer(input_shape=[3, 128, 128], brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3], num_samples=1)\n",
      "                      (1): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)\n",
      "                    )\n",
      "                    net=VisualCore(\n",
      "                      input_shape=[3, 116, 116]\n",
      "                      output_shape=[512]\n",
      "                      backbone_net=ResNet50Conv(input_channel=3, input_coord_conv=False)\n",
      "                      pool_net=None\n",
      "                    )\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=camera/image/varied_camera_2_left_image\n",
      "                    shape=[3, 128, 128]\n",
      "                    modality=rgb\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): ColorRandomizer(input_shape=[3, 128, 128], brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3], num_samples=1)\n",
      "                      (1): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)\n",
      "                    )\n",
      "                    net=VisualCore(\n",
      "                      input_shape=[3, 116, 116]\n",
      "                      output_shape=[512]\n",
      "                      backbone_net=ResNet50Conv(input_channel=3, input_coord_conv=False)\n",
      "                      pool_net=None\n",
      "                    )\n",
      "                    sharing_from=camera/image/varied_camera_1_left_image\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot_state/applied_force\n",
      "                    shape=[1]\n",
      "                    modality=low_dim\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): None\n",
      "                    )\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot_state/contact_force\n",
      "                    shape=[1]\n",
      "                    modality=low_dim\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): None\n",
      "                    )\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot_state/cartesian_position\n",
      "                    shape=[6]\n",
      "                    modality=low_dim\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): None\n",
      "                    )\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                output_shape=[1033]\n",
      "            )\n",
      "        )\n",
      "      )\n",
      "      (noise_pred_net): DataParallel(\n",
      "        (module): ConditionalUnet1D(\n",
      "          (mid_modules): ModuleList(\n",
      "            (0-1): 2 x ConditionalResidualBlock1D(\n",
      "              (blocks): ModuleList(\n",
      "                (0-1): 2 x Conv1dBlock(\n",
      "                  (block): Sequential(\n",
      "                    (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                    (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "                    (2): Mish()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (cond_encoder): Sequential(\n",
      "                (0): Mish()\n",
      "                (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "                (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "              )\n",
      "              (residual_conv): Identity()\n",
      "            )\n",
      "          )\n",
      "          (diffusion_step_encoder): Sequential(\n",
      "            (0): SinusoidalPosEmb()\n",
      "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (2): Mish()\n",
      "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (up_modules): ModuleList(\n",
      "            (0): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(2048, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Upsample1d(\n",
      "                (conv): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "              )\n",
      "            )\n",
      "            (1): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(1024, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Upsample1d(\n",
      "                (conv): ConvTranspose1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (down_modules): ModuleList(\n",
      "            (0): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(11, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(11, 256, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Downsample1d(\n",
      "                (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "              )\n",
      "            )\n",
      "            (1): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Downsample1d(\n",
      "                (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "              )\n",
      "            )\n",
      "            (2): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Identity()\n",
      "            )\n",
      "          )\n",
      "          (final_conv): Sequential(\n",
      "            (0): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "            (1): Conv1d(256, 11, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# dg-noforce\n",
    "dgnf = \"C:\\workspace\\droid_policy_learning\\logs\\droid\\im\\diffusion_policy/10-06-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_dg_noforce_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241006181237\\models\\model_epoch_10.pth\"\n",
    "dgnf = dgnf.replace('\\\\', '/')\n",
    "\n",
    "# dg\n",
    "# dg = \"C:\\workspace\\droid_policy_learning\\logs\\droid\\im\\diffusion_policy/10-06-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241006152541\\models\\model_epoch_45.pth\"\n",
    "# dg = \"C:/workspace/droid_policy_learning/logs/droid/im/diffusion_policy/10-05-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241005230511/models/model_epoch_25.pth\"\n",
    "# dg = \"C:\\workspace\\droid_policy_learning\\logs\\droid\\im\\diffusion_policy/10-07-None\\deligrasp_grasponly_cams_workspace_wrist/20241007185334\\models\\model_epoch_48.pth\"\n",
    "# dg = \"C:\\workspace\\droid_policy_learning\\logs\\droid\\im\\diffusion_policy/10-07-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_grasponly_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241007215755\\models\\model_epoch_30.pth\"\n",
    "# dg = \"C:\\workspace\\droid_policy_learning\\logs\\droid\\im\\diffusion_policy/10-07-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_grasponly_noforce_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241007223737\\models\\model_epoch_30.pth\"\n",
    "dg = \"C:\\workspace\\droid_policy_learning\\logs\\droid\\im\\diffusion_policy/10-07-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241007230909\\models\\deligrasp_model_epoch_30.pth\"\n",
    "# replace all '\\' chars with '/'\n",
    "dg = dg.replace('\\\\', '/')\n",
    "print(dg)\n",
    "device = TorchUtils.get_torch_device(try_to_use_cuda=True)\n",
    "\n",
    "# restore policy\n",
    "policy, ckpt_dict = FileUtils.policy_from_checkpoint(ckpt_path=dg, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872a3f0",
   "metadata": {},
   "source": [
    "### Creating rollout envionment\n",
    "The policy checkpoint also contains sufficient information to recreate the environment that it's trained with. Again, you may manually create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6291d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLDS Utils: robomimic_dg_transform: ds_format: dg_rlds\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"C:/Users/willi/tensorflow_datasets\"    # UPDATE WITH PATH TO RLDS DATASETS\n",
    "DATASET_NAME = \"deligrasp_dataset\"\n",
    "# DATASET_NAME = \"deligrasp_dataset_grasponly\"\n",
    "EXP_LOG_PATH = \"C:/workspace/deligrasp_policy_learning/logs\" # UPDATE WITH PATH TO DESIRED LOGGING DIRECTORY\n",
    "sample_weights = [1]\n",
    "\n",
    "BASE_DATASET_KWARGS = {\n",
    "    \"name\": DATASET_NAME,\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"image_obs_keys\": {\"primary\": \"image\", \"secondary\": \"wrist_image\"},\n",
    "    \"state_obs_keys\": [\"cartesian_position\", \"gripper_position\", \"applied_force\", \"contact_force\"],\n",
    "    # \"state_obs_keys\": [\"state\"], # this makes [\"observation\"]['proprio'].shape len 16\n",
    "    \"language_key\": \"language_instruction\",\n",
    "    \"norm_skip_keys\":  [\"proprio\"],\n",
    "    \"action_proprio_normalization_type\": \"bounds\",\n",
    "    \"absolute_action_mask\": [False] * 11,                    # droid_dataset_transform uses absolute actions\n",
    "    \"action_normalization_mask\": [False] * 11,      # don't normalize final (gripper) dimension\n",
    "    \"standardize_fn\": dg_dataset_transform,\n",
    "}\n",
    "\n",
    "dataset = make_single_dataset(\n",
    "    BASE_DATASET_KWARGS,\n",
    "    train=True,\n",
    "    traj_transform_kwargs=dict(\n",
    "        window_size=2,\n",
    "        future_action_window_size=15,\n",
    "        subsample_length=50,\n",
    "        skip_unlabeled=True,            # skip all trajectories without language annotation\n",
    "    ),\n",
    "    frame_transform_kwargs=dict(\n",
    "        image_augment_kwargs=dict(\n",
    "        ),\n",
    "        resize_size=dict(\n",
    "            primary=[128, 128],\n",
    "            secondary=[128, 128],\n",
    "        ),\n",
    "        num_parallel_calls=200,\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = dataset.map(robomimic_dg_transform, num_parallel_calls=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3703106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n",
      "(2, 3, 128, 128)\n",
      "(2, 3, 128, 128)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 6)\n"
     ]
    }
   ],
   "source": [
    "episode = None\n",
    "for i in dataset:\n",
    "    episode = i\n",
    "    break\n",
    "obs = episode[\"obs\"]\n",
    "# obs\n",
    "# # get first value of each key, that is the first observation step\n",
    "# obs_iter = [{k: obs[k][i] for k in obs.keys()} for i in range(len(obs['robot_state/applied_force']))]\n",
    "import numpy as np\n",
    "act = episode[\"actions\"]\n",
    "obs_iter = []\n",
    "for i in range(len(obs['robot_state/applied_force'])):\n",
    "    dict = {}\n",
    "    for k in obs.keys():\n",
    "        skip_keys = [\"raw\"]\n",
    "        if True in [sk in k for sk in skip_keys]:\n",
    "            # print(obs[k][i])\n",
    "            continue\n",
    "        # o = obs[k][i][0] # tensorflow eagertensor with dimension [T, ...]\n",
    "        o = obs[k][i] # tensorflow eagertensor with dimension [T, ...]\n",
    "        if \"image\" in k:\n",
    "            # o = o[k][i][0] # tensorflow eagertensor with dimension [T, ...]\n",
    "            # o = o[0] # tensorflow eagertensor with dimension [T, ...]\n",
    "            # currently in (H, W, C) format, need to change to (C, H, W)\n",
    "            # o = np.transpose(o, (2, 0, 1))\n",
    "            o = np.transpose(o, (0, 3, 1, 2))\n",
    "        else:\n",
    "            o = np.array(o)\n",
    "        # if \"image\" not in k:\n",
    "            # o = obs[k][i][0]\n",
    "        # add dimension 1, so that it is [B, T, ...]\n",
    "        # o = torch.from_numpy(np.expand_dims(o, axis=0))\n",
    "        # print(o.ndim)\n",
    "        print(o.shape)\n",
    "        # dict[k] = obs[k][i]\n",
    "        dict[k] = o\n",
    "    obs_iter.append(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27437985",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.start_episode()\n",
    "policy.goal_mode = None\n",
    "policy.action_queue = None\n",
    "policy.eval_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4374e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "TU: batch_size=1, seq_len=2\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "TU: outputs.shape=torch.Size([2, 512])\n",
      "TU: outputs=tensor([[-0.2057, -1.7795, -0.1767,  ..., -0.6907, -1.9359, -2.3945],\n",
      "        [-0.2057, -1.7795, -0.1767,  ..., -0.6907, -1.9359, -2.3945]],\n",
      "       device='cuda:0')\n",
      "TU: outputs.shape=torch.Size([1, 2, 512])\n",
      "TU: outputs=tensor([[[-0.2057, -1.7795, -0.1767,  ..., -0.6907, -1.9359, -2.3945],\n",
      "         [-0.2057, -1.7795, -0.1767,  ..., -0.6907, -1.9359, -2.3945]]],\n",
      "       device='cuda:0')\n",
      "DP: after time_distributed\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "DP: obs_features shape: torch.Size([1, 2, 512])\n",
      "DP: obs_cond shape: torch.Size([1, 1024])\n",
      "unnormalized action: {'action/rel_pos': array([-0.01507424, -0.00389477, -0.00696873], dtype=float32), 'action/rel_rot_6d': array([ 0.4786701 , -0.02365702, -0.03504921,  0.00479961,  0.53543746,\n",
      "        0.02115089], dtype=float32), 'action/gripper_position': array([0.01502148], dtype=float32), 'action/gripper_force': array([0.02163821], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.01507424, -0.00389477, -0.00696873]), 'action/rel_rot_6d': array([ 0.47867009, -0.02365702, -0.03504921,  0.00479961,  0.53543746,\n",
      "        0.02115089]), 'action/gripper_position': array([0.01502148]), 'action/gripper_force': array([0.02163821])}\n",
      "{'action/rel_pos': array([-0.01507424, -0.00389477, -0.00696873]), 'action/rel_rot_6d': array([-0.04011911, -0.07106104,  0.05214009], dtype=float32), 'action/gripper_position': array([0.01502148]), 'action/gripper_force': array([0.02163821])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01507424, -0.00389477, -0.00696873, -0.04011911, -0.07106104,\n",
       "        0.05214009,  0.01502148,  0.02163821])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(obs_iter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a20a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 0\n",
      "unnormalized action: {'action/rel_pos': array([ 0.04072944, -0.01496679,  0.00870464], dtype=float32), 'action/rel_rot_6d': array([ 0.12467919,  0.01749989, -0.0349833 ,  0.04255595,  0.13436852,\n",
      "       -0.01664682], dtype=float32), 'action/gripper_position': array([0.0192178], dtype=float32), 'action/gripper_force': array([0.0150936], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.04072944, -0.01496679,  0.00870464]), 'action/rel_rot_6d': array([ 0.12467919,  0.01749989, -0.0349833 ,  0.04255595,  0.13436852,\n",
      "       -0.01664682]), 'action/gripper_position': array([0.0192178]), 'action/gripper_force': array([0.0150936])}\n",
      "{'action/rel_pos': array([ 0.04072944, -0.01496679,  0.00870464]), 'action/rel_rot_6d': array([ 0.03663751, -0.2686051 , -0.14413816], dtype=float32), 'action/gripper_position': array([0.0192178]), 'action/gripper_force': array([0.0150936])}\n",
      "[  4.07294407  -1.49667906   0.87046437   3.66375074 -26.8605113\n",
      " -14.41381574   1.92177966   1.50936041]\n",
      "Ground Truth\n",
      "[ 1.0000000e-06  2.0000000e-06  2.8000000e-05  1.0000000e+00\n",
      "  1.0997425e-05 -2.5001133e-05 -1.1000000e-05  1.0000000e+00\n",
      " -1.0299972e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 1\n",
      "unnormalized action: {'action/rel_pos': array([0.0143675 , 0.03908677, 0.01831874], dtype=float32), 'action/rel_rot_6d': array([ 0.07735377,  0.04382788, -0.01218496, -0.01081892,  0.04523351,\n",
      "       -0.03641631], dtype=float32), 'action/gripper_position': array([0.03075772], dtype=float32), 'action/gripper_force': array([-0.04687346], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.0143675 , 0.03908677, 0.01831874]), 'action/rel_rot_6d': array([ 0.07735377,  0.04382788, -0.01218496, -0.01081892,  0.04523351,\n",
      "       -0.03641631]), 'action/gripper_position': array([0.03075772]), 'action/gripper_force': array([-0.04687346])}\n",
      "{'action/rel_pos': array([0.0143675 , 0.03908677, 0.01831874]), 'action/rel_rot_6d': array([ 0.63847303,  0.20812121, -0.49281585], dtype=float32), 'action/gripper_position': array([0.03075772]), 'action/gripper_force': array([-0.04687346])}\n",
      "[  1.4367504    3.90867703   1.83187407  63.84730339  20.81212103\n",
      " -49.28158522   3.07577197  -4.68734577]\n",
      "Ground Truth\n",
      "[ 3.8999999e-05 -7.9999998e-05 -2.9999999e-05  1.0000000e+00\n",
      " -5.7999387e-05  1.5002378e-05  5.8000001e-05  1.0000000e+00\n",
      " -4.0999130e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 2\n",
      "unnormalized action: {'action/rel_pos': array([0.01221911, 0.03641084, 0.0008319 ], dtype=float32), 'action/rel_rot_6d': array([ 0.00369504, -0.01374708,  0.00314807,  0.07617953,  0.03343455,\n",
      "       -0.06609489], dtype=float32), 'action/gripper_position': array([0.01934016], dtype=float32), 'action/gripper_force': array([-0.01940555], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.01221911, 0.03641084, 0.0008319 ]), 'action/rel_rot_6d': array([ 0.00369504, -0.01374708,  0.00314807,  0.07617953,  0.03343455,\n",
      "       -0.06609489]), 'action/gripper_position': array([0.01934016]), 'action/gripper_force': array([-0.01940555])}\n",
      "{'action/rel_pos': array([0.01221911, 0.03641084, 0.0008319 ]), 'action/rel_rot_6d': array([ 0.39203215, -0.5651247 ,  1.2659874 ], dtype=float32), 'action/gripper_position': array([0.01934016]), 'action/gripper_force': array([-0.01940555])}\n",
      "[ 1.22191086e+00  3.64108384e+00  8.31904355e-02  3.92032146e+01\n",
      " -5.65124691e+01  1.26598740e+02  1.93401556e+00 -1.94055457e+00]\n",
      "Ground Truth\n",
      "[ 4.8999998e-05 -5.5000000e-05 -4.9999999e-05  1.0000000e+00\n",
      "  1.4800366e-04 -7.2992603e-05 -1.4800001e-04  1.0000000e+00\n",
      "  5.0010804e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 3\n",
      "unnormalized action: {'action/rel_pos': array([0.0324727 , 0.00686712, 0.03011531], dtype=float32), 'action/rel_rot_6d': array([ 0.03687042, -0.03054195,  0.02765255,  0.01701901,  0.02661587,\n",
      "       -0.02902704], dtype=float32), 'action/gripper_position': array([0.03287019], dtype=float32), 'action/gripper_force': array([-0.0292848], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.0324727 , 0.00686712, 0.03011531]), 'action/rel_rot_6d': array([ 0.03687042, -0.03054195,  0.02765255,  0.01701901,  0.02661587,\n",
      "       -0.02902704]), 'action/gripper_position': array([0.03287019]), 'action/gripper_force': array([-0.0292848])}\n",
      "{'action/rel_pos': array([0.0324727 , 0.00686712, 0.03011531]), 'action/rel_rot_6d': array([ 0.79845667, -0.06986846,  0.838613  ], dtype=float32), 'action/gripper_position': array([0.03287019]), 'action/gripper_force': array([-0.0292848])}\n",
      "[ 3.24726999  0.68671233  3.01153138 79.84566689 -6.98684603 83.86129737\n",
      "  3.28701921 -2.92847995]\n",
      "Ground Truth\n",
      "[ 6.09999988e-05 -9.99999975e-05  7.59999966e-05  9.99999940e-01\n",
      "  1.12946327e-04 -2.84021342e-04 -1.12999995e-04  1.00000000e+00\n",
      " -1.88967897e-04  0.00000000e+00  0.00000000e+00]\n",
      "timestep 4\n",
      "unnormalized action: {'action/rel_pos': array([-0.03758946, -0.04534877, -0.01421583], dtype=float32), 'action/rel_rot_6d': array([-0.05398203,  0.04361124, -0.00374213,  0.00207032, -0.04210249,\n",
      "        0.00737228], dtype=float32), 'action/gripper_position': array([-0.00087214], dtype=float32), 'action/gripper_force': array([0.02759589], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.03758946, -0.04534877, -0.01421583]), 'action/rel_rot_6d': array([-0.05398203,  0.04361124, -0.00374213,  0.00207032, -0.04210249,\n",
      "        0.00737228]), 'action/gripper_position': array([-0.00087214]), 'action/gripper_force': array([0.02759589])}\n",
      "{'action/rel_pos': array([-0.03758946, -0.04534877, -0.01421583]), 'action/rel_rot_6d': array([ 0.1769277 , -0.07381848, -2.4636438 ], dtype=float32), 'action/gripper_position': array([-0.00087214]), 'action/gripper_force': array([0.02759589])}\n",
      "[-3.75894606e+00 -4.53487709e+00 -1.42158298e+00  1.76927701e+01\n",
      " -7.38184825e+00 -2.46364379e+02 -8.72136676e-02  2.75958907e+00]\n",
      "Ground Truth\n",
      "[ 5.29999998e-05 -1.63000004e-04 -1.17000003e-04  1.00000000e+00\n",
      "  1.18997785e-04  9.20028542e-05 -1.18999997e-04  1.00000000e+00\n",
      "  2.39890524e-05  0.00000000e+00  0.00000000e+00]\n",
      "timestep 5\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00724023, -0.03691074,  0.01106602], dtype=float32), 'action/rel_rot_6d': array([-0.04104965, -0.00065799, -0.01180438,  0.00934383,  0.00975619,\n",
      "        0.030511  ], dtype=float32), 'action/gripper_position': array([-0.00610999], dtype=float32), 'action/gripper_force': array([0.00232506], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00724023, -0.03691074,  0.01106602]), 'action/rel_rot_6d': array([-0.04104965, -0.00065799, -0.01180438,  0.00934383,  0.00975619,\n",
      "        0.030511  ]), 'action/gripper_position': array([-0.00610999]), 'action/gripper_force': array([0.00232506])}\n",
      "{'action/rel_pos': array([ 0.00724023, -0.03691074,  0.01106602]), 'action/rel_rot_6d': array([ 1.9032385 , -0.07853369, -2.8721147 ], dtype=float32), 'action/gripper_position': array([-0.00610999]), 'action/gripper_force': array([0.00232506])}\n",
      "[ 7.24023208e-01 -3.69107425e+00  1.10660186e+00  1.90323853e+02\n",
      " -7.85336867e+00 -2.87211466e+02 -6.10998785e-01  2.32505705e-01]\n",
      "Ground Truth\n",
      "[ 1.8000001e-05 -9.8999997e-05  4.2000000e-05  1.0000000e+00\n",
      "  1.0397637e-04 -1.7001445e-04 -1.0400000e-04  1.0000000e+00\n",
      " -1.3898232e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 6\n",
      "unnormalized action: {'action/rel_pos': array([-0.00728987,  0.02849663,  0.03297353], dtype=float32), 'action/rel_rot_6d': array([-0.04294718,  0.0178773 , -0.02884256,  0.0407293 ,  0.00869762,\n",
      "        0.00177866], dtype=float32), 'action/gripper_position': array([-0.00271231], dtype=float32), 'action/gripper_force': array([-0.00640107], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00728987,  0.02849663,  0.03297353]), 'action/rel_rot_6d': array([-0.04294718,  0.0178773 , -0.02884256,  0.0407293 ,  0.00869762,\n",
      "        0.00177866]), 'action/gripper_position': array([-0.00271231]), 'action/gripper_force': array([-0.00640107])}\n",
      "{'action/rel_pos': array([-0.00728987,  0.02849663,  0.03297353]), 'action/rel_rot_6d': array([-2.3577032 , -0.17973772,  2.493905  ], dtype=float32), 'action/gripper_position': array([-0.00271231]), 'action/gripper_force': array([-0.00640107])}\n",
      "[  -0.72898678    2.84966305    3.29735279 -235.77032089  -17.97377169\n",
      "  249.39050674   -0.27123105   -0.64010681]\n",
      "Ground Truth\n",
      "[ 3.99999990e-05 -1.13000002e-04  6.00000021e-06  1.00000000e+00\n",
      "  8.90017254e-05  1.49897642e-05 -8.90000010e-05  1.00000000e+00\n",
      " -1.15001334e-04  0.00000000e+00  0.00000000e+00]\n",
      "timestep 7\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "TU: batch_size=1, seq_len=2\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "TU: outputs.shape=torch.Size([2, 512])\n",
      "TU: outputs=tensor([[-0.2058, -1.7800, -0.1765,  ..., -0.6908, -1.9365, -2.3951],\n",
      "        [-0.2059, -1.7801, -0.1766,  ..., -0.6907, -1.9367, -2.3953]],\n",
      "       device='cuda:0')\n",
      "TU: outputs.shape=torch.Size([1, 2, 512])\n",
      "TU: outputs=tensor([[[-0.2058, -1.7800, -0.1765,  ..., -0.6908, -1.9365, -2.3951],\n",
      "         [-0.2059, -1.7801, -0.1766,  ..., -0.6907, -1.9367, -2.3953]]],\n",
      "       device='cuda:0')\n",
      "DP: after time_distributed\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "DP: obs_features shape: torch.Size([1, 2, 512])\n",
      "DP: obs_cond shape: torch.Size([1, 1024])\n",
      "unnormalized action: {'action/rel_pos': array([-1.9065982e-03,  1.2469770e-03,  8.8391789e-06], dtype=float32), 'action/rel_rot_6d': array([ 9.7959280e-01, -2.2777375e-04, -3.4066576e-03,  3.1794736e-03,\n",
      "        9.8379964e-01,  6.5055955e-04], dtype=float32), 'action/gripper_position': array([-0.00660044], dtype=float32), 'action/gripper_force': array([0.00135683], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-1.90659822e-03,  1.24697702e-03,  8.83917892e-06]), 'action/rel_rot_6d': array([ 9.79592800e-01, -2.27773751e-04, -3.40665760e-03,  3.17947357e-03,\n",
      "        9.83799636e-01,  6.50559552e-04]), 'action/gripper_position': array([-0.00660044]), 'action/gripper_force': array([0.00135683])}\n",
      "{'action/rel_pos': array([-1.90659822e-03,  1.24697702e-03,  8.83917892e-06]), 'action/rel_rot_6d': array([-0.00067251, -0.00347746,  0.00023486], dtype=float32), 'action/gripper_position': array([-0.00660044]), 'action/gripper_force': array([0.00135683])}\n",
      "[-0.19065982  0.1246977   0.00088392 -0.06725109 -0.34774551  0.02348561\n",
      " -0.66004372  0.13568305]\n",
      "Ground Truth\n",
      "[-3.7000002e-05  4.8000002e-05  5.6000001e-05  1.0000000e+00\n",
      "  1.4800069e-04  4.2997632e-05 -1.4800001e-04  1.0000000e+00\n",
      " -1.6006365e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 8\n",
      "unnormalized action: {'action/rel_pos': array([0.00056595, 0.00123684, 0.00210685], dtype=float32), 'action/rel_rot_6d': array([ 9.7968763e-01,  3.4798865e-04,  4.9899252e-05,  3.1397271e-03,\n",
      "        9.8079520e-01, -7.8307040e-04], dtype=float32), 'action/gripper_position': array([-0.00758931], dtype=float32), 'action/gripper_force': array([0.00086721], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.00056595, 0.00123684, 0.00210685]), 'action/rel_rot_6d': array([ 9.79687631e-01,  3.47988651e-04,  4.98992522e-05,  3.13972705e-03,\n",
      "        9.80795205e-01, -7.83070398e-04]), 'action/gripper_position': array([-0.00758931]), 'action/gripper_force': array([0.00086721])}\n",
      "{'action/rel_pos': array([0.00056595, 0.00123684, 0.00210685]), 'action/rel_rot_6d': array([ 7.9856737e-04,  5.1217477e-05, -3.5516289e-04], dtype=float32), 'action/gripper_position': array([-0.00758931]), 'action/gripper_force': array([0.00086721])}\n",
      "[ 0.05659459  0.12368357  0.21068549  0.07985674  0.00512175 -0.03551629\n",
      " -0.75893109  0.0867214 ]\n",
      "Ground Truth\n",
      "[-3.200000e-05  5.600000e-05  3.400000e-05  1.000000e+00  1.290024e-04\n",
      " -3.299058e-05 -1.290000e-04  1.000000e+00  7.300426e-05  0.000000e+00\n",
      "  0.000000e+00]\n",
      "timestep 9\n",
      "unnormalized action: {'action/rel_pos': array([-0.00013414,  0.00038522,  0.00151496], dtype=float32), 'action/rel_rot_6d': array([9.7480369e-01, 1.0462791e-03, 8.4175292e-05, 1.1914371e-03,\n",
      "       9.7815084e-01, 1.6345093e-03], dtype=float32), 'action/gripper_position': array([-0.00567745], dtype=float32), 'action/gripper_force': array([-0.00086614], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00013414,  0.00038522,  0.00151496]), 'action/rel_rot_6d': array([9.74803686e-01, 1.04627910e-03, 8.41752917e-05, 1.19143713e-03,\n",
      "       9.78150845e-01, 1.63450930e-03]), 'action/gripper_position': array([-0.00567745]), 'action/gripper_force': array([-0.00086614])}\n",
      "{'action/rel_pos': array([-0.00013414,  0.00038522,  0.00151496]), 'action/rel_rot_6d': array([-1.6709152e-03,  8.4557469e-05, -1.0734652e-03], dtype=float32), 'action/gripper_position': array([-0.00567745]), 'action/gripper_force': array([-0.00086614])}\n",
      "[-0.01341379  0.03852235  0.15149615 -0.16709152  0.00845575 -0.10734652\n",
      " -0.56774509 -0.08661352]\n",
      "Ground Truth\n",
      "[-1.2000000e-05  0.0000000e+00  4.0000000e-06  1.0000000e+00\n",
      "  1.6200040e-04  5.8998867e-05 -1.6200000e-04  1.0000000e+00\n",
      " -7.0095580e-06  0.0000000e+00  0.0000000e+00]\n",
      "timestep 10\n",
      "unnormalized action: {'action/rel_pos': array([-2.2196460e-03,  9.9325240e-05, -2.5231943e-03], dtype=float32), 'action/rel_rot_6d': array([ 9.7415870e-01,  1.4256325e-03, -3.6461427e-04,  3.7875564e-03,\n",
      "        9.7504270e-01,  9.1184775e-04], dtype=float32), 'action/gripper_position': array([-0.00420817], dtype=float32), 'action/gripper_force': array([-0.00054785], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-2.21964600e-03,  9.93252397e-05, -2.52319430e-03]), 'action/rel_rot_6d': array([ 9.74158704e-01,  1.42563251e-03, -3.64614272e-04,  3.78755643e-03,\n",
      "        9.75042701e-01,  9.11847746e-04]), 'action/gripper_position': array([-0.00420817]), 'action/gripper_force': array([-0.00054785])}\n",
      "{'action/rel_pos': array([-2.21964600e-03,  9.93252397e-05, -2.52319430e-03]), 'action/rel_rot_6d': array([-0.00093665, -0.00037566, -0.0014631 ], dtype=float32), 'action/gripper_position': array([-0.00420817]), 'action/gripper_force': array([-0.00054785])}\n",
      "[-0.2219646   0.00993252 -0.25231943 -0.09366465 -0.03756569 -0.14630976\n",
      " -0.42081745 -0.05478493]\n",
      "Ground Truth\n",
      "[-1.700000e-05  6.000000e-06  1.100000e-05  1.000000e+00  7.999986e-05\n",
      " -3.500032e-05 -8.000000e-05  1.000000e+00 -3.997200e-06 -6.781764e-01\n",
      "  1.635000e-02]\n",
      "timestep 11\n",
      "unnormalized action: {'action/rel_pos': array([-0.00148373,  0.00347582, -0.00231684], dtype=float32), 'action/rel_rot_6d': array([ 9.7374654e-01,  1.4249785e-03, -6.2396890e-04,  2.1393972e-03,\n",
      "        9.7633588e-01, -1.8860411e-04], dtype=float32), 'action/gripper_position': array([-0.00216985], dtype=float32), 'action/gripper_force': array([-7.0225964e-05], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00148373,  0.00347582, -0.00231684]), 'action/rel_rot_6d': array([ 9.73746538e-01,  1.42497849e-03, -6.23968896e-04,  2.13939720e-03,\n",
      "        9.76335883e-01, -1.88604114e-04]), 'action/gripper_position': array([-0.00216985]), 'action/gripper_force': array([-7.0225964e-05])}\n",
      "{'action/rel_pos': array([-0.00148373,  0.00347582, -0.00231684]), 'action/rel_rot_6d': array([ 0.00019177, -0.00064051, -0.00146352], dtype=float32), 'action/gripper_position': array([-0.00216985]), 'action/gripper_force': array([-7.0225964e-05])}\n",
      "[-0.14837336  0.34758248 -0.23168449  0.01917719 -0.06405112 -0.14635193\n",
      " -0.21698507 -0.0070226 ]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.05064166  0.0025    ]\n",
      "timestep 12\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00389874, -0.00016716,  0.00127121], dtype=float32), 'action/rel_rot_6d': array([ 9.7350782e-01,  8.2068908e-04, -4.9990549e-06,  2.0895093e-03,\n",
      "        9.7843826e-01, -9.7091230e-05], dtype=float32), 'action/gripper_position': array([-0.00357569], dtype=float32), 'action/gripper_force': array([-0.00038352], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00389874, -0.00016716,  0.00127121]), 'action/rel_rot_6d': array([ 9.73507822e-01,  8.20689078e-04, -4.99905491e-06,  2.08950927e-03,\n",
      "        9.78438258e-01, -9.70912297e-05]), 'action/gripper_position': array([-0.00357569]), 'action/gripper_force': array([-0.00038352])}\n",
      "{'action/rel_pos': array([ 0.00389874, -0.00016716,  0.00127121]), 'action/rel_rot_6d': array([ 9.922003e-05, -5.051450e-06, -8.430229e-04], dtype=float32), 'action/gripper_position': array([-0.00357569]), 'action/gripper_force': array([-0.00038352])}\n",
      "[ 0.38987412 -0.01671618  0.12712142  0.009922   -0.00050514 -0.08430229\n",
      " -0.35756934 -0.03835168]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.02302774  0.        ]\n",
      "timestep 13\n",
      "unnormalized action: {'action/rel_pos': array([ 0.0003799 , -0.00153366, -0.00147908], dtype=float32), 'action/rel_rot_6d': array([ 9.7533077e-01,  1.8585303e-03, -9.4291999e-04,  1.3348204e-03,\n",
      "        9.8104185e-01,  3.8869805e-03], dtype=float32), 'action/gripper_position': array([-0.00375572], dtype=float32), 'action/gripper_force': array([-0.00056108], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.0003799 , -0.00153366, -0.00147908]), 'action/rel_rot_6d': array([ 9.75330770e-01,  1.85853033e-03, -9.42919985e-04,  1.33482041e-03,\n",
      "        9.81041849e-01,  3.88698047e-03]), 'action/gripper_position': array([-0.00375572]), 'action/gripper_force': array([-0.00056108])}\n",
      "{'action/rel_pos': array([ 0.0003799 , -0.00153366, -0.00147908]), 'action/rel_rot_6d': array([-0.0039634 , -0.00097431, -0.00190169], dtype=float32), 'action/gripper_position': array([-0.00375572]), 'action/gripper_force': array([-0.00056108])}\n",
      "[ 0.03798998 -0.15336584 -0.14790756 -0.39633992 -0.09743139 -0.19016886\n",
      " -0.37557164 -0.05610773]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00230274  0.0025    ]\n",
      "timestep 14\n",
      "unnormalized action: {'action/rel_pos': array([-1.6905395e-03, -3.8034556e-04,  2.7000115e-05], dtype=float32), 'action/rel_rot_6d': array([ 9.8060668e-01,  2.0756586e-03, -1.3681275e-03,  1.5552152e-03,\n",
      "        9.8095793e-01, -2.5461573e-04], dtype=float32), 'action/gripper_position': array([-0.00590537], dtype=float32), 'action/gripper_force': array([0.00142145], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-1.69053953e-03, -3.80345562e-04,  2.70001146e-05]), 'action/rel_rot_6d': array([ 9.80606675e-01,  2.07565865e-03, -1.36812753e-03,  1.55521522e-03,\n",
      "        9.80957925e-01, -2.54615734e-04]), 'action/gripper_position': array([-0.00590537]), 'action/gripper_force': array([0.00142145])}\n",
      "{'action/rel_pos': array([-1.69053953e-03, -3.80345562e-04,  2.70001146e-05]), 'action/rel_rot_6d': array([ 0.00025735, -0.00139464, -0.00211706], dtype=float32), 'action/gripper_position': array([-0.00590537]), 'action/gripper_force': array([0.00142145])}\n",
      "[-0.16905395 -0.03803456  0.00270001  0.02573472 -0.13946391 -0.21170625\n",
      " -0.59053707  0.14214533]\n",
      "Ground Truth\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "timestep 15\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "TU: batch_size=1, seq_len=2\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "TU: outputs.shape=torch.Size([2, 512])\n",
      "TU: outputs=tensor([[-0.2070, -1.7791, -0.1829,  ..., -0.6925, -1.9289, -2.3968],\n",
      "        [-0.2071, -1.7790, -0.1829,  ..., -0.6921, -1.9286, -2.3962]],\n",
      "       device='cuda:0')\n",
      "TU: outputs.shape=torch.Size([1, 2, 512])\n",
      "TU: outputs=tensor([[[-0.2070, -1.7791, -0.1829,  ..., -0.6925, -1.9289, -2.3968],\n",
      "         [-0.2071, -1.7790, -0.1829,  ..., -0.6921, -1.9286, -2.3962]]],\n",
      "       device='cuda:0')\n",
      "DP: after time_distributed\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "DP: obs_features shape: torch.Size([1, 2, 512])\n",
      "DP: obs_cond shape: torch.Size([1, 1024])\n",
      "unnormalized action: {'action/rel_pos': array([-0.19395961,  0.0205373 ,  0.0333552 ], dtype=float32), 'action/rel_rot_6d': array([ 0.18286788,  0.23836431, -0.14201601, -0.22653301,  0.41375914,\n",
      "        0.2182977 ], dtype=float32), 'action/gripper_position': array([-0.27553952], dtype=float32), 'action/gripper_force': array([-0.01901452], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.19395961,  0.0205373 ,  0.0333552 ]), 'action/rel_rot_6d': array([ 0.18286788,  0.23836431, -0.14201601, -0.22653301,  0.41375914,\n",
      "        0.21829771]), 'action/gripper_position': array([-0.27553952]), 'action/gripper_force': array([-0.01901452])}\n",
      "{'action/rel_pos': array([-0.19395961,  0.0205373 ,  0.0333552 ]), 'action/rel_rot_6d': array([-0.05968752, -0.7062196 , -0.7622281 ], dtype=float32), 'action/gripper_position': array([-0.27553952]), 'action/gripper_force': array([-0.01901452])}\n",
      "[-19.39596087   2.05373038   3.33552025  -5.96875176 -70.62196136\n",
      " -76.22280717 -27.55395174  -1.90145168]\n",
      "Ground Truth\n",
      "[-3.1000000e-05 -4.0000000e-06 -2.9000001e-05  1.0000000e+00\n",
      "  3.9999639e-05  1.8000801e-05 -3.9999999e-05  1.0000000e+00\n",
      "  1.9999279e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 16\n",
      "unnormalized action: {'action/rel_pos': array([0.00376352, 0.05636228, 0.00603042], dtype=float32), 'action/rel_rot_6d': array([-0.01766609, -0.02304489,  0.00280968,  0.01841478,  0.07972435,\n",
      "        0.03407193], dtype=float32), 'action/gripper_position': array([0.01504685], dtype=float32), 'action/gripper_force': array([0.05761952], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.00376352, 0.05636228, 0.00603042]), 'action/rel_rot_6d': array([-0.01766609, -0.02304489,  0.00280968,  0.01841478,  0.07972435,\n",
      "        0.03407193]), 'action/gripper_position': array([0.01504685]), 'action/gripper_force': array([0.05761952])}\n",
      "{'action/rel_pos': array([0.00376352, 0.05636228, 0.00603042]), 'action/rel_rot_6d': array([ 2.555264 ,  0.7069574, -2.4921832], dtype=float32), 'action/gripper_position': array([0.01504685]), 'action/gripper_force': array([0.05761952])}\n",
      "[   0.37635209    5.63622788    0.60304236  255.52639961   70.69573998\n",
      " -249.21832085    1.50468517    5.76195233]\n",
      "Ground Truth\n",
      "[-1.8999999e-05  8.6000000e-05  9.5000003e-05  1.0000000e+00\n",
      "  1.1005424e-05 -1.1299947e-04 -1.1000000e-05  1.0000000e+00\n",
      "  4.8001246e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 17\n",
      "unnormalized action: {'action/rel_pos': array([0.00960124, 0.02497066, 0.05766485], dtype=float32), 'action/rel_rot_6d': array([ 0.08648631, -0.02234954, -0.02122587,  0.06767478,  0.03440423,\n",
      "       -0.02216122], dtype=float32), 'action/gripper_position': array([0.03384652], dtype=float32), 'action/gripper_force': array([-0.01918265], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.00960124, 0.02497066, 0.05766485]), 'action/rel_rot_6d': array([ 0.08648631, -0.02234954, -0.02122587,  0.06767478,  0.03440423,\n",
      "       -0.02216122]), 'action/gripper_position': array([0.03384652]), 'action/gripper_force': array([-0.01918265])}\n",
      "{'action/rel_pos': array([0.00960124, 0.02497066, 0.05766485]), 'action/rel_rot_6d': array([ 0.106588  , -0.26513162,  0.21919535], dtype=float32), 'action/gripper_position': array([0.03384652]), 'action/gripper_force': array([-0.01918265])}\n",
      "[  0.960124     2.497066     5.76648451  10.65879986 -26.51316226\n",
      "  21.9195351    3.38465162  -1.91826541]\n",
      "Ground Truth\n",
      "[-2.3000001e-05  1.2899999e-04  1.8000001e-05  9.9999994e-01\n",
      " -3.1991880e-05  2.8000094e-04  3.2000000e-05  1.0000000e+00\n",
      " -2.8991040e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 18\n",
      "unnormalized action: {'action/rel_pos': array([ 0.02829987, -0.09028974,  0.00164972], dtype=float32), 'action/rel_rot_6d': array([ 0.06696476, -0.03435234, -0.01582124, -0.04077487,  0.12296044,\n",
      "       -0.05600308], dtype=float32), 'action/gripper_position': array([0.03835371], dtype=float32), 'action/gripper_force': array([0.01444259], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.02829987, -0.09028974,  0.00164972]), 'action/rel_rot_6d': array([ 0.06696476, -0.03435234, -0.01582124, -0.04077487,  0.12296044,\n",
      "       -0.05600308]), 'action/gripper_position': array([0.03835371]), 'action/gripper_force': array([0.01444259])}\n",
      "{'action/rel_pos': array([ 0.02829987, -0.09028974,  0.00164972]), 'action/rel_rot_6d': array([ 0.5715966 , -0.4444443 ,  0.26756203], dtype=float32), 'action/gripper_position': array([0.03835371]), 'action/gripper_force': array([0.01444259])}\n",
      "[  2.82998662  -9.02897418   0.1649722   57.15966225 -44.44442987\n",
      "  26.75620317   3.83537114   1.44425947]\n",
      "Ground Truth\n",
      "[-1.0000000e-06  9.6000003e-05  1.2000000e-05  9.9999994e-01\n",
      " -2.5001002e-05  3.3399992e-04  2.4999998e-05  1.0000000e+00\n",
      "  3.0083502e-06  0.0000000e+00  0.0000000e+00]\n",
      "timestep 19\n",
      "unnormalized action: {'action/rel_pos': array([ 0.04037818, -0.03381857,  0.03903523], dtype=float32), 'action/rel_rot_6d': array([-0.0063615 ,  0.08728717, -0.07692358, -0.00920294,  0.07079909,\n",
      "        0.0800683 ], dtype=float32), 'action/gripper_position': array([0.06386684], dtype=float32), 'action/gripper_force': array([0.00362373], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.04037818, -0.03381857,  0.03903523]), 'action/rel_rot_6d': array([-0.0063615 ,  0.08728717, -0.07692358, -0.00920294,  0.07079909,\n",
      "        0.0800683 ]), 'action/gripper_position': array([0.06386684]), 'action/gripper_force': array([0.00362373])}\n",
      "{'action/rel_pos': array([ 0.04037818, -0.03381857,  0.03903523]), 'action/rel_rot_6d': array([ 1.2886146, -1.4692253, -2.1393826], dtype=float32), 'action/gripper_position': array([0.06386684]), 'action/gripper_force': array([0.00362373])}\n",
      "[   4.03781831   -3.3818569     3.90352346  128.86146307 -146.92252874\n",
      " -213.93826008    6.38668388    0.36237349]\n",
      "Ground Truth\n",
      "[-7.1000002e-05  5.1999999e-05 -5.1999999e-05  1.0000000e+00\n",
      " -9.5016869e-05  1.3598822e-04  9.5000003e-05  1.0000000e+00\n",
      "  1.2401292e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 20\n",
      "unnormalized action: {'action/rel_pos': array([-0.07140485,  0.07239465,  0.11037055], dtype=float32), 'action/rel_rot_6d': array([ 0.01596338,  0.04105932, -0.13838819,  0.02600662,  0.15632865,\n",
      "       -0.0299574 ], dtype=float32), 'action/gripper_position': array([-0.20065425], dtype=float32), 'action/gripper_force': array([0.04435474], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.07140485,  0.07239465,  0.11037055]), 'action/rel_rot_6d': array([ 0.01596338,  0.04105932, -0.13838819,  0.02600662,  0.15632865,\n",
      "       -0.0299574 ]), 'action/gripper_position': array([-0.20065425]), 'action/gripper_force': array([0.04435474])}\n",
      "{'action/rel_pos': array([-0.07140485,  0.07239465,  0.11037055]), 'action/rel_rot_6d': array([-1.1417338 , -1.4041604 ,  0.84639394], dtype=float32), 'action/gripper_position': array([-0.20065425]), 'action/gripper_force': array([0.04435474])}\n",
      "[  -7.1404852     7.23946467   11.03705466 -114.17337656 -140.41603804\n",
      "   84.63939428  -20.06542534    4.43547405]\n",
      "Ground Truth\n",
      "[-2.0100000e-04  4.9999999e-05 -1.4600001e-04  9.9999994e-01\n",
      "  1.8289949e-04  3.0006131e-04 -1.8299998e-04  9.9999994e-01\n",
      "  3.3494507e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 21\n",
      "unnormalized action: {'action/rel_pos': array([ 0.01651212, -0.01904705, -0.07607488], dtype=float32), 'action/rel_rot_6d': array([-0.06214485,  0.04164637, -0.02677767,  0.04491422, -0.00485324,\n",
      "       -0.04302201], dtype=float32), 'action/gripper_position': array([-0.00685654], dtype=float32), 'action/gripper_force': array([-0.04170556], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.01651212, -0.01904705, -0.07607488]), 'action/rel_rot_6d': array([-0.06214485,  0.04164637, -0.02677767,  0.04491422, -0.00485324,\n",
      "       -0.04302201]), 'action/gripper_position': array([-0.00685654]), 'action/gripper_force': array([-0.04170556])}\n",
      "{'action/rel_pos': array([ 0.01651212, -0.01904705, -0.07607488]), 'action/rel_rot_6d': array([-1.9553864 ,  0.43075535,  2.6075296 ], dtype=float32), 'action/gripper_position': array([-0.00685654]), 'action/gripper_force': array([-0.04170556])}\n",
      "[   1.65121164   -1.90470535   -7.60748759 -195.53864002   43.0755347\n",
      "  260.75296402   -0.68565393   -4.17055599]\n",
      "Ground Truth\n",
      "[-6.90000015e-05  5.50000004e-05  5.50000004e-05  1.00000000e+00\n",
      "  6.69978763e-05  1.12001275e-04 -6.70000009e-05  1.00000000e+00\n",
      "  1.89924958e-05  0.00000000e+00  0.00000000e+00]\n",
      "timestep 22\n",
      "unnormalized action: {'action/rel_pos': array([-0.03188647, -0.05734667, -0.0487804 ], dtype=float32), 'action/rel_rot_6d': array([-0.04560085, -0.02278722, -0.04304984, -0.066275  , -0.03498236,\n",
      "       -0.06771764], dtype=float32), 'action/gripper_position': array([-0.02537387], dtype=float32), 'action/gripper_force': array([0.09705402], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.03188647, -0.05734667, -0.0487804 ]), 'action/rel_rot_6d': array([-0.04560085, -0.02278722, -0.04304984, -0.066275  , -0.03498236,\n",
      "       -0.06771764]), 'action/gripper_position': array([-0.02537387]), 'action/gripper_force': array([0.09705402])}\n",
      "{'action/rel_pos': array([-0.03188647, -0.05734667, -0.0487804 ]), 'action/rel_rot_6d': array([-1.2235272 , -0.14750892,  2.3335803 ], dtype=float32), 'action/gripper_position': array([-0.02537387]), 'action/gripper_force': array([0.09705402])}\n",
      "[  -3.18864696   -5.73466681   -4.8780404  -122.35271931  -14.75089192\n",
      "  233.35802555   -2.53738724    9.70540196]\n",
      "Ground Truth\n",
      "[ 6.5000000e-05 -9.6000003e-05 -2.9999999e-05  1.0000000e+00\n",
      " -7.5020536e-05  1.5098980e-04  7.5000004e-05  1.0000000e+00\n",
      "  1.3601132e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 23\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "TU: batch_size=1, seq_len=2\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "TU: outputs.shape=torch.Size([2, 512])\n",
      "TU: outputs=tensor([[-0.2067, -1.7783, -0.1826,  ..., -0.6925, -1.9277, -2.3955],\n",
      "        [-0.2067, -1.7783, -0.1825,  ..., -0.6925, -1.9277, -2.3954]],\n",
      "       device='cuda:0')\n",
      "TU: outputs.shape=torch.Size([1, 2, 512])\n",
      "TU: outputs=tensor([[[-0.2067, -1.7783, -0.1826,  ..., -0.6925, -1.9277, -2.3955],\n",
      "         [-0.2067, -1.7783, -0.1825,  ..., -0.6925, -1.9277, -2.3954]]],\n",
      "       device='cuda:0')\n",
      "DP: after time_distributed\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "DP: obs_features shape: torch.Size([1, 2, 512])\n",
      "DP: obs_cond shape: torch.Size([1, 1024])\n",
      "unnormalized action: {'action/rel_pos': array([-0.01958842, -0.02421442, -0.03274655], dtype=float32), 'action/rel_rot_6d': array([ 0.8926817 ,  0.039248  , -0.0481293 , -0.10707952,  0.94680065,\n",
      "       -0.0994662 ], dtype=float32), 'action/gripper_position': array([-0.2297397], dtype=float32), 'action/gripper_force': array([0.04269951], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.01958842, -0.02421442, -0.03274655]), 'action/rel_rot_6d': array([ 0.89268172,  0.039248  , -0.0481293 , -0.10707952,  0.94680065,\n",
      "       -0.0994662 ]), 'action/gripper_position': array([-0.2297397]), 'action/gripper_force': array([0.04269951])}\n",
      "{'action/rel_pos': array([-0.01958842, -0.02421442, -0.03274655]), 'action/rel_rot_6d': array([ 0.110155  , -0.04871673, -0.04952755], dtype=float32), 'action/gripper_position': array([-0.2297397]), 'action/gripper_force': array([0.04269951])}\n",
      "[ -1.95884183  -2.42144186  -3.2746546   11.01550013  -4.87167276\n",
      "  -4.9527552  -22.97396958   4.26995084]\n",
      "Ground Truth\n",
      "[ 7.7999997e-05 -1.1300000e-04 -1.0100000e-04  9.9999994e-01\n",
      " -9.3008741e-05  3.7999786e-04  9.2999995e-05  1.0000000e+00\n",
      "  2.3035340e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 24\n",
      "unnormalized action: {'action/rel_pos': array([-0.02163478, -0.01759793,  0.05057979], dtype=float32), 'action/rel_rot_6d': array([ 0.9653904 , -0.04477868,  0.0051887 ,  0.00787822,  1.        ,\n",
      "        0.02223706], dtype=float32), 'action/gripper_position': array([-0.00520024], dtype=float32), 'action/gripper_force': array([0.10433001], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.02163478, -0.01759793,  0.05057979]), 'action/rel_rot_6d': array([ 0.96539038, -0.04477868,  0.0051887 ,  0.00787822,  1.        ,\n",
      "        0.02223706]), 'action/gripper_position': array([-0.00520024]), 'action/gripper_force': array([0.10433001])}\n",
      "{'action/rel_pos': array([-0.02163478, -0.01759793,  0.05057979]), 'action/rel_rot_6d': array([-0.02218297,  0.00640216,  0.0462195 ], dtype=float32), 'action/gripper_position': array([-0.00520024]), 'action/gripper_force': array([0.10433001])}\n",
      "[-2.16347836 -1.75979268  5.057979   -2.21829675  0.64021624  4.62194979\n",
      " -0.52002375 10.43300107]\n",
      "Ground Truth\n",
      "[ 2.9999999e-05 -1.6000000e-05  8.7000000e-05  1.0000000e+00\n",
      " -6.0003447e-05 -2.4991719e-05  5.9999998e-05  1.0000000e+00\n",
      " -1.3800150e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 25\n",
      "unnormalized action: {'action/rel_pos': array([-0.01265034,  0.00629587, -0.00496137], dtype=float32), 'action/rel_rot_6d': array([ 0.9950016 ,  0.02956351, -0.00535563, -0.02624778,  0.9068637 ,\n",
      "        0.0954104 ], dtype=float32), 'action/gripper_position': array([-0.03282607], dtype=float32), 'action/gripper_force': array([-0.01530224], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.01265034,  0.00629587, -0.00496137]), 'action/rel_rot_6d': array([ 0.99500161,  0.02956351, -0.00535563, -0.02624778,  0.90686369,\n",
      "        0.0954104 ]), 'action/gripper_position': array([-0.03282607]), 'action/gripper_force': array([-0.01530224])}\n",
      "{'action/rel_pos': array([-0.01265034,  0.00629587, -0.00496137]), 'action/rel_rot_6d': array([-0.1045802 , -0.00845455, -0.02897866], dtype=float32), 'action/gripper_position': array([-0.03282607]), 'action/gripper_force': array([-0.01530224])}\n",
      "[ -1.26503436   0.62958705  -0.49613719 -10.45802012  -0.84545538\n",
      "  -2.89786588  -3.2826066   -1.53022353]\n",
      "Ground Truth\n"
     ]
    }
   ],
   "source": [
    "timestep = 0\n",
    "for i in range(len(obs_iter)):\n",
    "    print(f\"timestep {i}\")\n",
    "    action = policy(obs_iter[i])\n",
    "    print(100*action)\n",
    "    print(f\"Ground Truth\")\n",
    "    if i < len(act):\n",
    "        print(np.array(act[i][0]))\n",
    "    timestep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e7fb32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionPolicyUNet (\n",
       "  ModuleDict(\n",
       "    (policy): ModuleDict(\n",
       "      (obs_encoder): DataParallel(\n",
       "        (module): ObservationGroupEncoder(\n",
       "            group=obs\n",
       "            ObservationEncoder(\n",
       "                Key(\n",
       "                    name=robot_state/gripper_position\n",
       "                    shape=[1]\n",
       "                    modality=low_dim\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): None\n",
       "                    )\n",
       "                    net=None\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=camera/image/varied_camera_1_left_image\n",
       "                    shape=[3, 128, 128]\n",
       "                    modality=rgb\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): ColorRandomizer(input_shape=[3, 128, 128], brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3], num_samples=1)\n",
       "                      (1): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)\n",
       "                    )\n",
       "                    net=VisualCore(\n",
       "                      input_shape=[3, 116, 116]\n",
       "                      output_shape=[512]\n",
       "                      backbone_net=ResNet50Conv(input_channel=3, input_coord_conv=False)\n",
       "                      pool_net=None\n",
       "                    )\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=camera/image/varied_camera_2_left_image\n",
       "                    shape=[3, 128, 128]\n",
       "                    modality=rgb\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): ColorRandomizer(input_shape=[3, 128, 128], brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3], num_samples=1)\n",
       "                      (1): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)\n",
       "                    )\n",
       "                    net=VisualCore(\n",
       "                      input_shape=[3, 116, 116]\n",
       "                      output_shape=[512]\n",
       "                      backbone_net=ResNet50Conv(input_channel=3, input_coord_conv=False)\n",
       "                      pool_net=None\n",
       "                    )\n",
       "                    sharing_from=camera/image/varied_camera_1_left_image\n",
       "                )\n",
       "                Key(\n",
       "                    name=robot_state/applied_force\n",
       "                    shape=[1]\n",
       "                    modality=low_dim\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): None\n",
       "                    )\n",
       "                    net=None\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=robot_state/contact_force\n",
       "                    shape=[1]\n",
       "                    modality=low_dim\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): None\n",
       "                    )\n",
       "                    net=None\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=robot_state/cartesian_position\n",
       "                    shape=[6]\n",
       "                    modality=low_dim\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): None\n",
       "                    )\n",
       "                    net=None\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                output_shape=[1033]\n",
       "            )\n",
       "        )\n",
       "      )\n",
       "      (noise_pred_net): DataParallel(\n",
       "        (module): ConditionalUnet1D(\n",
       "          (mid_modules): ModuleList(\n",
       "            (0-1): 2 x ConditionalResidualBlock1D(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x Conv1dBlock(\n",
       "                  (block): Sequential(\n",
       "                    (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                    (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                    (2): Mish()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (cond_encoder): Sequential(\n",
       "                (0): Mish()\n",
       "                (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
       "                (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "              )\n",
       "              (residual_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (diffusion_step_encoder): Sequential(\n",
       "            (0): SinusoidalPosEmb()\n",
       "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (2): Mish()\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (up_modules): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(2048, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Upsample1d(\n",
       "                (conv): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(1024, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Upsample1d(\n",
       "                (conv): ConvTranspose1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (down_modules): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(11, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(11, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Downsample1d(\n",
       "                (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Downsample1d(\n",
       "                (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (final_conv): Sequential(\n",
       "            (0): Conv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (1): Conv1d(256, 11, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac0e9f",
   "metadata": {},
   "source": [
    "### Define the rollout loop\n",
    "Now let's define the main rollout loop. The loop runs the policy to a target `horizon` and optionally writes the rollout to a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dd1375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(policy, env, horizon, render=False, video_writer=None, video_skip=5, camera_names=None):\n",
    "    \"\"\"\n",
    "    Helper function to carry out rollouts. Supports on-screen rendering, off-screen rendering to a video, \n",
    "    and returns the rollout trajectory.\n",
    "    Args:\n",
    "        policy (instance of RolloutPolicy): policy loaded from a checkpoint\n",
    "        env (instance of EnvBase): env loaded from a checkpoint or demonstration metadata\n",
    "        horizon (int): maximum horizon for the rollout\n",
    "        render (bool): whether to render rollout on-screen\n",
    "        video_writer (imageio writer): if provided, use to write rollout to video\n",
    "        video_skip (int): how often to write video frames\n",
    "        camera_names (list): determines which camera(s) are used for rendering. Pass more than\n",
    "            one to output a video with multiple camera views concatenated horizontally.\n",
    "    Returns:\n",
    "        stats (dict): some statistics for the rollout - such as return, horizon, and task success\n",
    "    \"\"\"\n",
    "    assert isinstance(env, EnvBase)\n",
    "    assert isinstance(policy, RolloutPolicy)\n",
    "    assert not (render and (video_writer is not None))\n",
    "\n",
    "    policy.start_episode()\n",
    "    obs = env.reset()\n",
    "    state_dict = env.get_state()\n",
    "\n",
    "    # hack that is necessary for robosuite tasks for deterministic action playback\n",
    "    obs = env.reset_to(state_dict)\n",
    "\n",
    "    results = {}\n",
    "    video_count = 0  # video frame counter\n",
    "    total_reward = 0.\n",
    "    try:\n",
    "        for step_i in range(horizon):\n",
    "\n",
    "            # get action from policy\n",
    "            act = policy(ob=obs)\n",
    "\n",
    "            # play action\n",
    "            next_obs, r, done, _ = env.step(act)\n",
    "\n",
    "            # compute reward\n",
    "            total_reward += r\n",
    "            success = env.is_success()[\"task\"]\n",
    "\n",
    "            # visualization\n",
    "            if render:\n",
    "                env.render(mode=\"human\", camera_name=camera_names[0])\n",
    "            if video_writer is not None:\n",
    "                if video_count % video_skip == 0:\n",
    "                    video_img = []\n",
    "                    for cam_name in camera_names:\n",
    "                        video_img.append(env.render(mode=\"rgb_array\", height=512, width=512, camera_name=cam_name))\n",
    "                    video_img = np.concatenate(video_img, axis=1) # concatenate horizontally\n",
    "                    video_writer.append_data(video_img)\n",
    "                video_count += 1\n",
    "\n",
    "            # break if done or if success\n",
    "            if done or success:\n",
    "                break\n",
    "\n",
    "            # update for next iter\n",
    "            obs = deepcopy(next_obs)\n",
    "            state_dict = env.get_state()\n",
    "\n",
    "    except env.rollout_exceptions as e:\n",
    "        print(\"WARNING: got rollout exception {}\".format(e))\n",
    "\n",
    "    stats = dict(Return=total_reward, Horizon=(step_i + 1), Success_Rate=float(success))\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43d371",
   "metadata": {},
   "source": [
    "### Run the policy\n",
    "Now let's rollout the policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be6e1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_horizon = 400\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "video_path = \"rollout.mp4\"\n",
    "video_writer = imageio.get_writer(video_path, fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fa67efe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m stats \u001b[38;5;241m=\u001b[39m rollout(\n\u001b[0;32m      2\u001b[0m     policy\u001b[38;5;241m=\u001b[39mpolicy, \n\u001b[1;32m----> 3\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m, \n\u001b[0;32m      4\u001b[0m     horizon\u001b[38;5;241m=\u001b[39mrollout_horizon, \n\u001b[0;32m      5\u001b[0m     render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m      6\u001b[0m     video_writer\u001b[38;5;241m=\u001b[39mvideo_writer, \n\u001b[0;32m      7\u001b[0m     video_skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m      8\u001b[0m     camera_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magentview\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(stats)\n\u001b[0;32m     11\u001b[0m video_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "stats = rollout(\n",
    "    policy=policy, \n",
    "    env=env, \n",
    "    horizon=rollout_horizon, \n",
    "    render=False, \n",
    "    video_writer=video_writer, \n",
    "    video_skip=5, \n",
    "    camera_names=[\"agentview\"]\n",
    ")\n",
    "print(stats)\n",
    "video_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79bc19",
   "metadata": {},
   "source": [
    "### Visualize the rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97472b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
