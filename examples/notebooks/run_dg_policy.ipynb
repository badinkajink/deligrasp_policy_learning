{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b15f2e",
   "metadata": {},
   "source": [
    "# Run a trained policy\n",
    "\n",
    "This notebook will provide examples on how to run a trained policy and visualize the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000a4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\willi\\.conda\\envs\\octo_85b83fc\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import h5py\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "\n",
    "import robomimic\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.torch_utils as TorchUtils\n",
    "import robomimic.utils.tensor_utils as TensorUtils\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "from robomimic.envs.env_base import EnvBase\n",
    "from robomimic.algo import RolloutPolicy\n",
    "import urllib.request\n",
    "# packages = [robomimic, FileUtils, TorchUtils, TensorUtils, ObsUtils, robomimic.envs.env_base, robomimic.algo]\n",
    "# import importlib\n",
    "# for i in packages:\n",
    "    # importlib.reload(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f4b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "from robomimic.utils.rlds_utils import droid_dataset_transform, robomimic_transform, TorchRLDSDataset, robomimic_dg_transform, dg_dataset_transform\n",
    "\n",
    "from octo.data.dataset import make_dataset_from_rlds, make_interleaved_dataset, make_single_dataset\n",
    "from octo.data.utils.data_utils import combine_dataset_statistics\n",
    "from octo.utils.spec import ModuleSpec\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "from octo.utils.spec import ModuleSpec\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47427159",
   "metadata": {},
   "source": [
    "### Download policy checkpoint\n",
    "First, let's try downloading a pretrained model from our model zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a32935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/workspace/droid_policy_learning/logs/droid/im/diffusion_policy/10-06-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241006152541/models/model_epoch_45.pth'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dg-noforce\n",
    "dgnf = \"C:\\workspace\\droid_policy_learning\\logs\\droid\\im\\diffusion_policy/10-06-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_dg_noforce_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241006181237\\models\\model_epoch_10.pth\"\n",
    "dgnf = dgnf.replace('\\\\', '/')\n",
    "\n",
    "# dg\n",
    "dg = \"C:\\workspace\\droid_policy_learning\\logs\\droid\\im\\diffusion_policy/10-06-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241006152541\\models\\model_epoch_45.pth\"\n",
    "# dg = \"C:/workspace/droid_policy_learning/logs/droid/im/diffusion_policy/10-05-None/bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None/20241005230511/models/model_epoch_25.pth\"\n",
    "# replace all '\\' chars with '/'\n",
    "dg = dg.replace('\\\\', '/')\n",
    "dg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c25c6",
   "metadata": {},
   "source": [
    "### Loading trained policy\n",
    "We have a convenient function called `policy_from_checkpoint` that takes care of building the correct model from the checkpoint and load the trained weights. Of course you could also load the checkpoint manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf84aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Loaded Config =============\n",
      "{\n",
      "    \"algo_name\": \"diffusion_policy\",\n",
      "    \"experiment\": {\n",
      "        \"name\": \"bz_16_noise_samples_8_sample_weights_1_dataset_names_deligrasp_cams_workspace_wrist_goal_mode_None_truncated_geom_factor_0.3_ldkeys_proprio-lang_visenc_VisualCore_fuser_None\",\n",
      "        \"validate\": false,\n",
      "        \"logging\": {\n",
      "            \"terminal_output_to_txt\": true,\n",
      "            \"log_tb\": true,\n",
      "            \"log_wandb\": true,\n",
      "            \"wandb_proj_name\": \"jaf\"\n",
      "        },\n",
      "        \"mse\": {\n",
      "            \"enabled\": true,\n",
      "            \"every_n_epochs\": 10,\n",
      "            \"on_save_ckpt\": true,\n",
      "            \"num_samples\": 6,\n",
      "            \"visualize\": true\n",
      "        },\n",
      "        \"save\": {\n",
      "            \"enabled\": true,\n",
      "            \"every_n_seconds\": null,\n",
      "            \"every_n_epochs\": 15,\n",
      "            \"epochs\": [],\n",
      "            \"on_best_validation\": false,\n",
      "            \"on_best_rollout_return\": false,\n",
      "            \"on_best_rollout_success_rate\": true\n",
      "        },\n",
      "        \"epoch_every_n_steps\": 100,\n",
      "        \"validation_epoch_every_n_steps\": 10,\n",
      "        \"env\": null,\n",
      "        \"additional_envs\": null,\n",
      "        \"render\": false,\n",
      "        \"render_video\": true,\n",
      "        \"keep_all_videos\": false,\n",
      "        \"video_skip\": 5,\n",
      "        \"rollout\": {\n",
      "            \"enabled\": false,\n",
      "            \"n\": 50,\n",
      "            \"horizon\": 400,\n",
      "            \"rate\": 40,\n",
      "            \"warmstart\": 0,\n",
      "            \"terminate_on_success\": true\n",
      "        },\n",
      "        \"env_meta_update_dict\": {},\n",
      "        \"ckpt_path\": null\n",
      "    },\n",
      "    \"train\": {\n",
      "        \"data\": null,\n",
      "        \"output_dir\": \"C:/workspace/droid_policy_learning/logs/droid/im/diffusion_policy\\\\10-06-None\",\n",
      "        \"num_data_workers\": 4,\n",
      "        \"hdf5_cache_mode\": null,\n",
      "        \"hdf5_use_swmr\": true,\n",
      "        \"hdf5_load_next_obs\": false,\n",
      "        \"hdf5_normalize_obs\": false,\n",
      "        \"hdf5_filter_key\": null,\n",
      "        \"hdf5_validation_filter_key\": null,\n",
      "        \"seq_length\": 15,\n",
      "        \"pad_seq_length\": true,\n",
      "        \"frame_stack\": 2,\n",
      "        \"pad_frame_stack\": true,\n",
      "        \"dataset_keys\": [],\n",
      "        \"action_keys\": [\n",
      "            \"action/rel_pos\",\n",
      "            \"action/rel_rot_6d\",\n",
      "            \"action/gripper_position\",\n",
      "            \"action/gripper_force\"\n",
      "        ],\n",
      "        \"action_shapes\": [\n",
      "            [\n",
      "                1,\n",
      "                3\n",
      "            ],\n",
      "            [\n",
      "                1,\n",
      "                6\n",
      "            ],\n",
      "            [\n",
      "                1,\n",
      "                1\n",
      "            ],\n",
      "            [\n",
      "                1,\n",
      "                1\n",
      "            ]\n",
      "        ],\n",
      "        \"action_config\": {\n",
      "            \"action/cartesian_position\": {\n",
      "                \"normalization\": \"min_max\"\n",
      "            },\n",
      "            \"action/abs_pos\": {\n",
      "                \"normalization\": \"min_max\"\n",
      "            },\n",
      "            \"action/abs_rot_6d\": {\n",
      "                \"normalization\": \"min_max\",\n",
      "                \"format\": \"rot_6d\",\n",
      "                \"convert_at_runtime\": \"rot_euler\"\n",
      "            },\n",
      "            \"action/abs_rot_euler\": {\n",
      "                \"normalization\": \"min_max\",\n",
      "                \"format\": \"rot_euler\"\n",
      "            },\n",
      "            \"action/gripper_position\": {\n",
      "                \"normalization\": \"min_max\"\n",
      "            },\n",
      "            \"action/gripper_force\": {\n",
      "                \"normalization\": \"min_max\"\n",
      "            },\n",
      "            \"action/cartesian_velocity\": {\n",
      "                \"normalization\": null\n",
      "            },\n",
      "            \"action/rel_pos\": {\n",
      "                \"normalization\": null\n",
      "            },\n",
      "            \"action/rel_rot_6d\": {\n",
      "                \"format\": \"rot_6d\",\n",
      "                \"normalization\": null,\n",
      "                \"convert_at_runtime\": \"rot_euler\"\n",
      "            },\n",
      "            \"action/rel_rot_euler\": {\n",
      "                \"format\": \"rot_euler\",\n",
      "                \"normalization\": null\n",
      "            },\n",
      "            \"action/gripper_velocity\": {\n",
      "                \"normalization\": null\n",
      "            }\n",
      "        },\n",
      "        \"goal_mode\": null,\n",
      "        \"truncated_geom_factor\": 0.3,\n",
      "        \"cuda\": true,\n",
      "        \"batch_size\": 16,\n",
      "        \"num_epochs\": 91,\n",
      "        \"seed\": 1,\n",
      "        \"max_grad_norm\": null,\n",
      "        \"data_format\": \"dg_rlds\",\n",
      "        \"shuffled_obs_key_groups\": [\n",
      "            [\n",
      "                [\n",
      "                    \"camera/image/varied_camera_1_left_image\",\n",
      "                    \"camera/image/varied_camera_1_right_image\",\n",
      "                    \"camera/extrinsics/varied_camera_1_left\",\n",
      "                    \"camera/extrinsics/varied_camera_1_right\"\n",
      "                ],\n",
      "                [\n",
      "                    \"camera/image/varied_camera_2_left_image\",\n",
      "                    \"camera/image/varied_camera_2_right_image\",\n",
      "                    \"camera/extrinsics/varied_camera_2_left\",\n",
      "                    \"camera/extrinsics/varied_camera_2_right\"\n",
      "                ]\n",
      "            ]\n",
      "        ],\n",
      "        \"data_path\": \"C:/Users/willi/tensorflow_datasets\",\n",
      "        \"shuffle_buffer_size\": 250,\n",
      "        \"sample_weights\": [\n",
      "            1\n",
      "        ],\n",
      "        \"dataset_names\": [\n",
      "            \"deligrasp_dataset\"\n",
      "        ],\n",
      "        \"subsample_length\": 50,\n",
      "        \"num_parallel_calls\": 200,\n",
      "        \"traj_transform_threads\": 48,\n",
      "        \"traj_read_threads\": 48\n",
      "    },\n",
      "    \"algo\": {\n",
      "        \"optim_params\": {\n",
      "            \"policy\": {\n",
      "                \"learning_rate\": {\n",
      "                    \"initial\": 0.0001,\n",
      "                    \"decay_factor\": 0.1,\n",
      "                    \"epoch_schedule\": []\n",
      "                },\n",
      "                \"regularization\": {\n",
      "                    \"L2\": 0.0\n",
      "                }\n",
      "            }\n",
      "        },\n",
      "        \"horizon\": {\n",
      "            \"observation_horizon\": 2,\n",
      "            \"action_horizon\": 8,\n",
      "            \"prediction_horizon\": 16\n",
      "        },\n",
      "        \"unet\": {\n",
      "            \"enabled\": true,\n",
      "            \"diffusion_step_embed_dim\": 256,\n",
      "            \"down_dims\": [\n",
      "                256,\n",
      "                512,\n",
      "                1024\n",
      "            ],\n",
      "            \"kernel_size\": 5,\n",
      "            \"n_groups\": 8\n",
      "        },\n",
      "        \"ema\": {\n",
      "            \"enabled\": true,\n",
      "            \"power\": 0.75\n",
      "        },\n",
      "        \"ddpm\": {\n",
      "            \"enabled\": false,\n",
      "            \"num_train_timesteps\": 100,\n",
      "            \"num_inference_timesteps\": 100,\n",
      "            \"beta_schedule\": \"squaredcos_cap_v2\",\n",
      "            \"clip_sample\": true,\n",
      "            \"prediction_type\": \"epsilon\"\n",
      "        },\n",
      "        \"noise_samples\": 8,\n",
      "        \"ddim\": {\n",
      "            \"enabled\": true,\n",
      "            \"num_train_timesteps\": 100,\n",
      "            \"num_inference_timesteps\": 10,\n",
      "            \"beta_schedule\": \"squaredcos_cap_v2\",\n",
      "            \"clip_sample\": true,\n",
      "            \"set_alpha_to_one\": true,\n",
      "            \"steps_offset\": 0,\n",
      "            \"prediction_type\": \"epsilon\"\n",
      "        }\n",
      "    },\n",
      "    \"observation\": {\n",
      "        \"image_dim\": [\n",
      "            128,\n",
      "            128\n",
      "        ],\n",
      "        \"modalities\": {\n",
      "            \"obs\": {\n",
      "                \"low_dim\": [\n",
      "                    \"robot_state/cartesian_position\",\n",
      "                    \"robot_state/gripper_position\",\n",
      "                    \"robot_state/applied_force\",\n",
      "                    \"robot_state/contact_force\"\n",
      "                ],\n",
      "                \"rgb\": [\n",
      "                    \"camera/image/varied_camera_1_left_image\",\n",
      "                    \"camera/image/varied_camera_2_left_image\"\n",
      "                ],\n",
      "                \"depth\": [],\n",
      "                \"scan\": []\n",
      "            },\n",
      "            \"goal\": {\n",
      "                \"low_dim\": [],\n",
      "                \"rgb\": [],\n",
      "                \"depth\": [],\n",
      "                \"scan\": []\n",
      "            }\n",
      "        },\n",
      "        \"encoder\": {\n",
      "            \"low_dim\": {\n",
      "                \"core_class\": null,\n",
      "                \"core_kwargs\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            },\n",
      "            \"rgb\": {\n",
      "                \"fuser\": null,\n",
      "                \"core_class\": \"VisualCore\",\n",
      "                \"core_kwargs\": {\n",
      "                    \"feature_dimension\": 512,\n",
      "                    \"backbone_class\": \"ResNet50Conv\",\n",
      "                    \"backbone_kwargs\": {\n",
      "                        \"pretrained\": true,\n",
      "                        \"input_coord_conv\": false,\n",
      "                        \"use_cam\": false,\n",
      "                        \"downsample\": false\n",
      "                    },\n",
      "                    \"pool_class\": null,\n",
      "                    \"pool_kwargs\": null,\n",
      "                    \"flatten\": true\n",
      "                },\n",
      "                \"input_maps\": {},\n",
      "                \"obs_randomizer_class\": [\n",
      "                    \"ColorRandomizer\",\n",
      "                    \"CropRandomizer\"\n",
      "                ],\n",
      "                \"obs_randomizer_kwargs\": [\n",
      "                    {},\n",
      "                    {\n",
      "                        \"crop_height\": 116,\n",
      "                        \"crop_width\": 116,\n",
      "                        \"num_crops\": 1,\n",
      "                        \"pos_enc\": false\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"depth\": {\n",
      "                \"fuser\": null,\n",
      "                \"core_class\": \"VisualCore\",\n",
      "                \"core_kwargs\": {},\n",
      "                \"input_maps\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            },\n",
      "            \"scan\": {\n",
      "                \"fuser\": null,\n",
      "                \"core_class\": \"ScanCore\",\n",
      "                \"core_kwargs\": {},\n",
      "                \"input_maps\": {},\n",
      "                \"obs_randomizer_class\": null,\n",
      "                \"obs_randomizer_kwargs\": {}\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"meta\": {\n",
      "        \"hp_base_config_file\": \"c:\\\\workspace\\\\droid_policy_learning\\\\robomimic/exps/templates/diffusion_policy.json\",\n",
      "        \"hp_keys\": [\n",
      "            \"bz\",\n",
      "            \"subsample_length\",\n",
      "            \"num_parallel_calls\",\n",
      "            \"traj_transform_threads\",\n",
      "            \"traj_read_threads\",\n",
      "            \"noise_samples\",\n",
      "            \"ddim\",\n",
      "            \"ddpm\",\n",
      "            \"sample_weights\",\n",
      "            \"dataset_names\",\n",
      "            \"ac_keys\",\n",
      "            \"ac_shapes\",\n",
      "            \"cams\",\n",
      "            \"obsrand\",\n",
      "            \"obsrandargs\",\n",
      "            \"goal_mode\",\n",
      "            \"truncated_geom_factor\",\n",
      "            \"ldkeys\",\n",
      "            \"visenc\",\n",
      "            \"visdim\",\n",
      "            \"flatten\",\n",
      "            \"fuser\"\n",
      "        ],\n",
      "        \"hp_values\": [\n",
      "            16,\n",
      "            50,\n",
      "            200,\n",
      "            48,\n",
      "            48,\n",
      "            \"8\",\n",
      "            true,\n",
      "            false,\n",
      "            [\n",
      "                1\n",
      "            ],\n",
      "            \"deligrasp\",\n",
      "            \"rel\",\n",
      "            \"ac_shapes\",\n",
      "            \"workspace_wrist\",\n",
      "            [\n",
      "                \"ColorRandomizer\",\n",
      "                \"CropRandomizer\"\n",
      "            ],\n",
      "            [\n",
      "                {},\n",
      "                {\n",
      "                    \"crop_height\": 116,\n",
      "                    \"crop_width\": 116,\n",
      "                    \"num_crops\": 1,\n",
      "                    \"pos_enc\": false\n",
      "                }\n",
      "            ],\n",
      "            null,\n",
      "            0.3,\n",
      "            \"proprio-lang\",\n",
      "            \"VisualCore\",\n",
      "            512,\n",
      "            true,\n",
      "            null\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['robot_state/applied_force', 'robot_state/gripper_position', 'robot_state/cartesian_position', 'robot_state/contact_force']\n",
      "using obs modality: rgb with keys: ['camera/image/varied_camera_1_left_image', 'camera/image/varied_camera_2_left_image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n",
      "Algo: _create_shapes\n",
      "Algo: Attempt 3\n",
      "Algo: obs_keys: {\n",
      "    \"obs\": {\n",
      "        \"low_dim\": [\n",
      "            \"robot_state/cartesian_position\",\n",
      "            \"robot_state/gripper_position\",\n",
      "            \"robot_state/applied_force\",\n",
      "            \"robot_state/contact_force\"\n",
      "        ],\n",
      "        \"rgb\": [\n",
      "            \"camera/image/varied_camera_1_left_image\",\n",
      "            \"camera/image/varied_camera_2_left_image\"\n",
      "        ],\n",
      "        \"depth\": [],\n",
      "        \"scan\": []\n",
      "    },\n",
      "    \"goal\": {\n",
      "        \"low_dim\": [],\n",
      "        \"rgb\": [],\n",
      "        \"depth\": [],\n",
      "        \"scan\": []\n",
      "    }\n",
      "}\n",
      "Algo: obs_key_shapes: OrderedDict([('robot_state/cartesian_position', [6]), ('robot_state/gripper_position', [1]), ('robot_state/applied_force', [1]), ('robot_state/contact_force', [1]), ('camera/image/varied_camera_1_left_image', [3, 128, 128]), ('camera/image/varied_camera_2_left_image', [3, 128, 128])])\n",
      "Algo: modalities: {\n",
      "    \"obs\": {\n",
      "        \"low_dim\": [\n",
      "            \"robot_state/cartesian_position\",\n",
      "            \"robot_state/gripper_position\",\n",
      "            \"robot_state/applied_force\",\n",
      "            \"robot_state/contact_force\"\n",
      "        ],\n",
      "        \"rgb\": [\n",
      "            \"camera/image/varied_camera_1_left_image\",\n",
      "            \"camera/image/varied_camera_2_left_image\"\n",
      "        ],\n",
      "        \"depth\": [],\n",
      "        \"scan\": []\n",
      "    },\n",
      "    \"goal\": {\n",
      "        \"low_dim\": [],\n",
      "        \"rgb\": [],\n",
      "        \"depth\": [],\n",
      "        \"scan\": []\n",
      "    }\n",
      "}\n",
      "Algo: modality values: dict_values([['robot_state/cartesian_position', 'robot_state/gripper_position', 'robot_state/applied_force', 'robot_state/contact_force'], ['camera/image/varied_camera_1_left_image', 'camera/image/varied_camera_2_left_image'], [], []])\n",
      "Algo: whatever this is: ['robot_state/cartesian_position', 'robot_state/gripper_position', 'robot_state/applied_force', 'robot_state/contact_force', 'camera/image/varied_camera_1_left_image', 'camera/image/varied_camera_2_left_image']\n",
      "Algo: now on key: robot_state/cartesian_position\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[robot_state/cartesian_position]: [6]\n",
      "Algo: now on key: robot_state/gripper_position\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[robot_state/gripper_position]: [1]\n",
      "Algo: now on key: robot_state/applied_force\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[robot_state/applied_force]: [1]\n",
      "Algo: now on key: robot_state/contact_force\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[robot_state/contact_force]: [1]\n",
      "Algo: now on key: camera/image/varied_camera_1_left_image\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[camera/image/varied_camera_1_left_image]: [3, 128, 128]\n",
      "Algo: now on key: camera/image/varied_camera_2_left_image\n",
      "Algo: checking obs\n",
      "Adding obs: obs_shapes[camera/image/varied_camera_2_left_image]: [3, 128, 128]\n",
      "obs_shapes OrderedDict([('robot_state/cartesian_position', [6]), ('robot_state/gripper_position', [1]), ('robot_state/applied_force', [1]), ('robot_state/contact_force', [1]), ('camera/image/varied_camera_1_left_image', [3, 128, 128]), ('camera/image/varied_camera_2_left_image', [3, 128, 128])])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\.conda\\envs\\octo_85b83fc\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\willi\\.conda\\envs\\octo_85b83fc\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 7.990606e+07\n",
      "============= Loaded Policy =============\n",
      "DiffusionPolicyUNet (\n",
      "  ModuleDict(\n",
      "    (policy): ModuleDict(\n",
      "      (obs_encoder): DataParallel(\n",
      "        (module): ObservationGroupEncoder(\n",
      "            group=obs\n",
      "            ObservationEncoder(\n",
      "                Key(\n",
      "                    name=robot_state/cartesian_position\n",
      "                    shape=[6]\n",
      "                    modality=low_dim\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): None\n",
      "                    )\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot_state/gripper_position\n",
      "                    shape=[1]\n",
      "                    modality=low_dim\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): None\n",
      "                    )\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot_state/applied_force\n",
      "                    shape=[1]\n",
      "                    modality=low_dim\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): None\n",
      "                    )\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=robot_state/contact_force\n",
      "                    shape=[1]\n",
      "                    modality=low_dim\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): None\n",
      "                    )\n",
      "                    net=None\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=camera/image/varied_camera_1_left_image\n",
      "                    shape=[3, 128, 128]\n",
      "                    modality=rgb\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): ColorRandomizer(input_shape=[3, 128, 128], brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3], num_samples=1)\n",
      "                      (1): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)\n",
      "                    )\n",
      "                    net=VisualCore(\n",
      "                      input_shape=[3, 116, 116]\n",
      "                      output_shape=[512]\n",
      "                      backbone_net=ResNet50Conv(input_channel=3, input_coord_conv=False)\n",
      "                      pool_net=None\n",
      "                    )\n",
      "                    sharing_from=None\n",
      "                )\n",
      "                Key(\n",
      "                    name=camera/image/varied_camera_2_left_image\n",
      "                    shape=[3, 128, 128]\n",
      "                    modality=rgb\n",
      "                    randomizer=ModuleList(\n",
      "                      (0): ColorRandomizer(input_shape=[3, 128, 128], brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3], num_samples=1)\n",
      "                      (1): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)\n",
      "                    )\n",
      "                    net=VisualCore(\n",
      "                      input_shape=[3, 116, 116]\n",
      "                      output_shape=[512]\n",
      "                      backbone_net=ResNet50Conv(input_channel=3, input_coord_conv=False)\n",
      "                      pool_net=None\n",
      "                    )\n",
      "                    sharing_from=camera/image/varied_camera_1_left_image\n",
      "                )\n",
      "                output_shape=[1033]\n",
      "            )\n",
      "        )\n",
      "      )\n",
      "      (noise_pred_net): DataParallel(\n",
      "        (module): ConditionalUnet1D(\n",
      "          (mid_modules): ModuleList(\n",
      "            (0-1): 2 x ConditionalResidualBlock1D(\n",
      "              (blocks): ModuleList(\n",
      "                (0-1): 2 x Conv1dBlock(\n",
      "                  (block): Sequential(\n",
      "                    (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                    (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "                    (2): Mish()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (cond_encoder): Sequential(\n",
      "                (0): Mish()\n",
      "                (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "                (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "              )\n",
      "              (residual_conv): Identity()\n",
      "            )\n",
      "          )\n",
      "          (diffusion_step_encoder): Sequential(\n",
      "            (0): SinusoidalPosEmb()\n",
      "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (2): Mish()\n",
      "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "          (up_modules): ModuleList(\n",
      "            (0): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(2048, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Upsample1d(\n",
      "                (conv): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "              )\n",
      "            )\n",
      "            (1): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(1024, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Upsample1d(\n",
      "                (conv): ConvTranspose1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (down_modules): ModuleList(\n",
      "            (0): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(11, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(11, 256, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Downsample1d(\n",
      "                (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "              )\n",
      "            )\n",
      "            (1): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Downsample1d(\n",
      "                (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "              )\n",
      "            )\n",
      "            (2): ModuleList(\n",
      "              (0): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                  (1): Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "              (1): ConditionalResidualBlock1D(\n",
      "                (blocks): ModuleList(\n",
      "                  (0-1): 2 x Conv1dBlock(\n",
      "                    (block): Sequential(\n",
      "                      (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
      "                      (2): Mish()\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (cond_encoder): Sequential(\n",
      "                  (0): Mish()\n",
      "                  (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
      "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
      "                )\n",
      "                (residual_conv): Identity()\n",
      "              )\n",
      "              (2): Identity()\n",
      "            )\n",
      "          )\n",
      "          (final_conv): Sequential(\n",
      "            (0): Conv1dBlock(\n",
      "              (block): Sequential(\n",
      "                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "                (2): Mish()\n",
      "              )\n",
      "            )\n",
      "            (1): Conv1d(256, 11, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = TorchUtils.get_torch_device(try_to_use_cuda=True)\n",
    "\n",
    "# restore policy\n",
    "policy, ckpt_dict = FileUtils.policy_from_checkpoint(ckpt_path=dg, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872a3f0",
   "metadata": {},
   "source": [
    "### Creating rollout envionment\n",
    "The policy checkpoint also contains sufficient information to recreate the environment that it's trained with. Again, you may manually create the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6291d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"C:/Users/willi/tensorflow_datasets\"    # UPDATE WITH PATH TO RLDS DATASETS\n",
    "DATASET_NAME = \"deligrasp_dataset\"\n",
    "EXP_LOG_PATH = \"C:/workspace/deligrasp_policy_learning/logs\" # UPDATE WITH PATH TO DESIRED LOGGING DIRECTORY\n",
    "sample_weights = [1]\n",
    "\n",
    "BASE_DATASET_KWARGS = {\n",
    "    \"name\": DATASET_NAME,\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"image_obs_keys\": {\"primary\": \"image\", \"secondary\": \"wrist_image\"},\n",
    "    \"state_obs_keys\": [\"cartesian_position\", \"gripper_position\", \"applied_force\", \"contact_force\"],\n",
    "    # \"state_obs_keys\": [\"state\"], # this makes [\"observation\"]['proprio'].shape len 16\n",
    "    \"language_key\": \"language_instruction\",\n",
    "    \"norm_skip_keys\":  [\"proprio\"],\n",
    "    \"action_proprio_normalization_type\": \"bounds\",\n",
    "    \"absolute_action_mask\": [False] * 11,                    # droid_dataset_transform uses absolute actions\n",
    "    \"action_normalization_mask\": [False] * 11,      # don't normalize final (gripper) dimension\n",
    "    \"standardize_fn\": dg_dataset_transform,\n",
    "}\n",
    "\n",
    "dataset = make_single_dataset(\n",
    "    BASE_DATASET_KWARGS,\n",
    "    train=True,\n",
    "    traj_transform_kwargs=dict(\n",
    "        window_size=2,\n",
    "        future_action_window_size=15,\n",
    "        subsample_length=50,\n",
    "        skip_unlabeled=True,            # skip all trajectories without language annotation\n",
    "    ),\n",
    "    frame_transform_kwargs=dict(\n",
    "        image_augment_kwargs=dict(\n",
    "        ),\n",
    "        resize_size=dict(\n",
    "            primary=[128, 128],\n",
    "            secondary=[128, 128],\n",
    "        ),\n",
    "        num_parallel_calls=200,\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = dataset.map(robomimic_dg_transform, num_parallel_calls=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3703106",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = None\n",
    "for i in dataset:\n",
    "    episode = i\n",
    "    break\n",
    "obs = episode[\"obs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a308a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs\n",
    "# get first value of each key, that is the first observation step\n",
    "obs_iter = [{k: obs[k][i] for k in obs.keys()} for i in range(len(obs['robot_state/applied_force']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1d9c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = episode[\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e37536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "4\n",
      "(2, 3, 128, 128)\n",
      "tf.Tensor(b'grasp scallion stalk and return to original position', shape=(), dtype=string)\n",
      "2\n",
      "(2, 6)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n",
      "2\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "obs_iter = []\n",
    "for i in range(len(obs['robot_state/applied_force'])):\n",
    "    dict = {}\n",
    "    for k in obs.keys():\n",
    "        skip_keys = [\"raw\"]\n",
    "        if True in [sk in k for sk in skip_keys]:\n",
    "            print(obs[k][i])\n",
    "            continue\n",
    "        # o = obs[k][i][0] # tensorflow eagertensor with dimension [T, ...]\n",
    "        o = obs[k][i] # tensorflow eagertensor with dimension [T, ...]\n",
    "        if \"image\" in k:\n",
    "            # o = o[k][i][0] # tensorflow eagertensor with dimension [T, ...]\n",
    "            # o = o[0] # tensorflow eagertensor with dimension [T, ...]\n",
    "            # currently in (H, W, C) format, need to change to (C, H, W)\n",
    "            # o = np.transpose(o, (2, 0, 1))\n",
    "            o = np.transpose(o, (0, 3, 1, 2))\n",
    "        else:\n",
    "            o = np.array(o)\n",
    "        # if \"image\" not in k:\n",
    "            # o = obs[k][i][0]\n",
    "        # add dimension 1, so that it is [B, T, ...]\n",
    "        # o = torch.from_numpy(np.expand_dims(o, axis=0))\n",
    "        print(o.ndim)\n",
    "        print(o.shape)\n",
    "        # dict[k] = obs[k][i]\n",
    "        dict[k] = o\n",
    "    obs_iter.append(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27437985",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.start_episode()\n",
    "policy.goal_mode = None\n",
    "policy.action_queue = None\n",
    "policy.eval_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4374e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "DP: after time_distributed\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "unnormalized action: {'action/rel_pos': array([-0.03846476, -0.01724374,  0.17625722], dtype=float32), 'action/rel_rot_6d': array([ 0.9646064 , -0.06793312,  0.01329179, -0.03624332,  0.9043823 ,\n",
      "       -0.01418067], dtype=float32), 'action/gripper_position': array([0.09562177], dtype=float32), 'action/gripper_force': array([-0.00977834], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.03846476, -0.01724374,  0.17625722]), 'action/rel_rot_6d': array([ 0.9646064 , -0.06793312,  0.01329179, -0.03624332,  0.90438229,\n",
      "       -0.01418067]), 'action/gripper_position': array([-0.39542635]), 'action/gripper_force': array([0.16041591])}\n",
      "{'action/rel_pos': array([-0.03846476, -0.01724374,  0.17625722]), 'action/rel_rot_6d': array([0.01516939, 0.01270895, 0.0705039 ], dtype=float32), 'action/gripper_position': array([-0.39542635]), 'action/gripper_force': array([0.16041591])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03846476, -0.01724374,  0.17625722,  0.01516939,  0.01270895,\n",
       "        0.0705039 , -0.39542635,  0.16041591])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy(obs_iter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4f18e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obs_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19a20a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep 0\n",
      "unnormalized action: {'action/rel_pos': array([0.03136781, 0.01407079, 0.01451293], dtype=float32), 'action/rel_rot_6d': array([0.9206735 , 0.05531171, 0.00120147, 0.0439284 , 0.97378135,\n",
      "       0.03935396], dtype=float32), 'action/gripper_position': array([-0.02126858], dtype=float32), 'action/gripper_force': array([0.02261909], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.03136781, 0.01407079, 0.01451293]), 'action/rel_rot_6d': array([0.92067349, 0.05531171, 0.00120147, 0.0439284 , 0.97378135,\n",
      "       0.03935396]), 'action/gripper_position': array([-0.44858327]), 'action/gripper_force': array([0.1656643])}\n",
      "{'action/rel_pos': array([0.03136781, 0.01407079, 0.01451293]), 'action/rel_rot_6d': array([-0.04044228, -0.00112509, -0.0600089 ], dtype=float32), 'action/gripper_position': array([-0.44858327]), 'action/gripper_force': array([0.1656643])}\n",
      "[ 0.03136781  0.01407079  0.01451293 -0.04044228 -0.00112509 -0.0600089\n",
      " -0.44858327  0.1656643 ]\n",
      "Ground Truth\n",
      "[ 2.2000000e-05 -1.9999999e-05 -1.3000000e-05  1.0000000e+00\n",
      "  3.4000495e-05 -1.5998947e-05 -3.4000001e-05  1.0000000e+00\n",
      "  3.1000545e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 1\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "DP: after time_distributed\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "unnormalized action: {'action/rel_pos': array([-4.4393833e-05,  3.5749243e-03,  1.3732961e-03], dtype=float32), 'action/rel_rot_6d': array([ 1.00000000e+00, -1.11041954e-04, -2.02552648e-03, -4.53810254e-03,\n",
      "        1.00000000e+00, -5.66150877e-04], dtype=float32), 'action/gripper_position': array([-0.00755648], dtype=float32), 'action/gripper_force': array([0.00176529], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-4.43938334e-05,  3.57492431e-03,  1.37329614e-03]), 'action/rel_rot_6d': array([ 1.00000000e+00, -1.11041954e-04, -2.02552648e-03, -4.53810254e-03,\n",
      "        1.00000000e+00, -5.66150877e-04]), 'action/gripper_position': array([-0.44234757]), 'action/gripper_force': array([0.16228598])}\n",
      "{'action/rel_pos': array([-4.43938334e-05,  3.57492431e-03,  1.37329614e-03]), 'action/rel_rot_6d': array([ 0.00057534, -0.00202559,  0.00010988], dtype=float32), 'action/gripper_position': array([-0.44234757]), 'action/gripper_force': array([0.16228598])}\n",
      "[-4.43938334e-05  3.57492431e-03  1.37329614e-03  5.75343147e-04\n",
      " -2.02558725e-03  1.09876339e-04 -4.42347574e-01  1.62285977e-01]\n",
      "Ground Truth\n",
      "[-3.7999998e-05 -2.4000001e-05 -1.5000001e-04  1.0000000e+00\n",
      "  1.0399945e-04  2.6002184e-05 -1.0400000e-04  1.0000000e+00\n",
      "  2.0997295e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 2\n",
      "unnormalized action: {'action/rel_pos': array([-0.00011569, -0.00362624, -0.00082975], dtype=float32), 'action/rel_rot_6d': array([ 1.0000000e+00, -1.4436165e-03, -2.9736171e-03,  6.2452600e-04,\n",
      "        1.0000000e+00, -1.0809129e-03], dtype=float32), 'action/gripper_position': array([-0.00992943], dtype=float32), 'action/gripper_force': array([-0.00382893], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00011569, -0.00362624, -0.00082975]), 'action/rel_rot_6d': array([ 1.00000000e+00, -1.44361646e-03, -2.97361705e-03,  6.24526001e-04,\n",
      "        1.00000000e+00, -1.08091289e-03]), 'action/gripper_position': array([-0.4434267]), 'action/gripper_force': array([0.16137971])}\n",
      "{'action/rel_pos': array([-0.00011569, -0.00362624, -0.00082975]), 'action/rel_rot_6d': array([ 0.00107905, -0.00297516,  0.0014404 ], dtype=float32), 'action/gripper_position': array([-0.4434267]), 'action/gripper_force': array([0.16137971])}\n",
      "[-1.15685361e-04 -3.62623599e-03 -8.29753699e-04  1.07905443e-03\n",
      " -2.97516421e-03  1.44039956e-03 -4.43426698e-01  1.61379713e-01]\n",
      "Ground Truth\n",
      "[-9.1000002e-05 -2.8000000e-05 -2.8199999e-04  9.9999994e-01\n",
      " -1.8901730e-04  2.0598412e-04  1.8900000e-04  1.0000000e+00\n",
      "  8.4038933e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 3\n",
      "unnormalized action: {'action/rel_pos': array([-0.00209188,  0.00060277, -0.00195692], dtype=float32), 'action/rel_rot_6d': array([ 1.0000000e+00,  1.0060528e-03,  2.6406348e-04, -1.1351190e-04,\n",
      "        9.9634123e-01,  1.2401681e-04], dtype=float32), 'action/gripper_position': array([-0.00808861], dtype=float32), 'action/gripper_force': array([-0.00056822], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00209188,  0.00060277, -0.00195692]), 'action/rel_rot_6d': array([ 1.00000000e+00,  1.00605283e-03,  2.64063478e-04, -1.13511902e-04,\n",
      "        9.96341228e-01,  1.24016806e-04]), 'action/gripper_position': array([-0.44258956]), 'action/gripper_force': array([0.16190795])}\n",
      "{'action/rel_pos': array([-0.00209188,  0.00060277, -0.00195692]), 'action/rel_rot_6d': array([-0.0001245 ,  0.00026394, -0.00100609], dtype=float32), 'action/gripper_position': array([-0.44258956]), 'action/gripper_force': array([0.16190795])}\n",
      "[-2.09187856e-03  6.02770888e-04 -1.95692410e-03 -1.24502287e-04\n",
      "  2.63938215e-04 -1.00608531e-03 -4.42589565e-01  1.61907949e-01]\n",
      "Ground Truth\n",
      "[-9.9999997e-06 -1.3000000e-05 -1.7499999e-04  1.0000000e+00\n",
      " -4.6985573e-05  2.2900295e-04  4.7000001e-05  1.0000000e+00\n",
      " -6.2989238e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 4\n",
      "unnormalized action: {'action/rel_pos': array([3.1060099e-06, 2.7071766e-03, 1.5795225e-03], dtype=float32), 'action/rel_rot_6d': array([ 9.9662668e-01, -9.6335221e-04, -2.5997434e-03,  1.7371288e-03,\n",
      "        9.9543041e-01,  9.1266201e-04], dtype=float32), 'action/gripper_position': array([-0.01005869], dtype=float32), 'action/gripper_force': array([9.5142284e-05], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([3.10600990e-06, 2.70717661e-03, 1.57952250e-03]), 'action/rel_rot_6d': array([ 9.96626675e-01, -9.63352213e-04, -2.59974343e-03,  1.73712883e-03,\n",
      "        9.95430410e-01,  9.12662013e-04]), 'action/gripper_position': array([-0.44348548]), 'action/gripper_force': array([0.16201541])}\n",
      "{'action/rel_pos': array([3.10600990e-06, 2.70717661e-03, 1.57952250e-03]), 'action/rel_rot_6d': array([-0.0009214 , -0.00260765,  0.00096901], dtype=float32), 'action/gripper_position': array([-0.44348548]), 'action/gripper_force': array([0.16201541])}\n",
      "[ 3.10600990e-06  2.70717661e-03  1.57952250e-03 -9.21402010e-04\n",
      " -2.60764523e-03  9.69012443e-04 -4.43485479e-01  1.62015414e-01]\n",
      "Ground Truth\n",
      "[-4.0999999e-05 -4.7000001e-05  2.0000000e-06  1.0000000e+00\n",
      " -6.7010944e-05 -2.7973801e-05  6.7000001e-05  9.9999994e-01\n",
      " -3.9100187e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 5\n",
      "unnormalized action: {'action/rel_pos': array([-0.00031066, -0.00288221, -0.00287942], dtype=float32), 'action/rel_rot_6d': array([ 9.98231471e-01,  1.04478015e-04,  3.16657987e-03,  2.91585014e-03,\n",
      "        9.93965566e-01, -5.00883689e-05], dtype=float32), 'action/gripper_position': array([-0.0139858], dtype=float32), 'action/gripper_force': array([0.0027733], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00031066, -0.00288221, -0.00287942]), 'action/rel_rot_6d': array([ 9.98231471e-01,  1.04478015e-04,  3.16657987e-03,  2.91585014e-03,\n",
      "        9.93965566e-01, -5.00883689e-05]), 'action/gripper_position': array([-0.44527137]), 'action/gripper_force': array([0.16244928])}\n",
      "{'action/rel_pos': array([-0.00031066, -0.00288221, -0.00287942]), 'action/rel_rot_6d': array([ 5.9698265e-05,  3.1721855e-03, -1.0447321e-04], dtype=float32), 'action/gripper_position': array([-0.44527137]), 'action/gripper_force': array([0.16244928])}\n",
      "[-3.10658768e-04 -2.88220588e-03 -2.87942216e-03  5.96982645e-05\n",
      "  3.17218550e-03 -1.04473213e-04 -4.45271368e-01  1.62449276e-01]\n",
      "Ground Truth\n",
      "[ 1.0000000e-06 -3.7000002e-05  7.8999998e-05  1.0000000e+00\n",
      " -3.5945424e-05  1.8501061e-04  3.6000001e-05  9.9999994e-01\n",
      " -2.9499334e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 6\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00165228, -0.00157802,  0.0018748 ], dtype=float32), 'action/rel_rot_6d': array([ 9.9507952e-01,  3.6858818e-03,  1.3384089e-03, -2.5234644e-03,\n",
      "        9.9706656e-01, -2.4927361e-04], dtype=float32), 'action/gripper_position': array([-0.01152527], dtype=float32), 'action/gripper_force': array([-0.00243807], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00165228, -0.00157802,  0.0018748 ]), 'action/rel_rot_6d': array([ 9.95079517e-01,  3.68588185e-03,  1.33840891e-03, -2.52346438e-03,\n",
      "        9.97066557e-01, -2.49273609e-04]), 'action/gripper_position': array([-0.44415242]), 'action/gripper_force': array([0.16160503])}\n",
      "{'action/rel_pos': array([ 0.00165228, -0.00157802,  0.0018748 ]), 'action/rel_rot_6d': array([ 0.0002466 ,  0.00134594, -0.00370376], dtype=float32), 'action/gripper_position': array([-0.44415242]), 'action/gripper_force': array([0.16160503])}\n",
      "[ 1.65227626e-03 -1.57802075e-03  1.87479658e-03  2.46600568e-04\n",
      "  1.34593970e-03 -3.70375579e-03 -4.44152421e-01  1.61605033e-01]\n",
      "Ground Truth\n",
      "[-1.8999999e-05 -1.3000000e-05  1.0400000e-04  1.0000000e+00\n",
      " -3.9706238e-06  1.0200115e-04  4.0000000e-06  9.9999994e-01\n",
      " -2.8799960e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 7\n",
      "unnormalized action: {'action/rel_pos': array([-0.00078952,  0.00056553,  0.00328999], dtype=float32), 'action/rel_rot_6d': array([ 0.9964683 , -0.00356613,  0.00168384,  0.00407853,  0.9952591 ,\n",
      "       -0.00200815], dtype=float32), 'action/gripper_position': array([-0.0075403], dtype=float32), 'action/gripper_force': array([-0.00111617], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00078952,  0.00056553,  0.00328999]), 'action/rel_rot_6d': array([ 0.99646831, -0.00356613,  0.00168384,  0.00407853,  0.99525911,\n",
      "       -0.00200815]), 'action/gripper_position': array([-0.44234022]), 'action/gripper_force': array([0.16181918])}\n",
      "{'action/rel_pos': array([-0.00078952,  0.00056553,  0.00328999]), 'action/rel_rot_6d': array([0.00202461, 0.00168256, 0.00358216], dtype=float32), 'action/gripper_position': array([-0.44234022]), 'action/gripper_force': array([0.16181918])}\n",
      "[-0.00078952  0.00056553  0.00328999  0.00202461  0.00168256  0.00358216\n",
      " -0.44234022  0.16181918]\n",
      "Ground Truth\n",
      "[-6.1999999e-05  2.2000000e-05  2.6000000e-05  1.0000000e+00\n",
      " -9.9983527e-06 -6.1000268e-05  9.9999997e-06  1.0000000e+00\n",
      "  2.6999391e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 8\n",
      "unnormalized action: {'action/rel_pos': array([-0.00028836, -0.00053094, -0.00153212], dtype=float32), 'action/rel_rot_6d': array([ 9.9713516e-01, -9.0185151e-04,  2.0133280e-03,  4.8341404e-04,\n",
      "        9.9596530e-01, -2.1545941e-04], dtype=float32), 'action/gripper_position': array([-0.00843111], dtype=float32), 'action/gripper_force': array([-0.00183565], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00028836, -0.00053094, -0.00153212]), 'action/rel_rot_6d': array([ 9.97135162e-01, -9.01851512e-04,  2.01332802e-03,  4.83414042e-04,\n",
      "        9.95965302e-01, -2.15459411e-04]), 'action/gripper_position': array([-0.44274532]), 'action/gripper_force': array([0.16170262])}\n",
      "{'action/rel_pos': array([-0.00028836, -0.00053094, -0.00153212]), 'action/rel_rot_6d': array([0.00021731, 0.00201891, 0.00090488], dtype=float32), 'action/gripper_position': array([-0.44274532]), 'action/gripper_force': array([0.16170262])}\n",
      "[-2.88358657e-04 -5.30944671e-04 -1.53211516e-03  2.17312176e-04\n",
      "  2.01891316e-03  9.04879242e-04 -4.42745321e-01  1.61702625e-01]\n",
      "Ground Truth\n",
      "[-7.8999998e-05  8.0000000e-06  1.0000000e-06  1.0000000e+00\n",
      "  1.3099995e-04 -4.7000132e-05 -1.3099999e-04  1.0000000e+00\n",
      " -9.9384295e-07  0.0000000e+00  0.0000000e+00]\n",
      "timestep 9\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "DP: after time_distributed\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00108203, -0.01284578,  0.00130382], dtype=float32), 'action/rel_rot_6d': array([ 0.12332498, -0.01497489,  0.00848533, -0.00766693,  0.11028447,\n",
      "        0.00372818], dtype=float32), 'action/gripper_position': array([-0.01753219], dtype=float32), 'action/gripper_force': array([0.00139774], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00108203, -0.01284578,  0.00130382]), 'action/rel_rot_6d': array([ 0.12332498, -0.01497489,  0.00848533, -0.00766693,  0.11028447,\n",
      "        0.00372818]), 'action/gripper_position': array([-0.44688412]), 'action/gripper_force': array([0.16222644])}\n",
      "{'action/rel_pos': array([ 0.00108203, -0.01284578,  0.00130382]), 'action/rel_rot_6d': array([-0.03889731,  0.07334278,  0.11779195], dtype=float32), 'action/gripper_position': array([-0.44688412]), 'action/gripper_force': array([0.16222644])}\n",
      "[ 0.00108203 -0.01284578  0.00130382 -0.03889731  0.07334278  0.11779195\n",
      " -0.44688412  0.16222644]\n",
      "Ground Truth\n",
      "[ 2.7000000e-05 -2.2000000e-05 -6.0000002e-06  1.0000000e+00\n",
      " -1.0001201e-06  1.4999991e-05  1.0000000e-06  1.0000000e+00\n",
      "  8.0000145e-06  0.0000000e+00  0.0000000e+00]\n",
      "timestep 10\n",
      "unnormalized action: {'action/rel_pos': array([-9.9767763e-05, -4.7023753e-03, -6.0670716e-03], dtype=float32), 'action/rel_rot_6d': array([ 0.07413903,  0.00664342, -0.00134843,  0.00688335,  0.0751806 ,\n",
      "       -0.00120631], dtype=float32), 'action/gripper_position': array([0.00209574], dtype=float32), 'action/gripper_force': array([0.00427904], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-9.97677635e-05, -4.70237527e-03, -6.06707158e-03]), 'action/rel_rot_6d': array([ 0.07413903,  0.00664342, -0.00134843,  0.00688335,  0.0751806 ,\n",
      "       -0.00120631]), 'action/gripper_position': array([-0.43795814]), 'action/gripper_force': array([0.16269321])}\n",
      "{'action/rel_pos': array([-9.97677635e-05, -4.70237527e-03, -6.06707158e-03]), 'action/rel_rot_6d': array([ 0.01449821, -0.01688523, -0.08960846], dtype=float32), 'action/gripper_position': array([-0.43795814]), 'action/gripper_force': array([0.16269321])}\n",
      "[-9.97677635e-05 -4.70237527e-03 -6.06707158e-03  1.44982105e-02\n",
      " -1.68852285e-02 -8.96084607e-02 -4.37958143e-01  1.62693206e-01]\n",
      "Ground Truth\n",
      "[ 4.2000000e-05 -2.9999999e-05 -1.1000000e-05  1.0000000e+00\n",
      " -1.1999216e-05  4.9000191e-05  1.2000000e-05  1.0000000e+00\n",
      " -1.5999412e-05 -8.4135777e-01  4.9049999e-03]\n",
      "timestep 11\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00680346,  0.00130565, -0.00495024], dtype=float32), 'action/rel_rot_6d': array([ 0.05747729,  0.00284341,  0.00071353, -0.00508283,  0.06496143,\n",
      "        0.0024225 ], dtype=float32), 'action/gripper_position': array([-0.01238566], dtype=float32), 'action/gripper_force': array([0.00312473], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00680346,  0.00130565, -0.00495024]), 'action/rel_rot_6d': array([ 0.05747729,  0.00284341,  0.00071353, -0.00508283,  0.06496143,\n",
      "        0.0024225 ]), 'action/gripper_position': array([-0.44454369]), 'action/gripper_force': array([0.16250621])}\n",
      "{'action/rel_pos': array([ 0.00680346,  0.00130565, -0.00495024]), 'action/rel_rot_6d': array([-0.03809673,  0.01052046, -0.04986302], dtype=float32), 'action/gripper_position': array([-0.44454369]), 'action/gripper_force': array([0.16250621])}\n",
      "[ 0.00680346  0.00130565 -0.00495024 -0.03809673  0.01052046 -0.04986302\n",
      " -0.44454369  0.16250621]\n",
      "Ground Truth\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "timestep 12\n",
      "unnormalized action: {'action/rel_pos': array([ 0.0036776 ,  0.00542384, -0.00033722], dtype=float32), 'action/rel_rot_6d': array([ 0.04752556,  0.0027222 , -0.0051335 ,  0.00252379,  0.05710148,\n",
      "        0.00014115], dtype=float32), 'action/gripper_position': array([0.00021888], dtype=float32), 'action/gripper_force': array([0.00285603], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.0036776 ,  0.00542384, -0.00033722]), 'action/rel_rot_6d': array([ 0.04752556,  0.0027222 , -0.0051335 ,  0.00252379,  0.05710148,\n",
      "        0.00014115]), 'action/gripper_position': array([-0.43881166]), 'action/gripper_force': array([0.16246268])}\n",
      "{'action/rel_pos': array([ 0.0036776 ,  0.00542384, -0.00033722]), 'action/rel_rot_6d': array([-0.00726437, -0.10800688, -0.05610444], dtype=float32), 'action/gripper_position': array([-0.43881166]), 'action/gripper_force': array([0.16246268])}\n",
      "[ 3.67760286e-03  5.42384060e-03 -3.37215315e-04 -7.26437429e-03\n",
      " -1.08006880e-01 -5.61044402e-02 -4.38811661e-01  1.62462678e-01]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00228118  0.        ]\n",
      "timestep 13\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00510863, -0.00118639, -0.0016785 ], dtype=float32), 'action/rel_rot_6d': array([ 0.04872794,  0.00245725,  0.00116643,  0.00242995,  0.05035554,\n",
      "       -0.00338438], dtype=float32), 'action/gripper_position': array([-0.00370988], dtype=float32), 'action/gripper_force': array([-0.00036271], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00510863, -0.00118639, -0.0016785 ]), 'action/rel_rot_6d': array([ 0.04872794,  0.00245725,  0.00116643,  0.00242995,  0.05035554,\n",
      "       -0.00338438]), 'action/gripper_position': array([-0.4405983]), 'action/gripper_force': array([0.16194124])}\n",
      "{'action/rel_pos': array([ 0.00510863, -0.00118639, -0.0016785 ]), 'action/rel_rot_6d': array([ 0.06842461,  0.02732261, -0.04861673], dtype=float32), 'action/gripper_position': array([-0.4405983]), 'action/gripper_force': array([0.16194124])}\n",
      "[ 0.00510863 -0.00118639 -0.0016785   0.06842461  0.02732261 -0.04861673\n",
      " -0.4405983   0.16194124]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00227952  0.        ]\n",
      "timestep 14\n",
      "unnormalized action: {'action/rel_pos': array([-0.00124571, -0.00194937,  0.00179279], dtype=float32), 'action/rel_rot_6d': array([ 0.02544958,  0.0018007 ,  0.00120422,  0.00218776,  0.02884383,\n",
      "       -0.00216274], dtype=float32), 'action/gripper_position': array([-0.00200807], dtype=float32), 'action/gripper_force': array([0.00438011], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00124571, -0.00194937,  0.00179279]), 'action/rel_rot_6d': array([ 0.02544958,  0.0018007 ,  0.00120422,  0.00218776,  0.02884383,\n",
      "       -0.00216274]), 'action/gripper_position': array([-0.43982439]), 'action/gripper_force': array([0.16270958])}\n",
      "{'action/rel_pos': array([-0.00124571, -0.00194937,  0.00179279]), 'action/rel_rot_6d': array([ 0.07883016,  0.05269401, -0.06661829], dtype=float32), 'action/gripper_position': array([-0.43982439]), 'action/gripper_force': array([0.16270958])}\n",
      "[-0.00124571 -0.00194937  0.00179279  0.07883016  0.05269401 -0.06661829\n",
      " -0.43982439  0.16270958]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00455384  0.        ]\n",
      "timestep 15\n",
      "unnormalized action: {'action/rel_pos': array([-0.00200215, -0.00096506, -0.00157533], dtype=float32), 'action/rel_rot_6d': array([ 0.02344345,  0.00213514, -0.00126135, -0.00099141,  0.02295176,\n",
      "       -0.00108417], dtype=float32), 'action/gripper_position': array([-0.00247334], dtype=float32), 'action/gripper_force': array([-0.0018934], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00200215, -0.00096506, -0.00157533]), 'action/rel_rot_6d': array([ 0.02344345,  0.00213514, -0.00126135, -0.00099141,  0.02295176,\n",
      "       -0.00108417]), 'action/gripper_position': array([-0.44003597]), 'action/gripper_force': array([0.16169327])}\n",
      "{'action/rel_pos': array([-0.00200215, -0.00096506, -0.00157533]), 'action/rel_rot_6d': array([ 0.0493269 , -0.04920797, -0.09323384], dtype=float32), 'action/gripper_position': array([-0.44003597]), 'action/gripper_force': array([0.16169327])}\n",
      "[-0.00200215 -0.00096506 -0.00157533  0.0493269  -0.04920797 -0.09323384\n",
      " -0.44003597  0.16169327]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00227419  0.        ]\n",
      "timestep 16\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00254538, -0.00321395, -0.00230575], dtype=float32), 'action/rel_rot_6d': array([ 0.02309731, -0.00346318,  0.00063934, -0.00413463,  0.02099993,\n",
      "       -0.00209004], dtype=float32), 'action/gripper_position': array([-0.00324032], dtype=float32), 'action/gripper_force': array([0.00244506], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00254538, -0.00321395, -0.00230575]), 'action/rel_rot_6d': array([ 0.02309731, -0.00346318,  0.00063934, -0.00413463,  0.02099993,\n",
      "       -0.00209004]), 'action/gripper_position': array([-0.44038477]), 'action/gripper_force': array([0.1623961])}\n",
      "{'action/rel_pos': array([ 0.00254538, -0.00321395, -0.00230575]), 'action/rel_rot_6d': array([0.09663588, 0.01308328, 0.1507445 ], dtype=float32), 'action/gripper_position': array([-0.44038477]), 'action/gripper_force': array([0.1623961])}\n",
      "[ 0.00254538 -0.00321395 -0.00230575  0.09663588  0.01308328  0.1507445\n",
      " -0.44038477  0.1623961 ]\n",
      "Ground Truth\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "timestep 17\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "DP: after time_distributed\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "unnormalized action: {'action/rel_pos': array([-0.00469413,  0.02156768, -0.00889357], dtype=float32), 'action/rel_rot_6d': array([ 0.98617727,  0.0379808 , -0.00268013,  0.02805261,  0.9970837 ,\n",
      "       -0.01034067], dtype=float32), 'action/gripper_position': array([0.00405478], dtype=float32), 'action/gripper_force': array([-0.01558228], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00469413,  0.02156768, -0.00889357]), 'action/rel_rot_6d': array([ 0.98617727,  0.0379808 , -0.00268013,  0.02805261,  0.99708372,\n",
      "       -0.01034067]), 'action/gripper_position': array([-0.43706725]), 'action/gripper_force': array([0.15947567])}\n",
      "{'action/rel_pos': array([-0.00469413,  0.02156768, -0.00889357]), 'action/rel_rot_6d': array([ 0.01030525, -0.00232067, -0.03851995], dtype=float32), 'action/gripper_position': array([-0.43706725]), 'action/gripper_force': array([0.15947567])}\n",
      "[-0.00469413  0.02156768 -0.00889357  0.01030525 -0.00232067 -0.03851995\n",
      " -0.43706725  0.15947567]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.01138122  0.        ]\n",
      "timestep 18\n",
      "unnormalized action: {'action/rel_pos': array([ 0.03274995, -0.01552227,  0.01466934], dtype=float32), 'action/rel_rot_6d': array([ 9.7899956e-01,  6.9078053e-03,  4.6435852e-02, -1.9926466e-02,\n",
      "        9.8872852e-01, -2.8901451e-04], dtype=float32), 'action/gripper_position': array([0.01057769], dtype=float32), 'action/gripper_force': array([0.00190623], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.03274995, -0.01552227,  0.01466934]), 'action/rel_rot_6d': array([ 9.78999555e-01,  6.90780533e-03,  4.64358516e-02, -1.99264660e-02,\n",
      "        9.88728523e-01, -2.89014512e-04]), 'action/gripper_position': array([-0.4341009]), 'action/gripper_force': array([0.16230881])}\n",
      "{'action/rel_pos': array([ 0.03274995, -0.01552227,  0.01466934]), 'action/rel_rot_6d': array([-0.00066352,  0.04739174, -0.00707938], dtype=float32), 'action/gripper_position': array([-0.4341009]), 'action/gripper_force': array([0.16230881])}\n",
      "[ 0.03274995 -0.01552227  0.01466934 -0.00066352  0.04739174 -0.00707938\n",
      " -0.4341009   0.16230881]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00454911  0.        ]\n",
      "timestep 19\n",
      "unnormalized action: {'action/rel_pos': array([ 0.0083087 , -0.00114372,  0.0127734 ], dtype=float32), 'action/rel_rot_6d': array([ 9.9132550e-01, -1.8778073e-02,  9.7987114e-04,  1.1639816e-02,\n",
      "        9.7443992e-01,  3.1949538e-03], dtype=float32), 'action/gripper_position': array([-0.01964646], dtype=float32), 'action/gripper_force': array([0.00477441], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.0083087 , -0.00114372,  0.0127734 ]), 'action/rel_rot_6d': array([ 9.91325498e-01, -1.87780727e-02,  9.79871140e-04,  1.16398158e-02,\n",
      "        9.74439919e-01,  3.19495378e-03]), 'action/gripper_position': array([-0.4478456]), 'action/gripper_force': array([0.16277346])}\n",
      "{'action/rel_pos': array([ 0.0083087 , -0.00114372,  0.0127734 ]), 'action/rel_rot_6d': array([-0.0032662 ,  0.00105031,  0.01893678], dtype=float32), 'action/gripper_position': array([-0.4478456]), 'action/gripper_force': array([0.16277346])}\n",
      "[ 0.0083087  -0.00114372  0.0127734  -0.0032662   0.00105031  0.01893678\n",
      " -0.4478456   0.16277346]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -4.54538e-03\n",
      "  5.00000e-04]\n",
      "timestep 20\n",
      "unnormalized action: {'action/rel_pos': array([ 0.01546126, -0.01405823,  0.03219021], dtype=float32), 'action/rel_rot_6d': array([ 0.9894736 , -0.00110244,  0.00574361, -0.00716512,  0.98487073,\n",
      "        0.01936765], dtype=float32), 'action/gripper_position': array([-0.05406211], dtype=float32), 'action/gripper_force': array([-0.01654249], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.01546126, -0.01405823,  0.03219021]), 'action/rel_rot_6d': array([ 0.98947358, -0.00110244,  0.00574361, -0.00716512,  0.98487073,\n",
      "        0.01936765]), 'action/gripper_position': array([-0.46349643]), 'action/gripper_force': array([0.15932011])}\n",
      "{'action/rel_pos': array([ 0.01546126, -0.01405823,  0.03219021]), 'action/rel_rot_6d': array([-0.01970501,  0.00582548,  0.00099956], dtype=float32), 'action/gripper_position': array([-0.46349643]), 'action/gripper_force': array([0.15932011])}\n",
      "[ 0.01546126 -0.01405823  0.03219021 -0.01970501  0.00582548  0.00099956\n",
      " -0.46349643  0.15932011]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -6.81943e-03\n",
      "  5.00000e-04]\n",
      "timestep 21\n",
      "unnormalized action: {'action/rel_pos': array([ 0.01539953,  0.02078281, -0.03585862], dtype=float32), 'action/rel_rot_6d': array([ 0.9725703 , -0.01536214,  0.02000624,  0.00480478,  0.9716703 ,\n",
      "       -0.01543047], dtype=float32), 'action/gripper_position': array([0.00198279], dtype=float32), 'action/gripper_force': array([-0.00112808], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.01539953,  0.02078281, -0.03585862]), 'action/rel_rot_6d': array([ 0.9725703 , -0.01536214,  0.02000624,  0.00480478,  0.97167033,\n",
      "       -0.01543047]), 'action/gripper_position': array([-0.43800951]), 'action/gripper_force': array([0.16181725])}\n",
      "{'action/rel_pos': array([ 0.01539953,  0.02078281, -0.03585862]), 'action/rel_rot_6d': array([0.01597946, 0.02031267, 0.01611735], dtype=float32), 'action/gripper_position': array([-0.43800951]), 'action/gripper_force': array([0.16181725])}\n",
      "[ 0.01539953  0.02078281 -0.03585862  0.01597946  0.02031267  0.01611735\n",
      " -0.43800951  0.16181725]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -2.25968e-03\n",
      "  5.00000e-04]\n",
      "timestep 22\n",
      "unnormalized action: {'action/rel_pos': array([-0.00146823, -0.01198717,  0.0084354 ], dtype=float32), 'action/rel_rot_6d': array([ 0.998986  , -0.01856318, -0.03029766,  0.01747937,  0.979422  ,\n",
      "       -0.00103008], dtype=float32), 'action/gripper_position': array([-0.03401568], dtype=float32), 'action/gripper_force': array([-0.02578943], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00146823, -0.01198717,  0.0084354 ]), 'action/rel_rot_6d': array([ 0.99898601, -0.01856318, -0.03029766,  0.01747937,  0.97942197,\n",
      "       -0.00103008]), 'action/gripper_position': array([-0.45438013]), 'action/gripper_force': array([0.15782211])}\n",
      "{'action/rel_pos': array([-0.00146823, -0.01198717,  0.0084354 ]), 'action/rel_rot_6d': array([ 0.0005103 , -0.03032859,  0.01855588], dtype=float32), 'action/gripper_position': array([-0.45438013]), 'action/gripper_force': array([0.15782211])}\n",
      "[-0.00146823 -0.01198717  0.0084354   0.0005103  -0.03032859  0.01855588\n",
      " -0.45438013  0.15782211]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00227612  0.        ]\n",
      "timestep 23\n",
      "unnormalized action: {'action/rel_pos': array([ 0.03512016, -0.02455885, -0.00030365], dtype=float32), 'action/rel_rot_6d': array([0.96766746, 0.00776337, 0.01593961, 0.01970636, 0.99257267,\n",
      "       0.01400567], dtype=float32), 'action/gripper_position': array([-0.01671296], dtype=float32), 'action/gripper_force': array([-0.03079644], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.03512016, -0.02455885, -0.00030365]), 'action/rel_rot_6d': array([0.96766746, 0.00776337, 0.01593961, 0.01970636, 0.99257267,\n",
      "       0.01400567]), 'action/gripper_position': array([-0.44651157]), 'action/gripper_force': array([0.15701097])}\n",
      "{'action/rel_pos': array([ 0.03512016, -0.02455885, -0.00030365]), 'action/rel_rot_6d': array([-0.01378476,  0.01635859, -0.00824777], dtype=float32), 'action/gripper_position': array([-0.44651157]), 'action/gripper_force': array([0.15701097])}\n",
      "[ 3.51201631e-02 -2.45588459e-02 -3.03646491e-04 -1.37847569e-02\n",
      "  1.63585879e-02 -8.24777409e-03 -4.46511568e-01  1.57010971e-01]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00678665  0.        ]\n",
      "timestep 24\n",
      "unnormalized action: {'action/rel_pos': array([-0.00534312,  0.0452637 , -0.01010197], dtype=float32), 'action/rel_rot_6d': array([ 0.9848626 , -0.01270212,  0.00119605, -0.01374027,  1.        ,\n",
      "        0.01843189], dtype=float32), 'action/gripper_position': array([-0.00142528], dtype=float32), 'action/gripper_force': array([0.02379867], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00534312,  0.0452637 , -0.01010197]), 'action/rel_rot_6d': array([ 0.98486263, -0.01270212,  0.00119605, -0.01374027,  1.        ,\n",
      "        0.01843189]), 'action/gripper_position': array([-0.43955936]), 'action/gripper_force': array([0.16585539])}\n",
      "{'action/rel_pos': array([-0.00534312,  0.0452637 , -0.01010197]), 'action/rel_rot_6d': array([-0.01844976,  0.00145216,  0.01287203], dtype=float32), 'action/gripper_position': array([-0.43955936]), 'action/gripper_force': array([0.16585539])}\n",
      "[-0.00534312  0.0452637  -0.01010197 -0.01844976  0.00145216  0.01287203\n",
      " -0.43955936  0.16585539]\n",
      "Ground Truth\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "timestep 25\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "DP: after time_distributed\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "unnormalized action: {'action/rel_pos': array([-1.1314633e-03, -7.5255528e-05,  1.0908812e-03], dtype=float32), 'action/rel_rot_6d': array([ 0.98933077, -0.00265563,  0.00245334, -0.00320572,  0.9816383 ,\n",
      "        0.00453265], dtype=float32), 'action/gripper_position': array([-0.0104809], dtype=float32), 'action/gripper_force': array([-0.00137604], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-1.13146333e-03, -7.52555279e-05,  1.09088118e-03]), 'action/rel_rot_6d': array([ 0.98933077, -0.00265563,  0.00245334, -0.00320572,  0.98163831,\n",
      "        0.00453265]), 'action/gripper_position': array([-0.44367748]), 'action/gripper_force': array([0.16177708])}\n",
      "{'action/rel_pos': array([-1.13146333e-03, -7.52555279e-05,  1.09088118e-03]), 'action/rel_rot_6d': array([-0.00462554,  0.00249218,  0.00267276], dtype=float32), 'action/gripper_position': array([-0.44367748]), 'action/gripper_force': array([0.16177708])}\n",
      "[-1.13146333e-03 -7.52555279e-05  1.09088118e-03 -4.62554395e-03\n",
      "  2.49218079e-03  2.67275702e-03 -4.43677483e-01  1.61777083e-01]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00227239  0.        ]\n",
      "timestep 26\n",
      "unnormalized action: {'action/rel_pos': array([-0.00184357,  0.00145238, -0.0003162 ], dtype=float32), 'action/rel_rot_6d': array([ 0.98210764, -0.00106424,  0.00144545, -0.00163633,  0.9832773 ,\n",
      "       -0.00337277], dtype=float32), 'action/gripper_position': array([-0.00742874], dtype=float32), 'action/gripper_force': array([0.00076651], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00184357,  0.00145238, -0.0003162 ]), 'action/rel_rot_6d': array([ 0.98210764, -0.00106424,  0.00144545, -0.00163633,  0.98327732,\n",
      "       -0.00337277]), 'action/gripper_position': array([-0.44228949]), 'action/gripper_force': array([0.16212418])}\n",
      "{'action/rel_pos': array([-0.00184357,  0.00145238, -0.0003162 ]), 'action/rel_rot_6d': array([0.00342768, 0.00146806, 0.00108866], dtype=float32), 'action/gripper_position': array([-0.44228949]), 'action/gripper_force': array([0.16212418])}\n",
      "[-1.84356782e-03  1.45237823e-03 -3.16200138e-04  3.42767918e-03\n",
      "  1.46805577e-03  1.08866149e-03 -4.42289487e-01  1.62124175e-01]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00450265  0.        ]\n",
      "timestep 27\n",
      "unnormalized action: {'action/rel_pos': array([-0.00091878, -0.0045345 ,  0.00018593], dtype=float32), 'action/rel_rot_6d': array([ 9.8309672e-01,  3.5553521e-03, -1.1358606e-03, -2.3220312e-04,\n",
      "        9.7527981e-01, -1.8008337e-04], dtype=float32), 'action/gripper_position': array([-0.00953108], dtype=float32), 'action/gripper_force': array([0.00153846], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00091878, -0.0045345 ,  0.00018593]), 'action/rel_rot_6d': array([ 9.83096719e-01,  3.55535210e-03, -1.13586057e-03, -2.32203121e-04,\n",
      "        9.75279808e-01, -1.80083371e-04]), 'action/gripper_position': array([-0.44324554]), 'action/gripper_force': array([0.16224923])}\n",
      "{'action/rel_pos': array([-0.00091878, -0.0045345 ,  0.00018593]), 'action/rel_rot_6d': array([ 0.00018492, -0.00115472, -0.00361668], dtype=float32), 'action/gripper_position': array([-0.44324554]), 'action/gripper_force': array([0.16224923])}\n",
      "[-9.18784295e-04 -4.53449879e-03  1.85933925e-04  1.84922828e-04\n",
      " -1.15472113e-03 -3.61667783e-03 -4.43245541e-01  1.62249231e-01]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -9.00516e-03\n",
      "  5.00000e-04]\n",
      "timestep 28\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00056123, -0.0002226 ,  0.00106525], dtype=float32), 'action/rel_rot_6d': array([ 9.8232335e-01, -9.6983346e-04,  3.9568462e-04, -2.0334546e-03,\n",
      "        9.8190844e-01, -3.2012640e-03], dtype=float32), 'action/gripper_position': array([-0.01118742], dtype=float32), 'action/gripper_force': array([0.00063105], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00056123, -0.0002226 ,  0.00106525]), 'action/rel_rot_6d': array([ 9.82323349e-01, -9.69833462e-04,  3.95684619e-04, -2.03345460e-03,\n",
      "        9.81908441e-01, -3.20126396e-03]), 'action/gripper_position': array([-0.44399878]), 'action/gripper_force': array([0.16210223])}\n",
      "{'action/rel_pos': array([ 0.00056123, -0.0002226 ,  0.00106525]), 'action/rel_rot_6d': array([0.00325941, 0.00039958, 0.00098859], dtype=float32), 'action/gripper_position': array([-0.44399878]), 'action/gripper_force': array([0.16210223])}\n",
      "[ 5.61226392e-04 -2.22602350e-04  1.06525107e-03  3.25940782e-03\n",
      "  3.99584736e-04  9.88592627e-04 -4.43998781e-01  1.62102230e-01]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -4.53479e-03\n",
      "  5.00000e-04]\n",
      "timestep 29\n",
      "unnormalized action: {'action/rel_pos': array([-0.00031949,  0.00095052,  0.00138546], dtype=float32), 'action/rel_rot_6d': array([ 0.98170835,  0.00151151, -0.00260291,  0.00248397,  0.9811939 ,\n",
      "       -0.00214667], dtype=float32), 'action/gripper_position': array([-0.00741309], dtype=float32), 'action/gripper_force': array([0.00023443], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00031949,  0.00095052,  0.00138546]), 'action/rel_rot_6d': array([ 0.98170835,  0.00151151, -0.00260291,  0.00248397,  0.9811939 ,\n",
      "       -0.00214667]), 'action/gripper_position': array([-0.44228237]), 'action/gripper_force': array([0.16203798])}\n",
      "{'action/rel_pos': array([-0.00031949,  0.00095052,  0.00138546]), 'action/rel_rot_6d': array([ 0.0021811 , -0.00264804, -0.00154544], dtype=float32), 'action/gripper_position': array([-0.44228237]), 'action/gripper_force': array([0.16203798])}\n",
      "[-3.19485815e-04  9.50524758e-04  1.38546026e-03  2.18110462e-03\n",
      " -2.64803693e-03 -1.54544099e-03 -4.42282369e-01  1.62037979e-01]\n",
      "Ground Truth\n",
      "[ 0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.         -0.00450377  0.        ]\n",
      "timestep 30\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00147963, -0.00294422,  0.00192018], dtype=float32), 'action/rel_rot_6d': array([ 0.9825733 ,  0.001531  , -0.00117146, -0.00144575,  0.98088783,\n",
      "        0.00184642], dtype=float32), 'action/gripper_position': array([-0.00999181], dtype=float32), 'action/gripper_force': array([0.00099681], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00147963, -0.00294422,  0.00192018]), 'action/rel_rot_6d': array([ 0.98257327,  0.001531  , -0.00117146, -0.00144575,  0.98088783,\n",
      "        0.00184642]), 'action/gripper_position': array([-0.44345506]), 'action/gripper_force': array([0.16216148])}\n",
      "{'action/rel_pos': array([ 0.00147963, -0.00294422,  0.00192018]), 'action/rel_rot_6d': array([-0.00188064, -0.00119516, -0.0015559 ], dtype=float32), 'action/gripper_position': array([-0.44345506]), 'action/gripper_force': array([0.16216148])}\n",
      "[ 0.00147963 -0.00294422  0.00192018 -0.00188064 -0.00119516 -0.0015559\n",
      " -0.44345506  0.16216148]\n",
      "Ground Truth\n",
      "[ 0.0000e+00  0.0000e+00  0.0000e+00  1.0000e+00  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00  1.0000e+00  0.0000e+00 -4.4988e-03  5.0000e-04]\n",
      "timestep 31\n",
      "unnormalized action: {'action/rel_pos': array([-0.00210896,  0.00287414, -0.0020771 ], dtype=float32), 'action/rel_rot_6d': array([ 9.7909296e-01, -2.9070149e-03, -1.3029036e-03,  8.8597351e-04,\n",
      "        9.8242188e-01, -1.2032342e-03], dtype=float32), 'action/gripper_position': array([-0.00873888], dtype=float32), 'action/gripper_force': array([-0.00161423], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00210896,  0.00287414, -0.0020771 ]), 'action/rel_rot_6d': array([ 9.79092956e-01, -2.90701492e-03, -1.30290363e-03,  8.85973510e-04,\n",
      "        9.82421875e-01, -1.20323419e-03]), 'action/gripper_position': array([-0.44288528]), 'action/gripper_force': array([0.1617385])}\n",
      "{'action/rel_pos': array([-0.00210896,  0.00287414, -0.0020771 ]), 'action/rel_rot_6d': array([ 0.00122356, -0.00133436,  0.00296745], dtype=float32), 'action/gripper_position': array([-0.44288528]), 'action/gripper_force': array([0.1617385])}\n",
      "[-0.00210896  0.00287414 -0.0020771   0.00122356 -0.00133436  0.00296745\n",
      " -0.44288528  0.1617385 ]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -2.23392e-03\n",
      "  5.00000e-04]\n",
      "timestep 32\n",
      "unnormalized action: {'action/rel_pos': array([-0.00034796,  0.00014234, -0.00090168], dtype=float32), 'action/rel_rot_6d': array([9.8331809e-01, 9.0633909e-04, 3.1415343e-03, 9.2931744e-04,\n",
      "       9.8023337e-01, 4.4611861e-05], dtype=float32), 'action/gripper_position': array([-0.01058982], dtype=float32), 'action/gripper_force': array([0.00238136], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00034796,  0.00014234, -0.00090168]), 'action/rel_rot_6d': array([9.83318090e-01, 9.06339090e-04, 3.14153428e-03, 9.29317437e-04,\n",
      "       9.80233371e-01, 4.46118611e-05]), 'action/gripper_position': array([-0.44372702]), 'action/gripper_force': array([0.16238578])}\n",
      "{'action/rel_pos': array([-0.00034796,  0.00014234, -0.00090168]), 'action/rel_rot_6d': array([-4.2482625e-05,  3.1947801e-03, -9.2184584e-04], dtype=float32), 'action/gripper_position': array([-0.44372702]), 'action/gripper_force': array([0.16238578])}\n",
      "[-3.47959140e-04  1.42336939e-04 -9.01678693e-04 -4.24826248e-05\n",
      "  3.19478009e-03 -9.21845844e-04 -4.43727015e-01  1.62385781e-01]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -4.49081e-03\n",
      "  5.00000e-04]\n",
      "timestep 33\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "DP: after time_distributed\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "unnormalized action: {'action/rel_pos': array([-0.02421841,  0.00561481, -0.02602944], dtype=float32), 'action/rel_rot_6d': array([ 0.98994225,  0.04017244, -0.00581024,  0.03419467,  0.99999094,\n",
      "        0.0224158 ], dtype=float32), 'action/gripper_position': array([-0.54052514], dtype=float32), 'action/gripper_force': array([-0.03973076], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.02421841,  0.00561481, -0.02602944]), 'action/rel_rot_6d': array([ 0.98994225,  0.04017244, -0.00581024,  0.03419467,  0.99999094,\n",
      "        0.0224158 ]), 'action/gripper_position': array([-0.68471982]), 'action/gripper_force': array([0.15556361])}\n",
      "{'action/rel_pos': array([-0.02421841,  0.00561481, -0.02602944]), 'action/rel_rot_6d': array([-0.02264426, -0.0067865 , -0.04041434], dtype=float32), 'action/gripper_position': array([-0.68471982]), 'action/gripper_force': array([0.15556361])}\n",
      "[-0.02421841  0.00561481 -0.02602944 -0.02264426 -0.0067865  -0.04041434\n",
      " -0.68471982  0.15556361]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -4.48554e-03\n",
      "  5.00000e-04]\n",
      "timestep 34\n",
      "unnormalized action: {'action/rel_pos': array([ 0.01945861, -0.02912319, -0.03439639], dtype=float32), 'action/rel_rot_6d': array([ 0.9524355 , -0.04183596, -0.02170059,  0.0102843 ,  0.99683386,\n",
      "       -0.00682583], dtype=float32), 'action/gripper_position': array([0.01543995], dtype=float32), 'action/gripper_force': array([-0.02128789], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.01945861, -0.02912319, -0.03439639]), 'action/rel_rot_6d': array([ 0.95243549, -0.04183596, -0.02170059,  0.0102843 ,  0.99683386,\n",
      "       -0.00682583]), 'action/gripper_position': array([-0.43188974]), 'action/gripper_force': array([0.15855136])}\n",
      "{'action/rel_pos': array([ 0.01945861, -0.02912319, -0.03439639]), 'action/rel_rot_6d': array([ 0.00660936, -0.02307004,  0.04373414], dtype=float32), 'action/gripper_position': array([-0.43188974]), 'action/gripper_force': array([0.15855136])}\n",
      "[ 0.01945861 -0.02912319 -0.03439639  0.00660936 -0.02307004  0.04373414\n",
      " -0.43188974  0.15855136]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -2.22504e-03\n",
      "  5.00000e-04]\n",
      "timestep 35\n",
      "unnormalized action: {'action/rel_pos': array([0.00252406, 0.02495089, 0.03807403], dtype=float32), 'action/rel_rot_6d': array([ 0.9999585 , -0.02172554,  0.00488571, -0.07107445,  0.95512265,\n",
      "        0.05767983], dtype=float32), 'action/gripper_position': array([0.04618832], dtype=float32), 'action/gripper_force': array([-0.01519841], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.00252406, 0.02495089, 0.03807403]), 'action/rel_rot_6d': array([ 0.99995852, -0.02172554,  0.00488571, -0.07107445,  0.95512265,\n",
      "        0.05767983]), 'action/gripper_position': array([-0.41790665]), 'action/gripper_force': array([0.15953786])}\n",
      "{'action/rel_pos': array([0.00252406, 0.02495089, 0.03807403]), 'action/rel_rot_6d': array([-0.06077699,  0.00619647,  0.02138589], dtype=float32), 'action/gripper_position': array([-0.41790665]), 'action/gripper_force': array([0.15953786])}\n",
      "[ 0.00252406  0.02495089  0.03807403 -0.06077699  0.00619647  0.02138589\n",
      " -0.41790665  0.15953786]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -2.22197e-03\n",
      "  5.00000e-04]\n",
      "timestep 36\n",
      "unnormalized action: {'action/rel_pos': array([-0.03266707, -0.01645972,  0.01629091], dtype=float32), 'action/rel_rot_6d': array([ 0.9798641 , -0.02583924, -0.01418798, -0.00480365,  0.99370277,\n",
      "       -0.00307223], dtype=float32), 'action/gripper_position': array([0.01416375], dtype=float32), 'action/gripper_force': array([0.00832029], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.03266707, -0.01645972,  0.01629091]), 'action/rel_rot_6d': array([ 0.97986412, -0.02583924, -0.01418798, -0.00480365,  0.99370277,\n",
      "       -0.00307223]), 'action/gripper_position': array([-0.43247011]), 'action/gripper_force': array([0.16334789])}\n",
      "{'action/rel_pos': array([-0.03266707, -0.01645972,  0.01629091]), 'action/rel_rot_6d': array([ 0.00316209, -0.01456182,  0.02631545], dtype=float32), 'action/gripper_position': array([-0.43247011]), 'action/gripper_force': array([0.16334789])}\n",
      "[-0.03266707 -0.01645972  0.01629091  0.00316209 -0.01456182  0.02631545\n",
      " -0.43247011  0.16334789]\n",
      "Ground Truth\n",
      "[ 0.00000e+00  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00\n",
      "  0.00000e+00  0.00000e+00  1.00000e+00  0.00000e+00 -4.43448e-03\n",
      "  5.00000e-04]\n",
      "timestep 37\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00122878, -0.01026226,  0.01708528], dtype=float32), 'action/rel_rot_6d': array([ 0.9814558 , -0.01817007,  0.01066992, -0.03243466,  1.        ,\n",
      "        0.0235207 ], dtype=float32), 'action/gripper_position': array([0.00442187], dtype=float32), 'action/gripper_force': array([-0.00695203], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00122878, -0.01026226,  0.01708528]), 'action/rel_rot_6d': array([ 0.9814558 , -0.01817007,  0.01066992, -0.03243466,  1.        ,\n",
      "        0.0235207 ]), 'action/gripper_position': array([-0.43690031]), 'action/gripper_force': array([0.16087377])}\n",
      "{'action/rel_pos': array([ 0.00122878, -0.01026226,  0.01708528]), 'action/rel_rot_6d': array([-0.02388311,  0.01131005,  0.0182453 ], dtype=float32), 'action/gripper_position': array([-0.43690031]), 'action/gripper_force': array([0.16087377])}\n",
      "[ 0.00122878 -0.01026226  0.01708528 -0.02388311  0.01131005  0.0182453\n",
      " -0.43690031  0.16087377]\n",
      "Ground Truth\n",
      "[0.e+00 0.e+00 0.e+00 1.e+00 0.e+00 0.e+00 0.e+00 1.e+00 0.e+00 0.e+00\n",
      " 5.e-04]\n",
      "timestep 38\n",
      "unnormalized action: {'action/rel_pos': array([-0.03975752,  0.00192294,  0.00343269], dtype=float32), 'action/rel_rot_6d': array([ 0.9938781 ,  0.03743704, -0.02778684, -0.07721077,  0.9455894 ,\n",
      "       -0.05791646], dtype=float32), 'action/gripper_position': array([-0.23398755], dtype=float32), 'action/gripper_force': array([0.03445195], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.03975752,  0.00192294,  0.00343269]), 'action/rel_rot_6d': array([ 0.99387813,  0.03743704, -0.02778684, -0.07721077,  0.94558942,\n",
      "       -0.05791646]), 'action/gripper_position': array([-0.54531913]), 'action/gripper_force': array([0.16758122])}\n",
      "{'action/rel_pos': array([-0.03975752,  0.00192294,  0.00343269]), 'action/rel_rot_6d': array([ 0.06325264, -0.02551556, -0.03932644], dtype=float32), 'action/gripper_position': array([-0.54531913]), 'action/gripper_force': array([0.16758122])}\n",
      "[-0.03975752  0.00192294  0.00343269  0.06325264 -0.02551556 -0.03932644\n",
      " -0.54531913  0.16758122]\n",
      "Ground Truth\n",
      "[0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.]\n",
      "timestep 39\n",
      "unnormalized action: {'action/rel_pos': array([-0.00473354, -0.02865579, -0.01415823], dtype=float32), 'action/rel_rot_6d': array([ 1.0000000e+00, -2.1363137e-02,  2.6666035e-04, -4.0621561e-04,\n",
      "        9.9157435e-01,  2.2601448e-02], dtype=float32), 'action/gripper_position': array([-0.03314934], dtype=float32), 'action/gripper_force': array([-0.00239345], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00473354, -0.02865579, -0.01415823]), 'action/rel_rot_6d': array([ 1.00000000e+00, -2.13631373e-02,  2.66660354e-04, -4.06215608e-04,\n",
      "        9.91574347e-01,  2.26014480e-02]), 'action/gripper_position': array([-0.45398616]), 'action/gripper_force': array([0.16161226])}\n",
      "{'action/rel_pos': array([-0.00473354, -0.02865579, -0.01415823]), 'action/rel_rot_6d': array([-0.02278986,  0.00075341,  0.02134826], dtype=float32), 'action/gripper_position': array([-0.45398616]), 'action/gripper_force': array([0.16161226])}\n",
      "[-0.00473354 -0.02865579 -0.01415823 -0.02278986  0.00075341  0.02134826\n",
      " -0.45398616  0.16161226]\n",
      "Ground Truth\n",
      "[ 4.0999999e-05 -1.6000000e-05 -5.5000000e-05  1.0000000e+00\n",
      " -1.0200265e-04  5.0994695e-05  1.0200000e-04  1.0000000e+00\n",
      "  5.2005202e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 40\n",
      "unnormalized action: {'action/rel_pos': array([0.01582226, 0.03989085, 0.02312944], dtype=float32), 'action/rel_rot_6d': array([ 0.97215265, -0.01856283,  0.00324404, -0.00928567,  0.95291597,\n",
      "       -0.00974519], dtype=float32), 'action/gripper_position': array([-0.01257608], dtype=float32), 'action/gripper_force': array([-0.01587782], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.01582226, 0.03989085, 0.02312944]), 'action/rel_rot_6d': array([ 0.97215265, -0.01856283,  0.00324404, -0.00928567,  0.95291597,\n",
      "       -0.00974519]), 'action/gripper_position': array([-0.44463029]), 'action/gripper_force': array([0.15942779])}\n",
      "{'action/rel_pos': array([0.01582226, 0.03989085, 0.02312944]), 'action/rel_rot_6d': array([0.01019573, 0.0031421 , 0.01912517], dtype=float32), 'action/gripper_position': array([-0.44463029]), 'action/gripper_force': array([0.15942779])}\n",
      "[ 0.01582226  0.03989085  0.02312944  0.01019573  0.0031421   0.01912517\n",
      " -0.44463029  0.15942779]\n",
      "Ground Truth\n",
      "[ 4.8999998e-05  9.9999997e-06  9.6999996e-05  1.0000000e+00\n",
      " -1.3099854e-04 -8.6002226e-05  1.3099999e-04  1.0000000e+00\n",
      "  1.6988733e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 41\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "DP: after time_distributed\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00343605, -0.00342482,  0.00044794], dtype=float32), 'action/rel_rot_6d': array([ 1.0000000e+00,  1.1840382e-03, -5.0247484e-04, -2.7244547e-03,\n",
      "        9.9591184e-01,  2.6455557e-03], dtype=float32), 'action/gripper_position': array([-0.00132059], dtype=float32), 'action/gripper_force': array([0.00056555], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00343605, -0.00342482,  0.00044794]), 'action/rel_rot_6d': array([ 1.00000000e+00,  1.18403824e-03, -5.02474839e-04, -2.72445474e-03,\n",
      "        9.95911837e-01,  2.64555565e-03]), 'action/gripper_position': array([-0.43951175]), 'action/gripper_force': array([0.16209162])}\n",
      "{'action/rel_pos': array([ 0.00343605, -0.00342482,  0.00044794]), 'action/rel_rot_6d': array([-0.00265503, -0.00050562, -0.0011827 ], dtype=float32), 'action/gripper_position': array([-0.43951175]), 'action/gripper_force': array([0.16209162])}\n",
      "[ 0.00343605 -0.00342482  0.00044794 -0.00265503 -0.00050562 -0.0011827\n",
      " -0.43951175  0.16209162]\n",
      "Ground Truth\n",
      "[ 7.1000002e-05 -2.4999999e-05  1.8600000e-04  9.9999994e-01\n",
      "  3.4900833e-04  7.9963698e-05 -3.4900001e-04  9.9999994e-01\n",
      " -1.0402791e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 42\n",
      "unnormalized action: {'action/rel_pos': array([-0.00062847,  0.00200451, -0.00033672], dtype=float32), 'action/rel_rot_6d': array([ 9.9607658e-01, -1.5371443e-04,  1.5987513e-03,  7.6982682e-04,\n",
      "        9.9590707e-01, -4.0752318e-04], dtype=float32), 'action/gripper_position': array([0.00149993], dtype=float32), 'action/gripper_force': array([0.00036392], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00062847,  0.00200451, -0.00033672]), 'action/rel_rot_6d': array([ 9.96076584e-01, -1.53714427e-04,  1.59875129e-03,  7.69826816e-04,\n",
      "        9.95907068e-01, -4.07523185e-04]), 'action/gripper_position': array([-0.43822909]), 'action/gripper_force': array([0.16205896])}\n",
      "{'action/rel_pos': array([-0.00062847,  0.00200451, -0.00033672]), 'action/rel_rot_6d': array([0.00041044, 0.00160498, 0.00015498], dtype=float32), 'action/gripper_position': array([-0.43822909]), 'action/gripper_force': array([0.16205896])}\n",
      "[-6.28470443e-04  2.00451398e-03 -3.36718862e-04  4.10438632e-04\n",
      "  1.60498370e-03  1.54978450e-04 -4.38229092e-01  1.62058956e-01]\n",
      "Ground Truth\n",
      "[ 3.5000001e-05 -2.7000000e-05  1.7499999e-04  9.9999994e-01\n",
      "  1.7979699e-05 -3.0300120e-04 -1.8000001e-05  1.0000000e+00\n",
      " -6.6994544e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 43\n",
      "unnormalized action: {'action/rel_pos': array([-0.00410717,  0.0020174 , -0.00168069], dtype=float32), 'action/rel_rot_6d': array([ 9.9136281e-01,  4.2302010e-04,  1.6795931e-04, -6.1058992e-04,\n",
      "        9.9633265e-01, -1.4659305e-03], dtype=float32), 'action/gripper_position': array([0.00348752], dtype=float32), 'action/gripper_force': array([-2.6687092e-05], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00410717,  0.0020174 , -0.00168069]), 'action/rel_rot_6d': array([ 9.91362810e-01,  4.23020101e-04,  1.67959312e-04, -6.10589923e-04,\n",
      "        9.96332645e-01, -1.46593049e-03]), 'action/gripper_position': array([-0.43732522]), 'action/gripper_force': array([0.16199568])}\n",
      "{'action/rel_pos': array([-0.00410717,  0.0020174 , -0.00168069]), 'action/rel_rot_6d': array([ 0.00147122,  0.00017005, -0.00042646], dtype=float32), 'action/gripper_position': array([-0.43732522]), 'action/gripper_force': array([0.16199568])}\n",
      "[-4.10717260e-03  2.01739627e-03 -1.68069056e-03  1.47122110e-03\n",
      "  1.70050247e-04 -4.26455896e-04 -4.37325221e-01  1.61995677e-01]\n",
      "Ground Truth\n",
      "[-5.6000001e-05 -3.5000001e-05  9.6999996e-05  9.9999994e-01\n",
      "  1.5099632e-04 -3.3500165e-04 -1.5099999e-04  1.0000000e+00\n",
      " -1.0949415e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 44\n",
      "unnormalized action: {'action/rel_pos': array([ 0.00062536, -0.00345699,  0.00335318], dtype=float32), 'action/rel_rot_6d': array([9.8845220e-01, 1.4650036e-03, 1.7381824e-03, 6.0702092e-04,\n",
      "       9.9056780e-01, 6.1993685e-04], dtype=float32), 'action/gripper_position': array([-0.00064978], dtype=float32), 'action/gripper_force': array([-0.00021472], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.00062536, -0.00345699,  0.00335318]), 'action/rel_rot_6d': array([9.88452196e-01, 1.46500359e-03, 1.73818239e-03, 6.07020920e-04,\n",
      "       9.90567803e-01, 6.19936851e-04]), 'action/gripper_position': array([-0.43920669]), 'action/gripper_force': array([0.16196522])}\n",
      "{'action/rel_pos': array([ 0.00062536, -0.00345699,  0.00335318]), 'action/rel_rot_6d': array([-0.00062476,  0.00175756, -0.00148321], dtype=float32), 'action/gripper_position': array([-0.43920669]), 'action/gripper_force': array([0.16196522])}\n",
      "[ 0.00062536 -0.00345699  0.00335318 -0.00062476  0.00175756 -0.00148321\n",
      " -0.43920669  0.16196522]\n",
      "Ground Truth\n",
      "[-4.7000001e-05 -2.9999999e-05 -1.2000000e-04  1.0000000e+00\n",
      "  1.0701056e-04 -3.9971750e-05 -1.0700000e-04  9.9999994e-01\n",
      "  2.6400428e-04  0.0000000e+00  0.0000000e+00]\n",
      "timestep 45\n",
      "unnormalized action: {'action/rel_pos': array([0.00097481, 0.00291219, 0.00105621], dtype=float32), 'action/rel_rot_6d': array([ 0.9886456 ,  0.0025199 ,  0.00237775, -0.00437963,  0.9776299 ,\n",
      "       -0.00164711], dtype=float32), 'action/gripper_position': array([-0.00243605], dtype=float32), 'action/gripper_force': array([-0.00069105], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.00097481, 0.00291219, 0.00105621]), 'action/rel_rot_6d': array([ 0.98864561,  0.0025199 ,  0.00237775, -0.00437963,  0.9776299 ,\n",
      "       -0.00164711]), 'action/gripper_position': array([-0.44001901]), 'action/gripper_force': array([0.16188805])}\n",
      "{'action/rel_pos': array([0.00097481, 0.00291219, 0.00105621]), 'action/rel_rot_6d': array([ 0.001674  ,  0.00240932, -0.0025448 ], dtype=float32), 'action/gripper_position': array([-0.44001901]), 'action/gripper_force': array([0.16188805])}\n",
      "[ 0.00097481  0.00291219  0.00105621  0.001674    0.00240932 -0.0025448\n",
      " -0.44001901  0.16188805]\n",
      "Ground Truth\n",
      "[-3.5000001e-05  0.0000000e+00 -5.6000001e-05  1.0000000e+00\n",
      "  1.4300013e-04 -1.6998856e-05 -1.4300000e-04  1.0000000e+00\n",
      "  8.0024311e-06  0.0000000e+00  0.0000000e+00]\n",
      "timestep 46\n",
      "unnormalized action: {'action/rel_pos': array([-0.0009832 , -0.00102321,  0.00340277], dtype=float32), 'action/rel_rot_6d': array([ 0.01813411,  0.00034237, -0.00503047, -0.00113039,  0.02316947,\n",
      "        0.00015882], dtype=float32), 'action/gripper_position': array([-0.00070206], dtype=float32), 'action/gripper_force': array([-0.0025899], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.0009832 , -0.00102321,  0.00340277]), 'action/rel_rot_6d': array([ 0.01813411,  0.00034237, -0.00503047, -0.00113039,  0.02316947,\n",
      "        0.00015882]), 'action/gripper_position': array([-0.43923047]), 'action/gripper_force': array([0.16158044])}\n",
      "{'action/rel_pos': array([-0.0009832 , -0.00102321,  0.00340277]), 'action/rel_rot_6d': array([ 0.00667292, -0.27047673, -0.01997419], dtype=float32), 'action/gripper_position': array([-0.43923047]), 'action/gripper_force': array([0.16158044])}\n",
      "[-0.0009832  -0.00102321  0.00340277  0.00667292 -0.27047673 -0.01997419\n",
      " -0.43923047  0.16158044]\n",
      "Ground Truth\n",
      "[ 8.80000007e-05  1.99999999e-06  7.99999998e-06  1.00000000e+00\n",
      "  1.08994987e-04  1.09005014e-04 -1.09000001e-04  1.00000000e+00\n",
      "  4.59881194e-05  0.00000000e+00  0.00000000e+00]\n",
      "timestep 47\n",
      "unnormalized action: {'action/rel_pos': array([-0.00429392, -0.00165877,  0.00266322], dtype=float32), 'action/rel_rot_6d': array([ 0.00929171,  0.00033976, -0.00193326, -0.00169518,  0.01125559,\n",
      "        0.00063155], dtype=float32), 'action/gripper_position': array([0.00301909], dtype=float32), 'action/gripper_force': array([0.00149113], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([-0.00429392, -0.00165877,  0.00266322]), 'action/rel_rot_6d': array([ 0.00929171,  0.00033976, -0.00193326, -0.00169518,  0.01125559,\n",
      "        0.00063155]), 'action/gripper_position': array([-0.43753824]), 'action/gripper_force': array([0.16224156])}\n",
      "{'action/rel_pos': array([-0.00429392, -0.00165877,  0.00266322]), 'action/rel_rot_6d': array([-0.02463298, -0.20593882, -0.03075576], dtype=float32), 'action/gripper_position': array([-0.43753824]), 'action/gripper_force': array([0.16224156])}\n",
      "[-0.00429392 -0.00165877  0.00266322 -0.02463298 -0.20593882 -0.03075576\n",
      " -0.43753824  0.16224156]\n",
      "Ground Truth\n",
      "[ 9.0000001e-05 -9.9999997e-06  7.4000003e-05  1.0000000e+00\n",
      " -6.9990551e-06  3.5000190e-05  7.0000001e-06  1.0000000e+00\n",
      " -2.6999754e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 48\n",
      "unnormalized action: {'action/rel_pos': array([0.00226413, 0.00269901, 0.00221444], dtype=float32), 'action/rel_rot_6d': array([ 0.00499302,  0.00152814, -0.00423912,  0.00293529,  0.00993779,\n",
      "        0.00236795], dtype=float32), 'action/gripper_position': array([-0.00013014], dtype=float32), 'action/gripper_force': array([-0.00026497], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([0.00226413, 0.00269901, 0.00221444]), 'action/rel_rot_6d': array([ 0.00499302,  0.00152814, -0.00423912,  0.00293529,  0.00993779,\n",
      "        0.00236795]), 'action/gripper_position': array([-0.43897038]), 'action/gripper_force': array([0.16195708])}\n",
      "{'action/rel_pos': array([0.00226413, 0.00269901, 0.00221444]), 'action/rel_rot_6d': array([-0.49331158, -0.7287738 ,  0.09850872], dtype=float32), 'action/gripper_position': array([-0.43897038]), 'action/gripper_force': array([0.16195708])}\n",
      "[ 0.00226413  0.00269901  0.00221444 -0.49331158 -0.72877377  0.09850872\n",
      " -0.43897038  0.16195708]\n",
      "Ground Truth\n",
      "[ 3.2000000e-05  9.0000003e-06  7.4000003e-05  1.0000000e+00\n",
      " -2.8999797e-05  6.0009861e-06  2.9000001e-05  1.0000000e+00\n",
      " -3.3999826e-05  0.0000000e+00  0.0000000e+00]\n",
      "timestep 49\n",
      "key: robot_state/cartesian_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 6])\n",
      "len shapes: 1 shape: [6]\n",
      "key: robot_state/gripper_position\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/applied_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: robot_state/contact_force\n",
      "ndim: 3, shape: torch.Size([1, 2, 1])\n",
      "len shapes: 1 shape: [1]\n",
      "key: camera/image/varied_camera_1_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "center_crop: im.shape: torch.Size([2, 128, 128, 3])\n",
      "DP: after time_distributed\n",
      "key: camera/image/varied_camera_2_left_image\n",
      "ndim: 5, shape: torch.Size([1, 2, 3, 128, 128])\n",
      "len shapes: 3 shape: [3, 128, 128]\n",
      "unnormalized action: {'action/rel_pos': array([ 0.05971329, -0.12982583,  0.10883674], dtype=float32), 'action/rel_rot_6d': array([ 0.99447703, -0.33619118, -0.3412339 ,  0.14024034,  0.9852037 ,\n",
      "        0.08940701], dtype=float32), 'action/gripper_position': array([-0.42128813], dtype=float32), 'action/gripper_force': array([0.2883654], dtype=float32)}\n",
      "normalized action: {'action/rel_pos': array([ 0.05971329, -0.12982583,  0.10883674]), 'action/rel_rot_6d': array([ 0.99447703, -0.33619118, -0.34123391,  0.14024034,  0.98520368,\n",
      "        0.08940701]), 'action/gripper_position': array([-0.63049573]), 'action/gripper_force': array([0.20871524])}\n",
      "{'action/rel_pos': array([ 0.05971329, -0.12982583,  0.10883674]), 'action/rel_rot_6d': array([-0.1324048 , -0.28731942,  0.3498027 ], dtype=float32), 'action/gripper_position': array([-0.63049573]), 'action/gripper_force': array([0.20871524])}\n",
      "[ 0.05971329 -0.12982583  0.10883674 -0.1324048  -0.28731942  0.3498027\n",
      " -0.63049573  0.20871524]\n",
      "Ground Truth\n"
     ]
    }
   ],
   "source": [
    "timestep = 0\n",
    "for i in range(len(obs_iter)):\n",
    "    print(f\"timestep {i}\")\n",
    "    action = policy(obs_iter[i])\n",
    "    print(action)\n",
    "    print(f\"Ground Truth\")\n",
    "    if i < len(act):\n",
    "        print(np.array(act[i][0]))\n",
    "    timestep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e7fb32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionPolicyUNet (\n",
       "  ModuleDict(\n",
       "    (policy): ModuleDict(\n",
       "      (obs_encoder): DataParallel(\n",
       "        (module): ObservationGroupEncoder(\n",
       "            group=obs\n",
       "            ObservationEncoder(\n",
       "                Key(\n",
       "                    name=robot_state/cartesian_position\n",
       "                    shape=[6]\n",
       "                    modality=low_dim\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): None\n",
       "                    )\n",
       "                    net=None\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=robot_state/gripper_position\n",
       "                    shape=[1]\n",
       "                    modality=low_dim\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): None\n",
       "                    )\n",
       "                    net=None\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=robot_state/applied_force\n",
       "                    shape=[1]\n",
       "                    modality=low_dim\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): None\n",
       "                    )\n",
       "                    net=None\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=robot_state/contact_force\n",
       "                    shape=[1]\n",
       "                    modality=low_dim\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): None\n",
       "                    )\n",
       "                    net=None\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=camera/image/varied_camera_1_left_image\n",
       "                    shape=[3, 128, 128]\n",
       "                    modality=rgb\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): ColorRandomizer(input_shape=[3, 128, 128], brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3], num_samples=1)\n",
       "                      (1): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)\n",
       "                    )\n",
       "                    net=VisualCore(\n",
       "                      input_shape=[3, 116, 116]\n",
       "                      output_shape=[512]\n",
       "                      backbone_net=ResNet50Conv(input_channel=3, input_coord_conv=False)\n",
       "                      pool_net=None\n",
       "                    )\n",
       "                    sharing_from=None\n",
       "                )\n",
       "                Key(\n",
       "                    name=camera/image/varied_camera_2_left_image\n",
       "                    shape=[3, 128, 128]\n",
       "                    modality=rgb\n",
       "                    randomizer=ModuleList(\n",
       "                      (0): ColorRandomizer(input_shape=[3, 128, 128], brightness=[0.7, 1.3], contrast=[0.7, 1.3], saturation=[0.7, 1.3], hue=[-0.3, 0.3], num_samples=1)\n",
       "                      (1): CropRandomizer(input_shape=[3, 128, 128], crop_size=[116, 116], num_crops=1)\n",
       "                    )\n",
       "                    net=VisualCore(\n",
       "                      input_shape=[3, 116, 116]\n",
       "                      output_shape=[512]\n",
       "                      backbone_net=ResNet50Conv(input_channel=3, input_coord_conv=False)\n",
       "                      pool_net=None\n",
       "                    )\n",
       "                    sharing_from=camera/image/varied_camera_1_left_image\n",
       "                )\n",
       "                output_shape=[1033]\n",
       "            )\n",
       "        )\n",
       "      )\n",
       "      (noise_pred_net): DataParallel(\n",
       "        (module): ConditionalUnet1D(\n",
       "          (mid_modules): ModuleList(\n",
       "            (0-1): 2 x ConditionalResidualBlock1D(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x Conv1dBlock(\n",
       "                  (block): Sequential(\n",
       "                    (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                    (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                    (2): Mish()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (cond_encoder): Sequential(\n",
       "                (0): Mish()\n",
       "                (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
       "                (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "              )\n",
       "              (residual_conv): Identity()\n",
       "            )\n",
       "          )\n",
       "          (diffusion_step_encoder): Sequential(\n",
       "            (0): SinusoidalPosEmb()\n",
       "            (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (2): Mish()\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "          (up_modules): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(2048, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Upsample1d(\n",
       "                (conv): ConvTranspose1d(512, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(1024, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(1024, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Upsample1d(\n",
       "                (conv): ConvTranspose1d(256, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (down_modules): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(11, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(11, 256, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=512, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Downsample1d(\n",
       "                (conv): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(256, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 512, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Downsample1d(\n",
       "                (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              )\n",
       "            )\n",
       "            (2): ModuleList(\n",
       "              (0): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(512, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                  (1): Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "              )\n",
       "              (1): ConditionalResidualBlock1D(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x Conv1dBlock(\n",
       "                    (block): Sequential(\n",
       "                      (0): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                      (1): GroupNorm(8, 1024, eps=1e-05, affine=True)\n",
       "                      (2): Mish()\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (cond_encoder): Sequential(\n",
       "                  (0): Mish()\n",
       "                  (1): Linear(in_features=1280, out_features=2048, bias=True)\n",
       "                  (2): Unflatten(dim=-1, unflattened_size=(-1, 1))\n",
       "                )\n",
       "                (residual_conv): Identity()\n",
       "              )\n",
       "              (2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (final_conv): Sequential(\n",
       "            (0): Conv1dBlock(\n",
       "              (block): Sequential(\n",
       "                (0): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "                (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "                (2): Mish()\n",
       "              )\n",
       "            )\n",
       "            (1): Conv1d(256, 11, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac0e9f",
   "metadata": {},
   "source": [
    "### Define the rollout loop\n",
    "Now let's define the main rollout loop. The loop runs the policy to a target `horizon` and optionally writes the rollout to a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd1375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(policy, env, horizon, render=False, video_writer=None, video_skip=5, camera_names=None):\n",
    "    \"\"\"\n",
    "    Helper function to carry out rollouts. Supports on-screen rendering, off-screen rendering to a video, \n",
    "    and returns the rollout trajectory.\n",
    "    Args:\n",
    "        policy (instance of RolloutPolicy): policy loaded from a checkpoint\n",
    "        env (instance of EnvBase): env loaded from a checkpoint or demonstration metadata\n",
    "        horizon (int): maximum horizon for the rollout\n",
    "        render (bool): whether to render rollout on-screen\n",
    "        video_writer (imageio writer): if provided, use to write rollout to video\n",
    "        video_skip (int): how often to write video frames\n",
    "        camera_names (list): determines which camera(s) are used for rendering. Pass more than\n",
    "            one to output a video with multiple camera views concatenated horizontally.\n",
    "    Returns:\n",
    "        stats (dict): some statistics for the rollout - such as return, horizon, and task success\n",
    "    \"\"\"\n",
    "    assert isinstance(env, EnvBase)\n",
    "    assert isinstance(policy, RolloutPolicy)\n",
    "    assert not (render and (video_writer is not None))\n",
    "\n",
    "    policy.start_episode()\n",
    "    obs = env.reset()\n",
    "    state_dict = env.get_state()\n",
    "\n",
    "    # hack that is necessary for robosuite tasks for deterministic action playback\n",
    "    obs = env.reset_to(state_dict)\n",
    "\n",
    "    results = {}\n",
    "    video_count = 0  # video frame counter\n",
    "    total_reward = 0.\n",
    "    try:\n",
    "        for step_i in range(horizon):\n",
    "\n",
    "            # get action from policy\n",
    "            act = policy(ob=obs)\n",
    "\n",
    "            # play action\n",
    "            next_obs, r, done, _ = env.step(act)\n",
    "\n",
    "            # compute reward\n",
    "            total_reward += r\n",
    "            success = env.is_success()[\"task\"]\n",
    "\n",
    "            # visualization\n",
    "            if render:\n",
    "                env.render(mode=\"human\", camera_name=camera_names[0])\n",
    "            if video_writer is not None:\n",
    "                if video_count % video_skip == 0:\n",
    "                    video_img = []\n",
    "                    for cam_name in camera_names:\n",
    "                        video_img.append(env.render(mode=\"rgb_array\", height=512, width=512, camera_name=cam_name))\n",
    "                    video_img = np.concatenate(video_img, axis=1) # concatenate horizontally\n",
    "                    video_writer.append_data(video_img)\n",
    "                video_count += 1\n",
    "\n",
    "            # break if done or if success\n",
    "            if done or success:\n",
    "                break\n",
    "\n",
    "            # update for next iter\n",
    "            obs = deepcopy(next_obs)\n",
    "            state_dict = env.get_state()\n",
    "\n",
    "    except env.rollout_exceptions as e:\n",
    "        print(\"WARNING: got rollout exception {}\".format(e))\n",
    "\n",
    "    stats = dict(Return=total_reward, Horizon=(step_i + 1), Success_Rate=float(success))\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43d371",
   "metadata": {},
   "source": [
    "### Run the policy\n",
    "Now let's rollout the policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be6e1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_horizon = 400\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "video_path = \"rollout.mp4\"\n",
    "video_writer = imageio.get_writer(video_path, fps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fa67efe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m stats \u001b[38;5;241m=\u001b[39m rollout(\n\u001b[0;32m      2\u001b[0m     policy\u001b[38;5;241m=\u001b[39mpolicy, \n\u001b[1;32m----> 3\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m, \n\u001b[0;32m      4\u001b[0m     horizon\u001b[38;5;241m=\u001b[39mrollout_horizon, \n\u001b[0;32m      5\u001b[0m     render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m      6\u001b[0m     video_writer\u001b[38;5;241m=\u001b[39mvideo_writer, \n\u001b[0;32m      7\u001b[0m     video_skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m      8\u001b[0m     camera_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magentview\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(stats)\n\u001b[0;32m     11\u001b[0m video_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "stats = rollout(\n",
    "    policy=policy, \n",
    "    env=env, \n",
    "    horizon=rollout_horizon, \n",
    "    render=False, \n",
    "    video_writer=video_writer, \n",
    "    video_skip=5, \n",
    "    camera_names=[\"agentview\"]\n",
    ")\n",
    "print(stats)\n",
    "video_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79bc19",
   "metadata": {},
   "source": [
    "### Visualize the rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97472b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
